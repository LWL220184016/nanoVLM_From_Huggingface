{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a ALM in Google Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "fdd4fe4c-9758-43ea-f7a2-716032d8e32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nanoVLM_From_Huggingface' already exists and is not an empty directory.\n",
            "/content/nanoVLM_From_Huggingface\n",
            "assets\t\t\tdebug_func.py\t\t\t\t  nanoALM.ipynb\n",
            "benchmark-inference.py\tdebug_tokenizer_dataset_compatibility.py  __pycache__\n",
            "benchmark_suite.py\tgenerate.py\t\t\t\t  README.md\n",
            "checkpoints\t\tmeasure_vram.py\t\t\t\t  train.py\n",
            "data\t\t\tmodels\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoALM'):\n",
        "    !git clone https://github.com/LWL220184016/nanoVLM_From_Huggingface.git\n",
        "%cd nanoVLM_From_Huggingface/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "54bc8463",
      "metadata": {
        "id": "54bc8463"
      },
      "outputs": [],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "# !huggingface-cli login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bcw8qQqoOSR7",
        "outputId": "394f9139-3584-42b9-99b2-af7b19a2342e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting datasets==3.6.0\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ],
      "source": [
        "# If you get an \"Error\" from pip's dependency resolver but the cell complets fine, this is not an issue, you can continue :)\n",
        "!pip -q install torch\n",
        "!pip -q install gcsfs\n",
        "!pip -q install tqdm\n",
        "!pip -q install huggingface_hub\n",
        "!pip -q install librosa\n",
        "!pip install soundfile librosa -q\n",
        "# !pip install --upgrade transformers\n",
        "!pip install datasets==3.6.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoALM\"\n",
        "# hf_model_name = \"YOUR_HF_USER_NAME/nanoALM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "025aa292-7d4a-4adb-ae4b-b637811b9052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# nanoALM Imports (please check out the implementations in detail, that's where all the interessting stuff is!)\n",
        "from data.collators import AlignmentCollator, AudioQACollator, SAVEECollator\n",
        "from data.datasets import SAVEEDataset, AudioQADataset\n",
        "from data.processors import get_audio_processor\n",
        "from data.processors import get_tokenizer\n",
        "from models.audio_language_model import AudioLanguageModel\n",
        "import models.utils as utils\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "#Otherwise, the tokenizer will through a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "trained_model = None\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, alm_cfg):\n",
        "    # Create datasets\n",
        "    audio_processor = get_audio_processor(alm_cfg)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # text = \"splitting datasets, disable in get_dataloaders function\"\n",
        "    # print(f\"\\n\\033[38;5;05m{text}05m\\033[0m\")\n",
        "    # Load and combine all training datasets\n",
        "    combined_train_data = []\n",
        "    for dataset_name in train_cfg.train_dataset_name:\n",
        "        train_ds = load_dataset(\n",
        "        path = train_cfg.train_dataset_path,\n",
        "        name = dataset_name,\n",
        "    )\n",
        "        combined_train_data.append(train_ds['train'])\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    test_ds = load_dataset(train_cfg.test_dataset_path)\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    train_dataset = AudioQADataset(train_ds.select(range(train_size)), tokenizer, audio_processor)\n",
        "    val_dataset = AudioQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, audio_processor)\n",
        "    test_dataset = SAVEEDataset(test_ds, tokenizer, audio_processor)\n",
        "\n",
        "    # Create collators\n",
        "    alignment_collator = AlignmentCollator(tokenizer, alm_cfg.lm_max_length, audio_processor)\n",
        "    aqa_collator = AudioQACollator(tokenizer, alm_cfg.lm_max_length)\n",
        "    savee_collator = SAVEECollator(tokenizer)\n",
        "\n",
        "    # Create dataloaders\n",
        "    alignment_train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=alignment_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=train_cfg.savee_batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=savee_collator,\n",
        "        pin_memory=True,\n",
        "        )\n",
        "\n",
        "    return alignment_train_loader, train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D7NIuEDuOuuJ",
      "metadata": {
        "id": "D7NIuEDuOuuJ"
      },
      "source": [
        "### Prepare the testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9fnh6wOlOzat",
      "metadata": {
        "id": "9fnh6wOlOzat"
      },
      "outputs": [],
      "source": [
        "def test_savee(model, tokenizer, test_loader, device):\n",
        "    total_examples = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            audio = batch['audios'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            gen = model.generate(input_ids, audio, attention_mask)\n",
        "            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "            is_correct = utils.check_multiple_choice_with_regex(model_output, correct_answer)\n",
        "\n",
        "            total_examples += len(is_correct)\n",
        "            if is_correct:\n",
        "                correct_predictions += sum(is_correct)\n",
        "    accuracy = correct_predictions / total_examples if total_examples > 0 else 0\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe359194",
      "metadata": {
        "id": "fe359194"
      },
      "source": [
        "### Add debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5d7b7e46",
      "metadata": {
        "id": "5d7b7e46"
      },
      "outputs": [],
      "source": [
        "# 在训练开始前添加这个检查函数\n",
        "def debug_model_dimensions(model, input_ids, audio):\n",
        "    \"\"\"调试模型各层的维度\"\"\"\n",
        "    print(\"=== Model Dimension Debug ===\")\n",
        "\n",
        "    # 检查音频编码器\n",
        "    audio_features = model.audio_encoder.forward(audio, output_hidden_states=True)\n",
        "    print(f\"Audio features shape: {audio_features.shape}\")\n",
        "\n",
        "    # 检查模态投影器\n",
        "    audio_embeds = model.MP(audio_features)\n",
        "    print(f\"Audio embeds shape: {audio_embeds.shape}\")\n",
        "\n",
        "    # 检查文本嵌入\n",
        "    text_embeds = model.decoder.token_embedding(input_ids)\n",
        "    print(f\"Text embeds shape: {text_embeds.shape}\")\n",
        "\n",
        "    # 检查拼接后的嵌入\n",
        "    inputs_embeds = torch.cat([audio_embeds, text_embeds], dim=1)\n",
        "    print(f\"Combined embeds shape: {inputs_embeds.shape}\")\n",
        "\n",
        "    # 检查语言模型输出\n",
        "    logits = model.decoder(inputs_embeds)\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "    print(f\"Vocab size (last dim): {logits.shape[-1]}\")\n",
        "\n",
        "    # 检查语言模型配置\n",
        "    print(f\"LM vocab size config: {model.cfg.lm_vocab_size}\")\n",
        "    print(f\"Decoder vocab size: {getattr(model.decoder, 'vocab_size', 'Not found')}\")\n",
        "\n",
        "    return logits.shape[-1]\n",
        "\n",
        "# 在训练循环开始前调用\n",
        "# vocab_size = debug_model_dimensions(model, input_ids, audios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2d48d0df",
      "metadata": {
        "id": "2d48d0df"
      },
      "outputs": [],
      "source": [
        "def debug_training_step(model, input_ids, audios, attention_mask, labels):\n",
        "    \"\"\"调试训练步骤\"\"\"\n",
        "    # 添加这些调试行：\n",
        "    print(f\"Batch debug - input_ids shape: {input_ids.shape}, max: {input_ids.max().item()}\")\n",
        "    print(f\"Batch debug - labels shape: {labels.shape}, max: {labels.max().item()}\")\n",
        "    print(f\"Batch debug - Model vocab config: {model.cfg.lm_vocab_size}\")\n",
        "\n",
        "    # 检查decoder的实际vocab_size\n",
        "    if hasattr(model.decoder, 'head') and hasattr(model.decoder.head, 'out_features'):\n",
        "        print(f\"Decoder head in_features: {model.decoder.head.in_features}\")\n",
        "        print(f\"Decoder head out_features: {model.decoder.head.out_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f3dce46",
      "metadata": {
        "id": "2f3dce46"
      },
      "source": [
        "#### Supervised learning and Generative Training 監督學習生成式訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "KxOtMU5zPD-4",
      "metadata": {
        "id": "KxOtMU5zPD-4"
      },
      "outputs": [],
      "source": [
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def train(train_cfg, alm_cfg):\n",
        "    alignment_train_loader, train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # Initialize model\n",
        "    if train_cfg.resume_from_alm_checkpoint:\n",
        "        model = AudioLanguageModel.from_pretrained(alm_cfg.alm_checkpoint_path)\n",
        "    else:\n",
        "        model = AudioLanguageModel(alm_cfg)\n",
        "\n",
        "    print(f\"nanoALM initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    print(f\"Training summary: {len(train_loader.dataset)} samples, {len(train_loader)} batches/epoch, batch size {train_cfg.batch_size}\")\n",
        "\n",
        "    # Define optimizer groups\n",
        "    param_groups = [{'params': model.MP.parameters(), 'lr': train_cfg.lr_mp},\n",
        "                    {'params': list(model.decoder.parameters()) + list(model.audio_encoder.parameters()), 'lr': train_cfg.lr_backbones}]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    epoch_times = []\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    best_accuracy = 0\n",
        "    global_step = 0\n",
        "    for epoch in range(train_cfg.epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_tokens_processed = 0\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "            batch_start_time = time.time()\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            # debug_model_dimensions(model, input_ids, audios)  # Debug model dimensions with dummy data\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16): # Mixed precision training\n",
        "                # debug_training_step(model, input_ids, audios, attention_mask, labels)  # Debug training step\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            adj_lr_mp = get_lr(global_step, train_cfg.lr_mp, len(train_loader) * train_cfg.epochs)\n",
        "            adj_lr_backbones = get_lr(global_step, train_cfg.lr_backbones, len(train_loader) * train_cfg.epochs)\n",
        "            optimizer.param_groups[0]['lr'] = adj_lr_mp\n",
        "            optimizer.param_groups[1]['lr'] = adj_lr_backbones\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            num_tokens = torch.sum(attention_mask).item()\n",
        "            # 修改音頻token計算：根據實際的音頻處理方式\n",
        "            audio_tokens = audios.shape[0] * alm_cfg.mp_target_length  # 使用配置的目標長度\n",
        "            num_tokens += audio_tokens\n",
        "            total_tokens_processed += num_tokens\n",
        "\n",
        "            batch_end_time = time.time()\n",
        "            batch_duration = batch_end_time - batch_start_time\n",
        "            tokens_per_second = num_tokens / batch_duration\n",
        "\n",
        "            if global_step % 5 == 0:\n",
        "                model.eval()\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory\n",
        "                with torch.no_grad():\n",
        "                    total_val_loss = 0\n",
        "                    for batch in val_loader:\n",
        "                        audios = batch[\"audio\"].to(device)\n",
        "                        input_ids = batch[\"input_ids\"].to(device)\n",
        "                        labels = batch[\"labels\"].to(device)\n",
        "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "                        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                            # debug_training_step(model, input_ids, audios, attention_mask, labels)  # Debug training step\n",
        "                            _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "                        total_val_loss += loss.item()\n",
        "                    avg_val_loss = total_val_loss / len(val_loader)\n",
        "                    val_losses.append(avg_val_loss)\n",
        "                    val_plot_steps.append(global_step)\n",
        "                epoch_accuracy = 0\n",
        "                if train_cfg.eval_in_epochs:\n",
        "                    epoch_accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "                    if epoch_accuracy > best_accuracy:\n",
        "                      best_accuracy = epoch_accuracy\n",
        "                      model.save_pretrained(save_directory=alm_cfg.alm_checkpoint_path)\n",
        "                    print(f\"\\nStep: {global_step}, Loss: {batch_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Tokens/s: {tokens_per_second:.2f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "                model.train()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        epoch_tokens_per_second = total_tokens_processed / epoch_duration\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{train_cfg.epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\")\n",
        "\n",
        "    # Summary Statistics\n",
        "    if not train_cfg.eval_in_epochs:\n",
        "        model.save_pretrained(save_directory=alm_cfg.alm_checkpoint_path)\n",
        "    try:\n",
        "        model.push_to_hub(hf_model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error pushing model to hub: {e}\")\n",
        "\n",
        "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
        "    total_training_time = sum(epoch_times)\n",
        "    total_samples_processed = len(train_loader.dataset) * train_cfg.epochs\n",
        "    avg_time_per_sample = total_training_time / total_samples_processed\n",
        "    print(f\"Average time per epoch: {avg_epoch_time:.2f}s\")\n",
        "    print(f\"Average time per sample: {avg_time_per_sample:.4f}s\")\n",
        "\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "    # With this code you can test the accuracy of the model on the SAVEE dataset\n",
        "    # But if you only train with few samples, the accuracy will be very low\n",
        "    # print(\"Testing SAVEE Accuracy:\")\n",
        "    # accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "    # print(f\"SAVEE Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04da5b01",
      "metadata": {
        "id": "04da5b01"
      },
      "source": [
        "#### Three-stage training (contrast training, generative training, instruction fine-tuning) 三段式訓練(對比訓練, 生成式訓練, 指令微調)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fbb323c8",
      "metadata": {
        "id": "fbb323c8"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from debug_func import debug_contrastive_learning\n",
        "\n",
        "# 改進對比學習訓練\n",
        "def contrastive_loss(audio_embeds, text_embeds, temperature=0.07):\n",
        "    \"\"\"\n",
        "    標準、高效的對比學習損失 (CLIP Loss)。\n",
        "    注意：輸入的 embeds 應該是池化後的 [B, D] 維度向量。\n",
        "    \"\"\"\n",
        "    # 歸一化\n",
        "    audio_embeds = F.normalize(audio_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "\n",
        "    # 計算相似度矩陣\n",
        "    # temperature 是一個重要的超參數，CLIP 論文中是可學習的，但固定值也可以\n",
        "    logits_per_audio = torch.matmul(audio_embeds, text_embeds.T) / temperature\n",
        "    logits_per_text = logits_per_audio.T\n",
        "\n",
        "    # 創建標籤 (0, 1, 2, ..., B-1)\n",
        "    labels = torch.arange(audio_embeds.shape[0]).to(logits_per_audio.device)\n",
        "\n",
        "    # 對稱的交叉熵損失\n",
        "    loss_a = F.cross_entropy(logits_per_audio, labels)\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "\n",
        "    total_loss = (loss_a + loss_t) / 2\n",
        "\n",
        "    # 監控指標 (可選但推薦)\n",
        "    with torch.no_grad():\n",
        "        pos_sim = torch.diagonal(logits_per_audio * temperature).mean()\n",
        "        mask = ~torch.eye(labels.shape[0], dtype=torch.bool, device=labels.device)\n",
        "        neg_sim = (logits_per_audio * temperature)[mask].mean()\n",
        "\n",
        "    return total_loss, {\n",
        "        \"loss\": total_loss.item(),\n",
        "        \"pos_sim\": pos_sim.item(), # 正樣本對的餘弦相似度\n",
        "        \"neg_sim\": neg_sim.item()  # 負樣本對的餘弦相似度\n",
        "    }\n",
        "\n",
        "def train_step1_alignment(train_cfg, alm_cfg):\n",
        "\n",
        "    # 直接初始化模型\n",
        "    model = AudioLanguageModel(alm_cfg)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # 凍結音頻編碼器和語言模型\n",
        "    model.audio_encoder.requires_grad_(False)\n",
        "    model.decoder.requires_grad_(False)\n",
        "    model.MP.requires_grad_(True)\n",
        "\n",
        "    alignment_train_loader, train_loader, val_loader, _ = get_dataloaders(train_cfg, alm_cfg)\n",
        "\n",
        "    optimizer = optim.AdamW(model.MP.parameters(), lr=train_cfg.lr_mp, weight_decay=0.01)\n",
        "    # 可選：添加學習率調度器，如 warmup\n",
        "\n",
        "    best_alignment = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage1_epochs):\n",
        "        model.train()\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage1 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device) # 需要 attention_mask\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. 音頻編碼 -> 投影\n",
        "            with torch.no_grad():\n",
        "                audio_features = model.audio_encoder.encoder(audios, output_hidden_states=True)\n",
        "            projected_audio_features = model.MP(audio_features)\n",
        "\n",
        "            # 2. 文本編碼 (!!! 關鍵修正 !!!)\n",
        "            with torch.no_grad():\n",
        "                text_outputs = model.decoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                text_features = text_outputs.last_hidden_state # [B, S, D]\n",
        "\n",
        "            # 3. 池化操作 (Pooling)\n",
        "            # 音頻池化\n",
        "            audio_pooled = projected_audio_features.mean(dim=1) # [B, D]\n",
        "            # 文本池化 (注意要忽略 padding)\n",
        "            last_hidden = text_features\n",
        "            # 根據 attention_mask 來安全地做平均池化\n",
        "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden * input_mask_expanded, 1)\n",
        "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "            text_pooled = sum_embeddings / sum_mask # [B, D]\n",
        "\n",
        "            # 4. 計算簡化後的對比損失\n",
        "            loss, metrics = contrastive_loss(audio_pooled, text_pooled)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.MP.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"Stage1 Epoch {epoch+1}: Contrastive Loss {avg_train_loss:.4f}\")\n",
        "\n",
        "        # 每5個epoch驗證一次\n",
        "        if epoch % 5 == 0:\n",
        "            model.eval()\n",
        "            total_alignment_score = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i, batch in enumerate(val_loader):\n",
        "                    if i >= 20:  # 只驗證前20個batch以節省時間\n",
        "                        break\n",
        "                    audios = batch[\"audio\"].to(device)\n",
        "                    input_ids = batch[\"input_ids\"].to(device)\n",
        "                    alignment_score = model.validate_audio_text_alignment(input_ids, audios)\n",
        "                    total_alignment_score += alignment_score\n",
        "\n",
        "            avg_alignment = total_alignment_score / min(20, len(val_loader))\n",
        "            print(f\"Epoch {epoch}: Average alignment score: {avg_alignment:.4f}\")\n",
        "\n",
        "            if avg_alignment > best_alignment:\n",
        "                best_alignment = avg_alignment\n",
        "                patience = 0\n",
        "                model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage1_best\")\n",
        "                print(f\"  New best alignment: {best_alignment:.4f}\")\n",
        "            else:\n",
        "                patience += 1\n",
        "\n",
        "            model.train()\n",
        "\n",
        "    print(f\"Stage 1 completed! Best alignment: {best_alignment:.4f}\")\n",
        "    return model\n",
        "\n",
        "def train_step2_pretraining(train_cfg, alm_cfg, stage1_model=None):\n",
        "    \"\"\"第二步：语言模型预训练\"\"\"\n",
        "    print(\"=== Stage 2: Language Model Pretraining ===\")\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # 加载第一阶段模型或从头开始\n",
        "    if stage1_model is not None:\n",
        "        model = stage1_model\n",
        "    else:\n",
        "        try:\n",
        "            model = AudioLanguageModel.from_pretrained(f\"{alm_cfg.alm_checkpoint_path}/stage1_final\")\n",
        "            print(\"Loaded Stage 1 model\")\n",
        "        except:\n",
        "            model = AudioLanguageModel(alm_cfg)\n",
        "            print(\"Starting Stage 2 from scratch\")\n",
        "\n",
        "    # 冻结音频编码器，解冻语言模型和模态投影器\n",
        "    for param in model.audio_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.decoder.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.MP.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Stage 2: Training {trainable_params:,} parameters\")\n",
        "\n",
        "    # 不同学习率\n",
        "    param_groups = [\n",
        "        {'params': model.MP.parameters(), 'lr': train_cfg.lr_mp * 0.1},\n",
        "        {'params': model.decoder.parameters(), 'lr': train_cfg.lr_backbones}\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    best_loss = float('inf')\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage2_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage2 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                # 使用因果语言建模损失\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # 动态学习率调整\n",
        "            adj_lr_mp = get_lr(global_step, train_cfg.lr_mp * 0.1, len(train_loader) * train_cfg.stage2_epochs)\n",
        "            adj_lr_backbones = get_lr(global_step, train_cfg.lr_backbones, len(train_loader) * train_cfg.stage2_epochs)\n",
        "            optimizer.param_groups[0]['lr'] = adj_lr_mp\n",
        "            optimizer.param_groups[1]['lr'] = adj_lr_backbones\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        if avg_train_loss < best_loss:\n",
        "            best_loss = avg_train_loss\n",
        "            model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_best\")\n",
        "\n",
        "    # 保存第二阶段模型\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_final\")\n",
        "    print(\"Stage 2 completed!\")\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model=None):\n",
        "    \"\"\"第三步：指令微调\"\"\"\n",
        "    print(\"=== Stage 3: Instruction Tuning ===\")\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # 加载第二阶段模型\n",
        "    if stage2_model is not None:\n",
        "        model = stage2_model\n",
        "    else:\n",
        "        try:\n",
        "            model = AudioLanguageModel.from_pretrained(f\"{alm_cfg.alm_checkpoint_path}/stage2_final\")\n",
        "            print(\"Loaded Stage 2 model\")\n",
        "        except:\n",
        "            print(\"No Stage 2 model found, using current model\")\n",
        "            model = AudioLanguageModel(alm_cfg)\n",
        "\n",
        "    # 全部解冻，使用较小学习率\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    print(f\"Stage 3: Training all {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    # 更小的学习率\n",
        "    param_groups = [\n",
        "        {'params': model.MP.parameters(), 'lr': train_cfg.lr_mp * 0.01},\n",
        "        {'params': model.decoder.parameters(), 'lr': train_cfg.lr_backbones * 0.1},\n",
        "        {'params': model.audio_encoder.parameters(), 'lr': train_cfg.lr_backbones * 0.01}\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # 这里可以使用原来的训练循环，但数据应该是指令格式\n",
        "    # 暂时使用相同的数据格式\n",
        "    best_accuracy = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage3_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage3 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                print(f\"Stage3 Step: {global_step}, Instruction Loss: {batch_loss:.4f}\")\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # 评估性能\n",
        "        if train_cfg.eval_in_epochs:\n",
        "            accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage3_best\")\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Loss: {avg_train_loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
        "        else:\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Instruction Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 保存最终模型\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/final_model\")\n",
        "    print(\"Stage 3 completed!\")\n",
        "    return model\n",
        "\n",
        "def train_three_stages(train_cfg, alm_cfg):\n",
        "    \"\"\"完整的三阶段训练\"\"\"\n",
        "    print(\"Starting Three-Stage Training Pipeline\")\n",
        "\n",
        "    # 第一阶段：模态投影器对齐\n",
        "    stage1_model = train_step1_alignment(train_cfg, alm_cfg)\n",
        "\n",
        "    # 第二阶段：语言模型预训练\n",
        "    stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model)\n",
        "\n",
        "    # 第三阶段：指令微调\n",
        "    final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model)\n",
        "\n",
        "    print(\"=== Training Pipeline Completed! ===\")\n",
        "    return stage1_model, stage2_model, final_model\n",
        "\n",
        "\n",
        "# # 替换原来的训练调用\n",
        "# alm_cfg = ALMConfig()\n",
        "# train_cfg = TrainConfig()\n",
        "\n",
        "# # 运行三阶段训练\n",
        "# final_model = train_three_stages(train_cfg, alm_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "### Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "BXUaUEUcJCp2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXUaUEUcJCp2",
        "outputId": "5e7dcdf3-ea4f-433c-d333-649b99b9f19a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'checkpoints' already exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from models.config import ALMConfig, TrainConfig\n",
        "\n",
        "# 要創建的目錄路徑\n",
        "dir_name = ALMConfig.alm_checkpoint_path\n",
        "\n",
        "try:\n",
        "    os.mkdir(dir_name)\n",
        "    print(f\"Directory '{dir_name}' created successfully.\")\n",
        "except FileExistsError:\n",
        "    print(f\"Directory '{dir_name}' already exists.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Parent directory does not exist for '{dir_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9MlFpXQFSNdx",
        "outputId": "df6ab4b0-e686-4366-8de6-0974f5241060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from backbone weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded HuggingFaceTB/SmolLM2-1.7B weights from safetensors. Model has 1,711,376,384 parameters.\n",
            "AudioProcessor_from_HF initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
            "  Target feature frames from cfg: 1500\n",
            "  Using model sampling rate: 16000, hop_length: 160, n_fft: 400\n",
            "  Calculated max raw audio samples for processor: 240240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage1 Epoch 1:   0%|          | 0/68 [00:13<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "linear(): argument 'input' (position 1) must be Tensor, not BaseModelOutput",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1145085061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# trained_model = train(train_cfg, alm_cfg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstage1_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step1_alignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malm_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-11-2957447379.py\u001b[0m in \u001b[0;36mtrain_step1_alignment\u001b[0;34m(train_cfg, alm_cfg)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0maudio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mprojected_audio_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# 2. 文本編碼 (!!! 關鍵修正 !!!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nanoVLM_From_Huggingface/models/modality_projector.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# 特征提取\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# 自适应池化到目标长度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not BaseModelOutput"
          ]
        }
      ],
      "source": [
        "\n",
        "alm_cfg = ALMConfig()\n",
        "train_cfg = TrainConfig()\n",
        "\n",
        "# trained_model = train(train_cfg, alm_cfg)\n",
        "\n",
        "stage1_model = train_step1_alignment(train_cfg, alm_cfg)\n",
        "stage1_model.save_pretrained(\"/content/\")\n",
        "\n",
        "stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model)\n",
        "stage2_model.save_pretrained(\"/content/\")\n",
        "\n",
        "final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model)\n",
        "final_model.save_pretrained(\"/content/\")\n",
        "\n",
        "\n",
        "# stage1_model, stage2_model, final_model = train_three_stages(train_cfg, alm_cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VHmI2cQ28RAj",
      "metadata": {
        "id": "VHmI2cQ28RAj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mZWoCjkGGfMQ",
      "metadata": {
        "id": "mZWoCjkGGfMQ"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kjOsHkjpCoYp",
      "metadata": {
        "id": "kjOsHkjpCoYp"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/nanoALM/output_txt1.wav /content/output_txt1.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt2.wav /content/output_txt2.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt3.wav /content/output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dM8znZPTSj",
      "metadata": {
        "id": "07dM8znZPTSj"
      },
      "outputs": [],
      "source": [
        "!cp ../model.safetensors /content/drive/MyDrive/nanoALM/model.safetensors\n",
        "!cp ../config.json /content/drive/MyDrive/nanoALM/config.json\n",
        "!cp ./model.safetensors /content/drive/MyDrive/nanoALM\n",
        "!cp ./config.json /content/drive/MyDrive/nanoALM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSbBheB3HGns",
      "metadata": {
        "collapsed": true,
        "id": "RSbBheB3HGns"
      },
      "outputs": [],
      "source": [
        "# final_model.save_pretrained(\"/content/\")\n",
        "!python generate.py --checkpoint ../ --audio ../output_txt1.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FN6ra31lP8QM",
      "metadata": {
        "collapsed": true,
        "id": "FN6ra31lP8QM"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt2.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo_p1E3xP8gd",
      "metadata": {
        "collapsed": true,
        "id": "eo_p1E3xP8gd"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0cMUB4lb4zR",
      "metadata": {
        "id": "T0cMUB4lb4zR"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
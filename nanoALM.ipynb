{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a ALM in Google Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "d5dbe3b6-2159-4e0f-f740-a32d92e964ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nanoVLM_From_Huggingface' already exists and is not an empty directory.\n",
            "/content/nanoVLM_From_Huggingface\n",
            "assets\t\t\tdebug_tokenizer_dataset_compatibility.py\n",
            "benchmark-inference.py\tgenerate.py\n",
            "benchmark_suite.py\tmeasure_vram.py\n",
            "checkpoints\t\tmodels\n",
            "compare1.py\t\tnanoALM.ipynb\n",
            "compare2.py\t\tREADME.md\n",
            "data\t\t\ttrain.py\n",
            "debug_func.py\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoALM'):\n",
        "    !git clone https://github.com/LWL220184016/nanoVLM_From_Huggingface.git\n",
        "%cd nanoVLM_From_Huggingface/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "54bc8463",
      "metadata": {
        "id": "54bc8463"
      },
      "outputs": [],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bcw8qQqoOSR7",
        "outputId": "d0b6e4d4-45d7-4c3a-c3a6-6a984cab0101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Installing collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.3.0\n"
          ]
        }
      ],
      "source": [
        "# If you get an \"Error\" from pip's dependency resolver but the cell complets fine, this is not an issue, you can continue :)\n",
        "!pip -q install torch\n",
        "!pip -q install gcsfs\n",
        "!pip -q install tqdm\n",
        "!pip -q install huggingface_hub\n",
        "!pip -q install librosa\n",
        "!pip install --upgrade datasets\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoALM\"\n",
        "# hf_model_name = \"YOUR_HF_USER_NAME/nanoALM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "5d315cb1-5eb6-4420-fe87-1eeab41e4535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# nanoALM Imports (please check out the implementations in detail, that's where all the interessting stuff is!)\n",
        "from data.collators import AudioQACollator, SAVEECollator\n",
        "from data.datasets import SAVEEDataset, AudioQADataset\n",
        "from data.processors import get_audio_processor\n",
        "from data.processors import get_tokenizer\n",
        "from models.audio_language_model import AudioLanguageModel\n",
        "import models.utils as utils\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "#Otherwise, the tokenizer will through a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "trained_model = None\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, alm_cfg):\n",
        "    # Create datasets\n",
        "    audio_processor = get_audio_processor(alm_cfg)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # text = \"splitting datasets, disable in get_dataloaders function\"\n",
        "    # print(f\"\\n\\033[38;5;05m{text}05m\\033[0m\")\n",
        "    # Load and combine all training datasets\n",
        "    combined_train_data = []\n",
        "    for dataset_name in train_cfg.train_dataset_name:\n",
        "        train_ds = load_dataset(\n",
        "        path = train_cfg.train_dataset_path,\n",
        "        name = dataset_name,\n",
        "    )\n",
        "        combined_train_data.append(train_ds['train'])\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    test_ds = load_dataset(train_cfg.test_dataset_path)\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    train_dataset = AudioQADataset(train_ds.select(range(train_size)), tokenizer, audio_processor)\n",
        "    val_dataset = AudioQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, audio_processor)\n",
        "    test_dataset = SAVEEDataset(test_ds, tokenizer, audio_processor)\n",
        "\n",
        "    # Create collators\n",
        "    aqa_collator = AudioQACollator(tokenizer, alm_cfg.lm_max_length)\n",
        "    savee_collator = SAVEECollator(tokenizer)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=train_cfg.savee_batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=savee_collator,\n",
        "        pin_memory=True,\n",
        "        )\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D7NIuEDuOuuJ",
      "metadata": {
        "id": "D7NIuEDuOuuJ"
      },
      "source": [
        "### Prepare the testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9fnh6wOlOzat",
      "metadata": {
        "id": "9fnh6wOlOzat"
      },
      "outputs": [],
      "source": [
        "def test_savee(model, tokenizer, test_loader, device):\n",
        "    total_examples = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            audio = batch['audios'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            gen = model.generate(input_ids, audio, attention_mask)\n",
        "            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "            is_correct = utils.check_multiple_choice_with_regex(model_output, correct_answer)\n",
        "\n",
        "            total_examples += len(is_correct)\n",
        "            if is_correct:\n",
        "                correct_predictions += sum(is_correct)\n",
        "    accuracy = correct_predictions / total_examples if total_examples > 0 else 0\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe359194",
      "metadata": {
        "id": "fe359194"
      },
      "source": [
        "### Add debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5d7b7e46",
      "metadata": {
        "id": "5d7b7e46"
      },
      "outputs": [],
      "source": [
        "# 在训练开始前添加这个检查函数\n",
        "def debug_model_dimensions(model, input_ids, audio):\n",
        "    \"\"\"调试模型各层的维度\"\"\"\n",
        "    print(\"=== Model Dimension Debug ===\")\n",
        "\n",
        "    # 检查音频编码器\n",
        "    audio_features = model.audio_encoder(audio)\n",
        "    print(f\"Audio features shape: {audio_features.shape}\")\n",
        "\n",
        "    # 检查模态投影器\n",
        "    audio_embeds = model.MP(audio_features)\n",
        "    print(f\"Audio embeds shape: {audio_embeds.shape}\")\n",
        "\n",
        "    # 检查文本嵌入\n",
        "    text_embeds = model.decoder.token_embedding(input_ids)\n",
        "    print(f\"Text embeds shape: {text_embeds.shape}\")\n",
        "\n",
        "    # 检查拼接后的嵌入\n",
        "    inputs_embeds = torch.cat([audio_embeds, text_embeds], dim=1)\n",
        "    print(f\"Combined embeds shape: {inputs_embeds.shape}\")\n",
        "\n",
        "    # 检查语言模型输出\n",
        "    logits = model.decoder(inputs_embeds)\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "    print(f\"Vocab size (last dim): {logits.shape[-1]}\")\n",
        "\n",
        "    # 检查语言模型配置\n",
        "    print(f\"LM vocab size config: {model.cfg.lm_vocab_size}\")\n",
        "    print(f\"Decoder vocab size: {getattr(model.decoder, 'vocab_size', 'Not found')}\")\n",
        "\n",
        "    return logits.shape[-1]\n",
        "\n",
        "# 在训练循环开始前调用\n",
        "# vocab_size = debug_model_dimensions(model, input_ids, audios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2d48d0df",
      "metadata": {
        "id": "2d48d0df"
      },
      "outputs": [],
      "source": [
        "def debug_training_step(model, input_ids, audios, attention_mask, labels):\n",
        "    \"\"\"调试训练步骤\"\"\"\n",
        "    # 添加这些调试行：\n",
        "    print(f\"Batch debug - input_ids shape: {input_ids.shape}, max: {input_ids.max().item()}\")\n",
        "    print(f\"Batch debug - labels shape: {labels.shape}, max: {labels.max().item()}\")\n",
        "    print(f\"Batch debug - Model vocab config: {model.cfg.lm_vocab_size}\")\n",
        "\n",
        "    # 检查decoder的实际vocab_size\n",
        "    if hasattr(model.decoder, 'head') and hasattr(model.decoder.head, 'out_features'):\n",
        "        print(f\"Decoder head in_features: {model.decoder.head.in_features}\")\n",
        "        print(f\"Decoder head out_features: {model.decoder.head.out_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "KxOtMU5zPD-4",
      "metadata": {
        "id": "KxOtMU5zPD-4"
      },
      "outputs": [],
      "source": [
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def train(train_cfg, alm_cfg):\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # Initialize model\n",
        "    if train_cfg.resume_from_alm_checkpoint:\n",
        "        model = AudioLanguageModel.from_pretrained(alm_cfg.alm_checkpoint_path)\n",
        "    else:\n",
        "        model = AudioLanguageModel(alm_cfg)\n",
        "\n",
        "    print(f\"nanoALM initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    print(f\"Training summary: {len(train_loader.dataset)} samples, {len(train_loader)} batches/epoch, batch size {train_cfg.batch_size}\")\n",
        "\n",
        "    # Define optimizer groups\n",
        "    param_groups = [{'params': model.MP.parameters(), 'lr': train_cfg.lr_mp},\n",
        "                    {'params': list(model.decoder.parameters()) + list(model.audio_encoder.parameters()), 'lr': train_cfg.lr_backbones}]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    epoch_times = []\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    best_accuracy = 0\n",
        "    global_step = 0\n",
        "    for epoch in range(train_cfg.epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_tokens_processed = 0\n",
        "\n",
        "        for batch in tqdm(train_loader):\n",
        "            batch_start_time = time.time()\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            # debug_model_dimensions(model, input_ids, audios)  # Debug model dimensions with dummy data\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16): # Mixed precision training\n",
        "                # debug_training_step(model, input_ids, audios, attention_mask, labels)  # Debug training step\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            adj_lr_mp = get_lr(global_step, train_cfg.lr_mp, len(train_loader) * train_cfg.epochs)\n",
        "            adj_lr_backbones = get_lr(global_step, train_cfg.lr_backbones, len(train_loader) * train_cfg.epochs)\n",
        "            optimizer.param_groups[0]['lr'] = adj_lr_mp\n",
        "            optimizer.param_groups[1]['lr'] = adj_lr_backbones\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            num_tokens = torch.sum(attention_mask).item()\n",
        "            # 修改音頻token計算：根據實際的音頻處理方式\n",
        "            audio_tokens = audios.shape[0] * alm_cfg.mp_target_length  # 使用配置的目標長度\n",
        "            num_tokens += audio_tokens\n",
        "            total_tokens_processed += num_tokens\n",
        "\n",
        "            batch_end_time = time.time()\n",
        "            batch_duration = batch_end_time - batch_start_time\n",
        "            tokens_per_second = num_tokens / batch_duration\n",
        "\n",
        "            if global_step % 5 == 0:\n",
        "                model.eval()\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory\n",
        "                with torch.no_grad():\n",
        "                    total_val_loss = 0\n",
        "                    for batch in val_loader:\n",
        "                        audios = batch[\"audio\"].to(device)\n",
        "                        input_ids = batch[\"input_ids\"].to(device)\n",
        "                        labels = batch[\"labels\"].to(device)\n",
        "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "                        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                            # debug_training_step(model, input_ids, audios, attention_mask, labels)  # Debug training step\n",
        "                            _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "                        total_val_loss += loss.item()\n",
        "                    avg_val_loss = total_val_loss / len(val_loader)\n",
        "                    val_losses.append(avg_val_loss)\n",
        "                    val_plot_steps.append(global_step)\n",
        "                epoch_accuracy = 0\n",
        "                if train_cfg.eval_in_epochs:\n",
        "                    epoch_accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "                    if epoch_accuracy > best_accuracy:\n",
        "                      best_accuracy = epoch_accuracy\n",
        "                      model.save_pretrained(save_directory=alm_cfg.alm_checkpoint_path)\n",
        "                    print(f\"\\nStep: {global_step}, Loss: {batch_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Tokens/s: {tokens_per_second:.2f}, Accuracy: {epoch_accuracy:.4f}\")\n",
        "                model.train()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        epoch_tokens_per_second = total_tokens_processed / epoch_duration\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{train_cfg.epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\")\n",
        "\n",
        "    # Summary Statistics\n",
        "    if not train_cfg.eval_in_epochs:\n",
        "        model.save_pretrained(save_directory=alm_cfg.alm_checkpoint_path)\n",
        "    try:\n",
        "        model.push_to_hub(hf_model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error pushing model to hub: {e}\")\n",
        "\n",
        "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
        "    total_training_time = sum(epoch_times)\n",
        "    total_samples_processed = len(train_loader.dataset) * train_cfg.epochs\n",
        "    avg_time_per_sample = total_training_time / total_samples_processed\n",
        "    print(f\"Average time per epoch: {avg_epoch_time:.2f}s\")\n",
        "    print(f\"Average time per sample: {avg_time_per_sample:.4f}s\")\n",
        "\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "    # With this code you can test the accuracy of the model on the SAVEE dataset\n",
        "    # But if you only train with few samples, the accuracy will be very low\n",
        "    # print(\"Testing SAVEE Accuracy:\")\n",
        "    # accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "    # print(f\"SAVEE Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4HmsEPNQZbh",
      "metadata": {
        "id": "d4HmsEPNQZbh"
      },
      "source": [
        "### Prepare the Configs\n",
        "Instead of using the config.py file in the repo (which was created to run on one H100), we will create our config here to play around with the parameters easier and adapt them to colabs capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h8FlqtizQdO-",
      "metadata": {
        "id": "h8FlqtizQdO-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ALMConfig:\n",
        "    audio_hidden_dim: int = 1280\n",
        "    audio_inter_dim: int = 4 * audio_hidden_dim\n",
        "    audio_patch_size: int = 16  # 音频patch大小（时间步数）\n",
        "    audio_n_heads: int = 12\n",
        "    audio_dropout: float = 0.0\n",
        "    audio_n_blocks: int = 12\n",
        "    audio_ln_eps: float = 1e-6\n",
        "    # audio_model_type: str = 'custom_audio_transformer'\n",
        "    # 如果使用 nvidia/parakeet-tdt-0.6b-v2, 以上參數將不會發生作用\n",
        "    # audio_model_type: str = 'nvidia/parakeet-tdt-0.6b-v2' # asr model for encoder from huggingface\n",
        "    audio_model_type: str = 'openai/whisper-large-v3' # asr model for encoder from huggingface\n",
        "\n",
        "    # 音频处理相关参数\n",
        "    audio_sample_rate: int = 16000  # 采样率\n",
        "    audio_n_fft: int = 400  # FFT窗口大小\n",
        "    audio_hop_length: int = 160  # 跳跃长度\n",
        "    audio_n_mels: int = 80  # 梅尔滤波器数量\n",
        "    audio_max_length: int = 3000  # 最大时间步数\n",
        "\n",
        "    lm_hidden_dim: int = 576\n",
        "    lm_inter_dim: int = 1536\n",
        "    lm_rms_eps: float = 1e-5\n",
        "    lm_re_base: int = 100000\n",
        "    lm_max_position_embeddings: int = 8192\n",
        "    lm_vocab_size: int = 49152\n",
        "    lm_n_heads: int = 9\n",
        "    lm_n_kv_heads: int = 3\n",
        "    lm_dropout: float = 0.0\n",
        "    lm_n_blocks: int = 30\n",
        "    lm_attn_scaling: float = 1.0\n",
        "    lm_eos_token_id: int = 0\n",
        "    lm_use_tokens: bool = False\n",
        "    lm_tie_weights: bool = True\n",
        "    lm_model_type: str = 'HuggingFaceTB/SmolLM2-135M'\n",
        "    lm_tokenizer: str = 'HuggingFaceTB/cosmo2-tokenizer'\n",
        "\n",
        "    # 模態投影器配置\n",
        "    mp_projection_type: str = 'adaptive_pool'\n",
        "    mp_target_length: int = 50\n",
        "    mp_use_position_aware: bool = True\n",
        "\n",
        "    # 計算語言模型最大長度\n",
        "    lm_max_length: int = 128 - 50  # 總長度 - 音頻token長度\n",
        "\n",
        "    # ALM特定配置\n",
        "    alm_load_backbone_weights: bool = True\n",
        "    alm_checkpoint_path: str = 'checkpoints'\n",
        "    alm_name: str = 'nanoALM-222M'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    lr_mp: float = 1e-5\n",
        "    lr_backbones: float = 5e-7\n",
        "    val_ratio: float = 0.2\n",
        "    compile: bool = False\n",
        "    data_cutoff_idx: int = 1024 # Let's only use a small subset of the data at first, otherwise it takes very long to see anything :D\n",
        "    batch_size: int = 12\n",
        "    savee_batch_size: int = 12\n",
        "    epochs: int = 20\n",
        "    eval_in_epochs: bool = False # Deactivating this in colab, because it would evaluate 1500 samples of SAVEE every time otherwise\n",
        "    resume_from_alm_checkpoint: bool = False # Indicate if the training should be resumed from a checkpoint of the whole ALM or you want to start from scratch\n",
        "\n",
        "    # train_dataset_path: str = 'AbstractTTS/IEMOCAP'\n",
        "    # train_dataset_name: tuple[str, ...] = ('default', ) #All options; (\"ai2d\", \"aokvqa\", \"chart2text\", \"chartqa\", \"clevr\", \"cocoqa\", \"datikz\", \"diagram_image_to_text\", \"docvqa\", \"dvqa\", \"figureqa\", \"finqa\", \"geomverse\", \"hateful_memes\", \"hitab\", \"iam\", \"iconqa\", \"infographic_vqa\", \"intergps\", \"localized_narratives\", \"mapqa\", \"multihiertt\", \"ocrvqa\", \"plotqa\", \"raven\", \"rendered_text\", \"robut_sqa\", \"robut_wikisql\", \"robut_wtq\", \"scienceqa\", \"screen2words\", \"st_vqa\", \"tabmwp\", \"tallyqa\", \"tat_qa\", \"textcaps\", \"textvqa\", \"tqa\", \"vistext\", \"visual7w\", \"visualmrc\", \"vqarad\", \"vqav2\", \"vsr\", \"websight\") # \"clevr_math\", \"okvqa\", \"spot_the_diff\", \"nlvr2\", \"mimic_cgd\",\n",
        "\n",
        "    # train_dataset_path: str = 'speechbrain/LoquaciousSet'\n",
        "    # train_dataset_name: tuple[str, ...] = ('medium', ) # small, medium\n",
        "\n",
        "    train_dataset_path: str = 'MLCommons/peoples_speech'\n",
        "    train_dataset_name: tuple[str, ...] = ('clean_sa', ) # small, medium\n",
        "    test_dataset_path: str = \"AbstractTTS/SAVEE\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "### Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "BXUaUEUcJCp2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXUaUEUcJCp2",
        "outputId": "c4c3c214-5c82-4709-97d5-5eb780cec4cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory 'checkpoints' already exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# 要創建的目錄路徑\n",
        "dir_name = ALMConfig.alm_checkpoint_path\n",
        "\n",
        "try:\n",
        "    os.mkdir(dir_name)\n",
        "    print(f\"Directory '{dir_name}' created successfully.\")\n",
        "except FileExistsError:\n",
        "    print(f\"Directory '{dir_name}' already exists.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Parent directory does not exist for '{dir_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9MlFpXQFSNdx",
        "outputId": "e70b7eb3-18f9-4544-df10-bd99076717e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AudioProcessor initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>, max_length: 3000\n",
            "Loading from backbone weights\n",
            "Successfully loaded HuggingFaceTB/SmolLM2-135M weights from safetensors. Model has 134,515,008 parameters.\n",
            "nanoALM initialized with 1,678,447,936 parameters\n",
            "Training summary: 820 samples, 68 batches/epoch, batch size 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/68 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputsinputs  {'input_features': tensor([[[-0.0634, -0.2617, -0.0124,  ..., -1.2219, -1.2219, -1.2219],\n",
            "         [ 0.0342, -0.1642,  0.0852,  ..., -1.2219, -1.2219, -1.2219],\n",
            "         [ 0.1202, -0.0546, -0.0564,  ..., -1.2219, -1.2219, -1.2219],\n",
            "         ...,\n",
            "         [-1.2219, -1.2219, -1.2219,  ..., -1.2219, -1.2219, -1.2219],\n",
            "         [-1.2219, -1.2219, -1.2219,  ..., -1.2219, -1.2219, -1.2219],\n",
            "         [-1.1947, -1.2219, -1.2219,  ..., -1.2219, -1.2219, -1.2219]]])}{'input_features': tensor([[[-0.0516, -0.0733,  0.0321,  ..., -1.0783, -1.0783, -1.0783],\n",
            "         [ 0.0460,  0.0243,  0.1297,  ..., -1.0783, -1.0783, -1.0783],\n",
            "         [-0.2745, -0.1145,  0.0082,  ..., -1.0783, -1.0783, -1.0783],\n",
            "         ...,\n",
            "         [-1.0783, -1.0783, -1.0783,  ..., -1.0783, -1.0783, -1.0783],\n",
            "         [-1.0783, -1.0783, -1.0783,  ..., -1.0783, -1.0783, -1.0783],\n",
            "         [-1.0783, -1.0783, -1.0783,  ..., -1.0783, -1.0783, -1.0783]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.1684,  0.0470,  0.0204,  ..., -0.9267, -0.9267, -0.9267],\n",
            "         [-0.0708,  0.1446,  0.1179,  ..., -0.9267, -0.9267, -0.9267],\n",
            "         [-0.0226,  0.1125, -0.4184,  ..., -0.9267, -0.9267, -0.9267],\n",
            "         ...,\n",
            "         [-0.9267, -0.9267, -0.9267,  ..., -0.9267, -0.9267, -0.9267],\n",
            "         [-0.9267, -0.9267, -0.9267,  ..., -0.9267, -0.9267, -0.9267],\n",
            "         [-0.9267, -0.9267, -0.9267,  ..., -0.9267, -0.9267, -0.9267]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.1391,  0.2412,  0.2153,  ..., -0.9263, -0.9263, -0.9263],\n",
            "         [-0.0415,  0.3387,  0.3129,  ..., -0.9263, -0.9263, -0.9263],\n",
            "         [ 0.1473,  0.3004, -0.0265,  ..., -0.9263, -0.9263, -0.9263],\n",
            "         ...,\n",
            "         [-0.9263, -0.9263, -0.9263,  ..., -0.9263, -0.9263, -0.9263],\n",
            "         [-0.9263, -0.9263, -0.9263,  ..., -0.9263, -0.9263, -0.9263],\n",
            "         [-0.9263, -0.9263, -0.9263,  ..., -0.9263, -0.9263, -0.9263]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputsinputs  {'input_features': tensor([[[ 0.1196,  0.2162,  0.0765,  ..., -0.7851, -0.7851, -0.7851],\n",
            "         [ 0.2172,  0.3137,  0.1741,  ..., -0.7851, -0.7851, -0.7851],\n",
            "         [ 0.2022,  0.3067, -0.0468,  ..., -0.7851, -0.7851, -0.7851],\n",
            "         ...,\n",
            "         [-0.7851, -0.7851, -0.7851,  ..., -0.7851, -0.7851, -0.7851],\n",
            "         [-0.7851, -0.7851, -0.7851,  ..., -0.7851, -0.7851, -0.7851],\n",
            "         [-0.7851, -0.7851, -0.7851,  ..., -0.7851, -0.7851, -0.7851]]])}{'input_features': tensor([[[-0.5306, -0.0219,  0.0929,  ..., -1.3471, -1.3471, -1.3471],\n",
            "         [-0.4330,  0.0757,  0.1904,  ..., -1.3471, -1.3471, -1.3471],\n",
            "         [-0.0389, -0.1364,  0.0267,  ..., -1.3471, -1.3471, -1.3471],\n",
            "         ...,\n",
            "         [-1.3471, -1.3471, -1.3471,  ..., -1.3471, -1.3471, -1.3471],\n",
            "         [-1.3471, -1.3471, -1.3471,  ..., -1.3471, -1.3471, -1.3471],\n",
            "         [-1.3471, -1.3471, -1.3471,  ..., -1.3471, -1.3471, -1.3471]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1046,  0.3002,  0.1869,  ..., -1.2874, -1.2874, -1.2874],\n",
            "         [ 0.2022,  0.3977,  0.2844,  ..., -1.2874, -1.2874, -1.2874],\n",
            "         [ 0.0941,  0.3103,  0.0680,  ..., -1.2874, -1.2874, -1.2874],\n",
            "         ...,\n",
            "         [-1.2240, -1.2874, -1.2874,  ..., -1.2874, -1.2874, -1.2874],\n",
            "         [-1.2874, -1.2874, -1.2874,  ..., -1.2874, -1.2874, -1.2874],\n",
            "         [-1.2874, -1.2874, -1.2874,  ..., -1.2874, -1.2874, -1.2874]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])inputs\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            " {'input_features': tensor([[[ 0.0546, -0.0273, -0.1964,  ..., -0.7312, -0.7312, -0.7312],\n",
            "         [ 0.1522,  0.0703, -0.0988,  ..., -0.7312, -0.7312, -0.7312],\n",
            "         [ 0.1448, -0.1468, -0.3802,  ..., -0.7312, -0.7312, -0.7312],\n",
            "         ...,\n",
            "         [-0.2707, -0.7312, -0.7312,  ..., -0.7312, -0.7312, -0.7312],\n",
            "         [-0.2768, -0.7312, -0.7312,  ..., -0.7312, -0.7312, -0.7312],\n",
            "         [-0.2777, -0.7312, -0.7312,  ..., -0.7312, -0.7312, -0.7312]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])inputs {'input_features': tensor([[[-0.1032,  0.1388, -0.1288,  ..., -0.6650, -0.6650, -0.6650],\n",
            "         [-0.0057,  0.2363, -0.0313,  ..., -0.6650, -0.6650, -0.6650],\n",
            "         [-0.3020,  0.2557,  0.1613,  ..., -0.6650, -0.6650, -0.6650],\n",
            "         ...,\n",
            "         [-0.6650, -0.6650, -0.6650,  ..., -0.6650, -0.6650, -0.6650],\n",
            "         [-0.6650, -0.6650, -0.6650,  ..., -0.6650, -0.6650, -0.6650],\n",
            "         [-0.6650, -0.6650, -0.6650,  ..., -0.6650, -0.6650, -0.6650]]])}\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs inputs {'input_features': tensor([[[ 0.0624,  0.2306,  0.1651,  ..., -0.3206, -0.3206, -0.3206],\n",
            "         [ 0.1600,  0.3282,  0.2627,  ..., -0.3206, -0.3206, -0.3206],\n",
            "         [-0.3206,  0.3182,  0.0850,  ..., -0.3206, -0.3206, -0.3206],\n",
            "         ...,\n",
            "         [-0.3206, -0.3206, -0.3206,  ..., -0.3206, -0.3206, -0.3206],\n",
            "         [-0.3206, -0.3206, -0.3206,  ..., -0.3206, -0.3206, -0.3206],\n",
            "         [-0.3206, -0.3206, -0.3206,  ..., -0.3206, -0.3206, -0.3206]]])}{'input_features': tensor([[[-0.0058,  0.0343, -0.0523,  ..., -0.9327, -0.9327, -0.9327],\n",
            "         [ 0.0918,  0.1318,  0.0453,  ..., -0.9327, -0.9327, -0.9327],\n",
            "         [ 0.0723,  0.0224, -0.0864,  ..., -0.9327, -0.9327, -0.9327],\n",
            "         ...,\n",
            "         [-0.9327, -0.9327, -0.9327,  ..., -0.9327, -0.9327, -0.9327],\n",
            "         [-0.9327, -0.9327, -0.9327,  ..., -0.9327, -0.9327, -0.9327],\n",
            "         [-0.9327, -0.9327, -0.9327,  ..., -0.9327, -0.9327, -0.9327]]])}\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "\n",
            "Processed audio shape2: torch.Size([128, 3000])Processed audio shape2: torch.Size([128, 3000])\n",
            "\n",
            "inputs {'input_features': tensor([[[-0.4100,  0.1265,  0.1354,  ..., -1.3971, -1.3971, -1.3971],\n",
            "         [-0.3124,  0.2241,  0.2329,  ..., -1.3971, -1.3971, -1.3971],\n",
            "         [ 0.1267, -0.1960,  0.0374,  ..., -1.3971, -1.3971, -1.3971],\n",
            "         ...,\n",
            "         [-1.3164, -1.3770, -1.3971,  ..., -1.3971, -1.3971, -1.3971],\n",
            "         [-1.3635, -1.3971, -1.2939,  ..., -1.3971, -1.3971, -1.3971],\n",
            "         [-1.2822, -1.3374, -1.3521,  ..., -1.3971, -1.3971, -1.3971]]])}\n",
            "inputsProcessed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000]) \n",
            "{'input_features': tensor([[[ 0.0095,  0.0193,  0.0820,  ..., -1.2126, -1.2126, -1.2126],\n",
            "         [ 0.1071,  0.1169,  0.1796,  ..., -1.2126, -1.2126, -1.2126],\n",
            "         [-0.1587,  0.0358,  0.0309,  ..., -1.2126, -1.2126, -1.2126],\n",
            "         ...,\n",
            "         [-1.2126, -1.2126, -1.2126,  ..., -1.2126, -1.2126, -1.2126],\n",
            "         [-1.2126, -1.2126, -1.2126,  ..., -1.2126, -1.2126, -1.2126],\n",
            "         [-1.2126, -1.2126, -1.2126,  ..., -1.2126, -1.2126, -1.2126]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.1885, -0.0207, -0.1717,  ..., -0.7170, -0.7170, -0.7170],\n",
            "         [-0.0909,  0.0768, -0.0741,  ..., -0.7170, -0.7170, -0.7170],\n",
            "         [-0.2173, -0.1045, -0.0979,  ..., -0.7170, -0.7170, -0.7170],\n",
            "         ...,\n",
            "         [-0.7170, -0.7170, -0.7170,  ..., -0.7170, -0.7170, -0.7170],\n",
            "         [-0.7170, -0.7170, -0.7170,  ..., -0.7170, -0.7170, -0.7170],\n",
            "         [-0.7170, -0.7170, -0.7170,  ..., -0.7170, -0.7170, -0.7170]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1815,  0.1037,  0.1362,  ..., -1.1131, -1.1131, -1.1131],\n",
            "         [ 0.2791,  0.2013,  0.2338,  ..., -1.1131, -1.1131, -1.1131],\n",
            "         [ 0.2925,  0.1669,  0.0317,  ..., -1.1131, -1.1131, -1.1131],\n",
            "         ...,\n",
            "         [-1.0372, -1.1131, -1.1131,  ..., -1.1131, -1.1131, -1.1131],\n",
            "         [-1.0293, -1.1131, -1.1131,  ..., -1.1131, -1.1131, -1.1131],\n",
            "         [-1.0140, -1.1131, -1.1131,  ..., -1.1131, -1.1131, -1.1131]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "inputsProcessed audio shape2: torch.Size([128, 3000]) {'input_features': tensor([[[ 0.1688,  0.2114,  0.0891,  ..., -1.2128, -1.2128, -1.2128],\n",
            "         [ 0.2664,  0.3090,  0.1866,  ..., -1.2128, -1.2128, -1.2128],\n",
            "         [ 0.0031,  0.1022,  0.1641,  ..., -1.2128, -1.2128, -1.2128],\n",
            "         ...,\n",
            "         [-1.2128, -1.2128, -1.2128,  ..., -1.2128, -1.2128, -1.2128],\n",
            "         [-1.2128, -1.2128, -1.2128,  ..., -1.2128, -1.2128, -1.2128],\n",
            "         [-1.2128, -1.2128, -1.2128,  ..., -1.2128, -1.2128, -1.2128]]])}\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.4145, -0.0459,  0.1189,  ..., -0.9922, -0.9922, -0.9922],\n",
            "         [-0.3169,  0.0516,  0.2165,  ..., -0.9922, -0.9922, -0.9922],\n",
            "         [-0.2773, -0.0498,  0.0173,  ..., -0.9922, -0.9922, -0.9922],\n",
            "         ...,\n",
            "         [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
            "         [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
            "         [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])inputs \n",
            "Processed audio shape2: torch.Size([128, 3000]){'input_features': tensor([[[ 0.1376,  0.2304,  0.0766,  ..., -0.8296, -0.8296, -0.8296],\n",
            "         [ 0.2352,  0.3280,  0.1742,  ..., -0.8296, -0.8296, -0.8296],\n",
            "         [ 0.0754,  0.3658,  0.3277,  ..., -0.8296, -0.8296, -0.8296],\n",
            "         ...,\n",
            "         [-0.8114, -0.8296, -0.8296,  ..., -0.8296, -0.8296, -0.8296],\n",
            "         [-0.8211, -0.8296, -0.8296,  ..., -0.8296, -0.8296, -0.8296],\n",
            "         [-0.8296, -0.8296, -0.8296,  ..., -0.8296, -0.8296, -0.8296]]])}\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 4.8877e-02,  7.4524e-04, -9.7409e-02,  ..., -8.9303e-01,\n",
            "          -8.9303e-01, -8.9303e-01],\n",
            "         [ 1.4644e-01,  9.8310e-02,  1.5515e-04,  ..., -8.9303e-01,\n",
            "          -8.9303e-01, -8.9303e-01],\n",
            "         [ 3.3965e-01,  2.7456e-01,  8.2224e-02,  ..., -8.9303e-01,\n",
            "          -8.9303e-01, -8.9303e-01],\n",
            "         ...,\n",
            "         [-8.9303e-01, -8.9303e-01, -8.9303e-01,  ..., -8.9303e-01,\n",
            "          -8.9303e-01, -8.9303e-01],\n",
            "         [-8.9303e-01, -8.9303e-01, -8.9303e-01,  ..., -8.9303e-01,\n",
            "          -8.9303e-01, -8.9303e-01],\n",
            "         [-8.9303e-01, -8.9303e-01, -8.9303e-01,  ..., -8.9303e-01,\n",
            "          -8.9303e-01, -8.9303e-01]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])inputs\n",
            "Processed audio shape2: torch.Size([128, 3000]) {'input_features': tensor([[[-0.2711, -0.0217,  0.0199,  ..., -1.0961, -1.0961, -1.0961],\n",
            "         [-0.1735,  0.0759,  0.1174,  ..., -1.0961, -1.0961, -1.0961],\n",
            "         [-0.3091, -0.0375,  0.0040,  ..., -1.0961, -1.0961, -1.0961],\n",
            "         ...,\n",
            "         [-0.9842, -1.0961, -1.0961,  ..., -1.0961, -1.0961, -1.0961],\n",
            "         [-0.9828, -1.0961, -1.0961,  ..., -1.0961, -1.0961, -1.0961],\n",
            "         [-0.9815, -1.0961, -1.0961,  ..., -1.0961, -1.0961, -1.0961]]])}\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.1051, -0.1347, -0.2714,  ..., -1.0930, -1.0930, -1.0930],\n",
            "         [-0.0076, -0.0371, -0.1738,  ..., -1.0930, -1.0930, -1.0930],\n",
            "         [-0.1142,  0.0236, -0.1532,  ..., -1.0930, -1.0930, -1.0930],\n",
            "         ...,\n",
            "         [-1.0930, -1.0930, -1.0930,  ..., -1.0930, -1.0930, -1.0930],\n",
            "         [-1.0930, -1.0930, -1.0930,  ..., -1.0930, -1.0930, -1.0930],\n",
            "         [-1.0930, -1.0930, -1.0930,  ..., -1.0930, -1.0930, -1.0930]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])inputs\n",
            " {'input_features': tensor([[[ 0.2873, -0.0227, -0.0761,  ..., -0.8159, -0.8159, -0.8159],\n",
            "         [ 0.3849,  0.0748,  0.0215,  ..., -0.8159, -0.8159, -0.8159],\n",
            "         [ 0.4205, -0.0506, -0.1459,  ..., -0.8159, -0.8159, -0.8159],\n",
            "         ...,\n",
            "         [-0.8159, -0.8159, -0.8159,  ..., -0.8159, -0.8159, -0.8159],\n",
            "         [-0.8159, -0.8159, -0.8159,  ..., -0.8159, -0.8159, -0.8159],\n",
            "         [-0.8159, -0.8159, -0.8159,  ..., -0.8159, -0.8159, -0.8159]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])inputs \n",
            "{'input_features': tensor([[[ 0.1063,  0.2620,  0.2811,  ..., -0.7408, -0.7408, -0.7408],\n",
            "         [ 0.2039,  0.3596,  0.3786,  ..., -0.7408, -0.7408, -0.7408],\n",
            "         [ 0.3367,  0.2882, -0.0491,  ..., -0.7408, -0.7408, -0.7408],\n",
            "         ...,\n",
            "         [-0.7408, -0.7408, -0.7408,  ..., -0.7408, -0.7408, -0.7408],\n",
            "         [-0.7408, -0.7408, -0.7408,  ..., -0.7408, -0.7408, -0.7408],\n",
            "         [-0.7408, -0.7408, -0.7408,  ..., -0.7408, -0.7408, -0.7408]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 1.1752e-02,  3.1482e-02, -5.3202e-02,  ..., -1.1878e+00,\n",
            "          -1.1878e+00, -1.1878e+00],\n",
            "         [ 1.0932e-01,  1.2905e-01,  4.4362e-02,  ..., -1.1878e+00,\n",
            "          -1.1878e+00, -1.1878e+00],\n",
            "         [-3.5078e-01,  5.2792e-04, -4.2695e-01,  ..., -1.1878e+00,\n",
            "          -1.1878e+00, -1.1878e+00],\n",
            "         ...,\n",
            "         [-1.1656e+00, -1.1878e+00, -1.1878e+00,  ..., -1.1878e+00,\n",
            "          -1.1878e+00, -1.1878e+00],\n",
            "         [-1.1564e+00, -1.1878e+00, -1.1878e+00,  ..., -1.1878e+00,\n",
            "          -1.1878e+00, -1.1878e+00],\n",
            "         [-1.1878e+00, -1.1878e+00, -1.1878e+00,  ..., -1.1878e+00,\n",
            "          -1.1878e+00, -1.1878e+00]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.0506, -0.0328,  0.0323,  ..., -1.1312, -1.1312, -1.1312],\n",
            "         [ 0.0470,  0.0647,  0.1299,  ..., -1.1312, -1.1312, -1.1312],\n",
            "         [-0.3927, -0.4590,  0.0451,  ..., -1.1312, -1.1312, -1.1312],\n",
            "         ...,\n",
            "         [-1.1312, -1.1312, -1.1312,  ..., -1.1312, -1.1312, -1.1312],\n",
            "         [-1.1312, -1.1312, -1.1312,  ..., -1.1312, -1.1312, -1.1312],\n",
            "         [-1.1312, -1.1312, -1.1312,  ..., -1.1312, -1.1312, -1.1312]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputsinputs  {'input_features': tensor([[[ 0.0554,  0.0480, -0.0014,  ..., -1.0124, -1.0124, -1.0124],\n",
            "         [ 0.1529,  0.1455,  0.0962,  ..., -1.0124, -1.0124, -1.0124],\n",
            "         [ 0.1487,  0.0465, -0.0121,  ..., -1.0124, -1.0124, -1.0124],\n",
            "         ...,\n",
            "         [-1.0124, -1.0124, -1.0124,  ..., -1.0124, -1.0124, -1.0124],\n",
            "         [-1.0124, -1.0124, -1.0124,  ..., -1.0124, -1.0124, -1.0124],\n",
            "         [-1.0124, -1.0124, -1.0124,  ..., -1.0124, -1.0124, -1.0124]]])}\n",
            "{'input_features': tensor([[[-0.2373,  0.0279,  0.0135,  ..., -1.0851, -1.0851, -1.0851],\n",
            "         [-0.1397,  0.1254,  0.1111,  ..., -1.0851, -1.0851, -1.0851],\n",
            "         [ 0.0638, -0.1253, -0.0428,  ..., -1.0851, -1.0851, -1.0851],\n",
            "         ...,\n",
            "         [-1.0851, -1.0851, -1.0851,  ..., -1.0851, -1.0851, -1.0851],\n",
            "         [-1.0851, -1.0851, -1.0851,  ..., -1.0851, -1.0851, -1.0851],\n",
            "         [-1.0851, -1.0851, -1.0851,  ..., -1.0851, -1.0851, -1.0851]]])}Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])Processed audio shape2: torch.Size([128, 3000])\n",
            "\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.0176,  0.0581, -0.0629,  ..., -1.2489, -1.2489, -1.2489],\n",
            "         [ 0.0800,  0.1557,  0.0347,  ..., -1.2489, -1.2489, -1.2489],\n",
            "         [-0.0929,  0.0904, -0.1019,  ..., -1.2489, -1.2489, -1.2489],\n",
            "         ...,\n",
            "         [-1.2489, -1.2489, -1.2489,  ..., -1.2489, -1.2489, -1.2489],\n",
            "         [-1.2489, -1.2489, -1.2489,  ..., -1.2489, -1.2489, -1.2489],\n",
            "         [-1.2489, -1.2489, -1.2489,  ..., -1.2489, -1.2489, -1.2489]]])}\n",
            "inputs Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000]){'input_features': tensor([[[-0.0052, -0.0388,  0.0279,  ..., -0.5660, -0.5660, -0.5660],\n",
            "         [ 0.0924,  0.0588,  0.1254,  ..., -0.5660, -0.5660, -0.5660],\n",
            "         [ 0.0438,  0.0980,  0.0448,  ..., -0.5660, -0.5660, -0.5660],\n",
            "         ...,\n",
            "         [-0.5660, -0.5660, -0.5660,  ..., -0.5660, -0.5660, -0.5660],\n",
            "         [-0.5660, -0.5660, -0.5660,  ..., -0.5660, -0.5660, -0.5660],\n",
            "         [-0.5660, -0.5660, -0.5660,  ..., -0.5660, -0.5660, -0.5660]]])}\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "inputs {'input_features': tensor([[[-2.8707e-01, -2.3220e-01, -3.8247e-02,  ..., -1.4407e+00,\n",
            "          -1.4407e+00, -1.4407e+00],\n",
            "         [-1.8951e-01, -1.3464e-01,  5.9318e-02,  ..., -1.4407e+00,\n",
            "          -1.4407e+00, -1.4407e+00],\n",
            "         [-1.0294e-01, -5.8734e-02,  1.1755e-03,  ..., -1.4407e+00,\n",
            "          -1.4407e+00, -1.4407e+00],\n",
            "         ...,\n",
            "         [-1.3357e+00, -1.3822e+00, -1.4401e+00,  ..., -1.4407e+00,\n",
            "          -1.4407e+00, -1.4407e+00],\n",
            "         [-1.3588e+00, -1.3843e+00, -1.4015e+00,  ..., -1.4407e+00,\n",
            "          -1.4407e+00, -1.4407e+00],\n",
            "         [-1.3705e+00, -1.3891e+00, -1.3087e+00,  ..., -1.4407e+00,\n",
            "          -1.4407e+00, -1.4407e+00]]])}\n",
            "inputsProcessed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000]) \n",
            "{'input_features': tensor([[[-0.0595, -0.0473, -0.0508,  ..., -1.3520, -1.3520, -1.3520],\n",
            "         [ 0.0380,  0.0503,  0.0467,  ..., -1.3520, -1.3520, -1.3520],\n",
            "         [-0.1169,  0.1045, -0.0907,  ..., -1.3520, -1.3520, -1.3520],\n",
            "         ...,\n",
            "         [-1.3312, -1.3520, -1.3364,  ..., -1.3520, -1.3520, -1.3520],\n",
            "         [-1.3520, -1.3520, -1.3232,  ..., -1.3520, -1.3520, -1.3520],\n",
            "         [-1.3520, -1.3520, -1.3208,  ..., -1.3520, -1.3520, -1.3520]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])inputs\n",
            " {'input_features': tensor([[[-0.1023, -0.1276,  0.0162,  ..., -1.1472, -1.1472, -1.1472],\n",
            "         [-0.0048, -0.0300,  0.1138,  ..., -1.1472, -1.1472, -1.1472],\n",
            "         [-0.0376, -0.0888,  0.0582,  ..., -1.1472, -1.1472, -1.1472],\n",
            "         ...,\n",
            "         [-1.1472, -1.1472, -1.1472,  ..., -1.1472, -1.1472, -1.1472],\n",
            "         [-1.1472, -1.1472, -1.1472,  ..., -1.1472, -1.1472, -1.1472],\n",
            "         [-1.1472, -1.1472, -1.1472,  ..., -1.1472, -1.1472, -1.1472]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.0206,  0.0835,  0.0248,  ..., -1.1459, -1.1459, -1.1459],\n",
            "         [ 0.1182,  0.1811,  0.1223,  ..., -1.1459, -1.1459, -1.1459],\n",
            "         [-0.3011,  0.0887,  0.0144,  ..., -1.1459, -1.1459, -1.1459],\n",
            "         ...,\n",
            "         [-1.1459, -1.1459, -1.1459,  ..., -1.1459, -1.1459, -1.1459],\n",
            "         [-1.1459, -1.1459, -1.1459,  ..., -1.1459, -1.1459, -1.1459],\n",
            "         [-1.1459, -1.1459, -1.1459,  ..., -1.1459, -1.1459, -1.1459]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.6296, -0.4433, -0.6085,  ..., -0.9976, -0.9976, -0.9976],\n",
            "         [-0.5320, -0.3457, -0.5109,  ..., -0.9976, -0.9976, -0.9976],\n",
            "         [-0.2895, -0.4825, -0.1925,  ..., -0.9976, -0.9976, -0.9976],\n",
            "         ...,\n",
            "         [-0.9976, -0.9976, -0.9976,  ..., -0.9976, -0.9976, -0.9976],\n",
            "         [-0.9976, -0.9976, -0.9976,  ..., -0.9976, -0.9976, -0.9976],\n",
            "         [-0.9976, -0.9976, -0.9976,  ..., -0.9976, -0.9976, -0.9976]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])inputs\n",
            "Processed audio shape2: torch.Size([128, 3000]) \n",
            "{'input_features': tensor([[[-0.2118,  0.1401,  0.0537,  ..., -0.5226, -0.5226, -0.5226],\n",
            "         [-0.1142,  0.2377,  0.1512,  ..., -0.5226, -0.5226, -0.5226],\n",
            "         [ 0.4253,  0.2660,  0.2345,  ..., -0.5226, -0.5226, -0.5226],\n",
            "         ...,\n",
            "         [-0.4946, -0.5226, -0.5226,  ..., -0.5226, -0.5226, -0.5226],\n",
            "         [-0.4937, -0.5226, -0.5226,  ..., -0.5226, -0.5226, -0.5226],\n",
            "         [-0.4976, -0.5226, -0.5226,  ..., -0.5226, -0.5226, -0.5226]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1579,  0.1096,  0.0747,  ..., -1.4270, -1.4270, -1.4270],\n",
            "         [ 0.2555,  0.2072,  0.1723,  ..., -1.4270, -1.4270, -1.4270],\n",
            "         [-0.3877, -0.0484,  0.0218,  ..., -1.4270, -1.4270, -1.4270],\n",
            "         ...,\n",
            "         [-1.4175, -1.3849, -1.4270,  ..., -1.4270, -1.4270, -1.4270],\n",
            "         [-1.3551, -1.3463, -1.4270,  ..., -1.4270, -1.4270, -1.4270],\n",
            "         [-1.3847, -1.3915, -1.4270,  ..., -1.4270, -1.4270, -1.4270]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])inputs\n",
            " {'input_features': tensor([[[ 0.2475,  0.1156,  0.1201,  ..., -0.9289, -0.9289, -0.9289],\n",
            "         [ 0.3451,  0.2131,  0.2176,  ..., -0.9289, -0.9289, -0.9289],\n",
            "         [-0.0963, -0.2038,  0.1905,  ..., -0.9289, -0.9289, -0.9289],\n",
            "         ...,\n",
            "         [-0.9289, -0.9289, -0.9289,  ..., -0.9289, -0.9289, -0.9289],\n",
            "         [-0.9289, -0.9289, -0.9289,  ..., -0.9289, -0.9289, -0.9289],\n",
            "         [-0.9289, -0.9289, -0.9289,  ..., -0.9289, -0.9289, -0.9289]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "inputsProcessed audio shape2: torch.Size([128, 3000]) \n",
            "{'input_features': tensor([[[ 0.1034, -0.0185, -0.0176,  ..., -0.9973, -0.9973, -0.9973],\n",
            "         [ 0.2010,  0.0791,  0.0800,  ..., -0.9973, -0.9973, -0.9973],\n",
            "         [-0.1460,  0.1594,  0.0969,  ..., -0.9973, -0.9973, -0.9973],\n",
            "         ...,\n",
            "         [-0.9973, -0.9973, -0.9973,  ..., -0.9973, -0.9973, -0.9973],\n",
            "         [-0.9973, -0.9973, -0.9973,  ..., -0.9973, -0.9973, -0.9973],\n",
            "         [-0.9973, -0.9973, -0.9973,  ..., -0.9973, -0.9973, -0.9973]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "inputsProcessed audio shape2: torch.Size([128, 3000]) \n",
            "{'input_features': tensor([[[ 6.4188e-04, -2.7354e-01, -2.3417e-01,  ..., -1.1881e+00,\n",
            "          -1.1881e+00, -1.1881e+00],\n",
            "         [ 9.8206e-02, -1.7598e-01, -1.3660e-01,  ..., -1.1881e+00,\n",
            "          -1.1881e+00, -1.1881e+00],\n",
            "         [-5.5959e-03, -1.4400e-01, -5.3546e-02,  ..., -1.1881e+00,\n",
            "          -1.1881e+00, -1.1881e+00],\n",
            "         ...,\n",
            "         [-1.1881e+00, -1.1881e+00, -1.1881e+00,  ..., -1.1881e+00,\n",
            "          -1.1881e+00, -1.1881e+00],\n",
            "         [-1.1881e+00, -1.1881e+00, -1.1881e+00,  ..., -1.1881e+00,\n",
            "          -1.1881e+00, -1.1881e+00],\n",
            "         [-1.1881e+00, -1.1881e+00, -1.1881e+00,  ..., -1.1881e+00,\n",
            "          -1.1881e+00, -1.1881e+00]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1552,  0.1288,  0.0308,  ..., -0.9095, -0.9095, -0.9095],\n",
            "         [ 0.2528,  0.2264,  0.1284,  ..., -0.9095, -0.9095, -0.9095],\n",
            "         [ 0.2002, -0.0168, -0.1688,  ..., -0.9095, -0.9095, -0.9095],\n",
            "         ...,\n",
            "         [-0.9095, -0.9095, -0.9095,  ..., -0.9095, -0.9095, -0.9095],\n",
            "         [-0.9095, -0.9095, -0.9095,  ..., -0.9095, -0.9095, -0.9095],\n",
            "         [-0.9095, -0.9095, -0.9095,  ..., -0.9095, -0.9095, -0.9095]]])}inputs\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            " {'input_features': tensor([[[-0.1349,  0.2339,  0.3092,  ..., -1.1539, -1.1539, -1.1539],\n",
            "         [-0.0374,  0.3315,  0.4068,  ..., -1.1539, -1.1539, -1.1539],\n",
            "         [ 0.1032,  0.1205,  0.2274,  ..., -1.1539, -1.1539, -1.1539],\n",
            "         ...,\n",
            "         [-1.0565, -1.1539, -1.1539,  ..., -1.1539, -1.1539, -1.1539],\n",
            "         [-1.1242, -1.1539, -1.1539,  ..., -1.1539, -1.1539, -1.1539],\n",
            "         [-1.0382, -1.1539, -1.1539,  ..., -1.1539, -1.1539, -1.1539]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])Processed audio shape2: torch.Size([128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "\n",
            "inputs {'input_features': tensor([[[-0.0530,  0.0380, -0.1042,  ..., -1.1053, -1.1053, -1.1053],\n",
            "         [ 0.0445,  0.1355, -0.0067,  ..., -1.1053, -1.1053, -1.1053],\n",
            "         [ 0.1429,  0.2574, -0.0234,  ..., -1.1053, -1.1053, -1.1053],\n",
            "         ...,\n",
            "         [-1.1053, -1.1053, -1.1053,  ..., -1.1053, -1.1053, -1.1053],\n",
            "         [-1.1053, -1.1053, -1.1053,  ..., -1.1053, -1.1053, -1.1053],\n",
            "         [-1.1053, -1.1053, -1.1053,  ..., -1.1053, -1.1053, -1.1053]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])inputs\n",
            " {'input_features': tensor([[[ 0.4328,  0.0495,  0.1515,  ..., -0.7193, -0.7193, -0.7193],\n",
            "         [ 0.5303,  0.1471,  0.2491,  ..., -0.7193, -0.7193, -0.7193],\n",
            "         [ 0.5834,  0.2086,  0.2410,  ..., -0.7193, -0.7193, -0.7193],\n",
            "         ...,\n",
            "         [-0.7193, -0.7193, -0.7193,  ..., -0.7193, -0.7193, -0.7193],\n",
            "         [-0.7193, -0.7193, -0.7193,  ..., -0.7193, -0.7193, -0.7193],\n",
            "         [-0.7193, -0.7193, -0.7193,  ..., -0.7193, -0.7193, -0.7193]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.2615,  0.2620,  0.2453,  ..., -0.3957, -0.3957, -0.3957],\n",
            "         [ 0.3591,  0.3596,  0.3429,  ..., -0.3957, -0.3957, -0.3957],\n",
            "         [ 0.1214,  0.3884, -0.1149,  ..., -0.3957, -0.3957, -0.3957],\n",
            "         ...,\n",
            "         [-0.3957, -0.3957, -0.3957,  ..., -0.3957, -0.3957, -0.3957],\n",
            "         [-0.3957, -0.3957, -0.3957,  ..., -0.3957, -0.3957, -0.3957],\n",
            "         [-0.3957, -0.3957, -0.3957,  ..., -0.3957, -0.3957, -0.3957]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])inputs\n",
            " Processed audio shape2: torch.Size([128, 3000]){'input_features': tensor([[[-0.2881, -0.1713,  0.0725,  ..., -0.8573, -0.8573, -0.8573],\n",
            "         [-0.1905, -0.0738,  0.1701,  ..., -0.8573, -0.8573, -0.8573],\n",
            "         [-0.1964, -0.0267,  0.2177,  ..., -0.8573, -0.8573, -0.8573],\n",
            "         ...,\n",
            "         [-0.7628, -0.8573, -0.8573,  ..., -0.8573, -0.8573, -0.8573],\n",
            "         [-0.7846, -0.8573, -0.8573,  ..., -0.8573, -0.8573, -0.8573],\n",
            "         [-0.7923, -0.8573, -0.8573,  ..., -0.8573, -0.8573, -0.8573]]])}\n",
            "\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.2914,  0.2236,  0.0202,  ..., -0.8449, -0.8449, -0.8449],\n",
            "         [ 0.3889,  0.3212,  0.1178,  ..., -0.8449, -0.8449, -0.8449],\n",
            "         [ 0.2290,  0.2113, -0.0419,  ..., -0.8449, -0.8449, -0.8449],\n",
            "         ...,\n",
            "         [-0.8449, -0.8449, -0.8449,  ..., -0.8449, -0.8449, -0.8449],\n",
            "         [-0.8449, -0.8449, -0.8449,  ..., -0.8449, -0.8449, -0.8449],\n",
            "         [-0.8449, -0.8449, -0.8449,  ..., -0.8449, -0.8449, -0.8449]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "inputsProcessed audio shape2: torch.Size([128, 3000]) \n",
            "{'input_features': tensor([[[-0.3173, -0.1179, -0.1067,  ..., -1.4695, -1.4695, -1.4695],\n",
            "         [-0.2197, -0.0204, -0.0091,  ..., -1.4695, -1.4695, -1.4695],\n",
            "         [-0.6473, -0.0880,  0.0203,  ..., -1.4695, -1.4695, -1.4695],\n",
            "         ...,\n",
            "         [-1.3154, -1.3641, -1.4216,  ..., -1.4695, -1.4695, -1.4695],\n",
            "         [-1.2938, -1.3203, -1.3160,  ..., -1.4695, -1.4695, -1.4695],\n",
            "         [-1.3222, -1.3624, -1.4104,  ..., -1.4695, -1.4695, -1.4695]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1961,  0.0786, -0.0576,  ..., -0.6982, -0.6982, -0.6982],\n",
            "         [ 0.2936,  0.1762,  0.0400,  ..., -0.6982, -0.6982, -0.6982],\n",
            "         [ 0.1901,  0.1482,  0.2306,  ..., -0.6982, -0.6982, -0.6982],\n",
            "         ...,\n",
            "         [-0.6982, -0.6982, -0.6982,  ..., -0.6982, -0.6982, -0.6982],\n",
            "         [-0.6982, -0.6982, -0.6982,  ..., -0.6982, -0.6982, -0.6982],\n",
            "         [-0.6982, -0.6982, -0.6982,  ..., -0.6982, -0.6982, -0.6982]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.2039,  0.1149,  0.1380,  ..., -0.9577, -0.9577, -0.9577],\n",
            "         [-0.1064,  0.2125,  0.2356,  ..., -0.9577, -0.9577, -0.9577],\n",
            "         [ 0.2154,  0.1825,  0.1862,  ..., -0.9577, -0.9577, -0.9577],\n",
            "         ...,\n",
            "         [-0.9577, -0.9577, -0.9577,  ..., -0.9577, -0.9577, -0.9577],\n",
            "         [-0.9577, -0.9577, -0.9577,  ..., -0.9577, -0.9577, -0.9577],\n",
            "         [-0.9577, -0.9577, -0.9577,  ..., -0.9577, -0.9577, -0.9577]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.0450,  0.0327, -0.0406,  ..., -1.2406, -1.2406, -1.2406],\n",
            "         [ 0.0526,  0.1303,  0.0570,  ..., -1.2406, -1.2406, -1.2406],\n",
            "         [-0.0972,  0.2368,  0.0643,  ..., -1.2406, -1.2406, -1.2406],\n",
            "         ...,\n",
            "         [-1.2406, -1.2406, -1.2406,  ..., -1.2406, -1.2406, -1.2406],\n",
            "         [-1.2406, -1.2406, -1.2406,  ..., -1.2406, -1.2406, -1.2406],\n",
            "         [-1.2406, -1.2406, -1.2406,  ..., -1.2406, -1.2406, -1.2406]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1308, -0.0373, -0.0630,  ..., -1.1215, -1.1215, -1.1215],\n",
            "         [ 0.2283,  0.0602,  0.0345,  ..., -1.1215, -1.1215, -1.1215],\n",
            "         [ 0.1478, -0.1933, -0.1121,  ..., -1.1215, -1.1215, -1.1215],\n",
            "         ...,\n",
            "         [-1.0824, -1.1215, -1.1215,  ..., -1.1215, -1.1215, -1.1215],\n",
            "         [-1.0770, -1.1215, -1.1215,  ..., -1.1215, -1.1215, -1.1215],\n",
            "         [-1.1215, -1.1215, -1.1215,  ..., -1.1215, -1.1215, -1.1215]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.0971,  0.1766,  0.1786,  ..., -0.9276, -0.9276, -0.9276],\n",
            "         [ 0.1947,  0.2742,  0.2762,  ..., -0.9276, -0.9276, -0.9276],\n",
            "         [-0.4695,  0.1499,  0.2541,  ..., -0.9276, -0.9276, -0.9276],\n",
            "         ...,\n",
            "         [-0.9276, -0.9276, -0.9276,  ..., -0.9276, -0.9276, -0.9276],\n",
            "         [-0.9276, -0.9276, -0.9276,  ..., -0.9276, -0.9276, -0.9276],\n",
            "         [-0.9276, -0.9276, -0.9276,  ..., -0.9276, -0.9276, -0.9276]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.3746,  0.1923,  0.2900,  ..., -0.5866, -0.5866, -0.5866],\n",
            "         [ 0.4721,  0.2899,  0.3875,  ..., -0.5866, -0.5866, -0.5866],\n",
            "         [ 0.5273,  0.3706,  0.3155,  ..., -0.5866, -0.5866, -0.5866],\n",
            "         ...,\n",
            "         [-0.5866, -0.5866, -0.5866,  ..., -0.5866, -0.5866, -0.5866],\n",
            "         [-0.5866, -0.5866, -0.5866,  ..., -0.5866, -0.5866, -0.5866],\n",
            "         [-0.5866, -0.5866, -0.5866,  ..., -0.5866, -0.5866, -0.5866]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[-0.0300, -0.2344, -0.0033,  ..., -1.1096, -1.1096, -1.1096],\n",
            "         [ 0.0676, -0.1369,  0.0942,  ..., -1.1096, -1.1096, -1.1096],\n",
            "         [ 0.0080, -0.1464, -0.1120,  ..., -1.1096, -1.1096, -1.1096],\n",
            "         ...,\n",
            "         [-1.1096, -1.1096, -1.1096,  ..., -1.1096, -1.1096, -1.1096],\n",
            "         [-1.1096, -1.1096, -1.1096,  ..., -1.1096, -1.1096, -1.1096],\n",
            "         [-1.1096, -1.1096, -1.1096,  ..., -1.1096, -1.1096, -1.1096]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.3675,  0.5252,  0.4867,  ..., -0.8595, -0.8595, -0.8595],\n",
            "         [ 0.4651,  0.6227,  0.5843,  ..., -0.8595, -0.8595, -0.8595],\n",
            "         [ 0.5054,  0.4545,  0.4050,  ..., -0.8595, -0.8595, -0.8595],\n",
            "         ...,\n",
            "         [-0.8595, -0.8595, -0.8595,  ..., -0.8595, -0.8595, -0.8595],\n",
            "         [-0.8595, -0.8595, -0.8595,  ..., -0.8595, -0.8595, -0.8595],\n",
            "         [-0.8595, -0.8595, -0.8595,  ..., -0.8595, -0.8595, -0.8595]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1380,  0.0762, -0.1265,  ..., -1.0763, -1.0763, -1.0763],\n",
            "         [ 0.2355,  0.1738, -0.0289,  ..., -1.0763, -1.0763, -1.0763],\n",
            "         [-0.4213, -0.0663, -0.0113,  ..., -1.0763, -1.0763, -1.0763],\n",
            "         ...,\n",
            "         [-1.0763, -1.0763, -1.0763,  ..., -1.0763, -1.0763, -1.0763],\n",
            "         [-1.0763, -1.0763, -1.0763,  ..., -1.0763, -1.0763, -1.0763],\n",
            "         [-1.0763, -1.0763, -1.0763,  ..., -1.0763, -1.0763, -1.0763]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.0141,  0.0501, -0.0311,  ..., -1.2636, -1.2636, -1.2636],\n",
            "         [ 0.1117,  0.1476,  0.0665,  ..., -1.2636, -1.2636, -1.2636],\n",
            "         [-0.7955,  0.0794, -0.1247,  ..., -1.2636, -1.2636, -1.2636],\n",
            "         ...,\n",
            "         [-1.2552, -1.2636, -1.2636,  ..., -1.2636, -1.2636, -1.2636],\n",
            "         [-1.2636, -1.2636, -1.2636,  ..., -1.2636, -1.2636, -1.2636],\n",
            "         [-1.2602, -1.2636, -1.2636,  ..., -1.2636, -1.2636, -1.2636]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.1489,  0.1278, -0.0306,  ..., -0.8638, -0.8638, -0.8638],\n",
            "         [ 0.2465,  0.2254,  0.0670,  ..., -0.8638, -0.8638, -0.8638],\n",
            "         [-0.8638,  0.0894,  0.0139,  ..., -0.8638, -0.8638, -0.8638],\n",
            "         ...,\n",
            "         [-0.8638, -0.8638, -0.8638,  ..., -0.8638, -0.8638, -0.8638],\n",
            "         [-0.8638, -0.8638, -0.8638,  ..., -0.8638, -0.8638, -0.8638],\n",
            "         [-0.8638, -0.8638, -0.8638,  ..., -0.8638, -0.8638, -0.8638]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.0544,  0.0252, -0.0457,  ..., -1.0483, -1.0483, -1.0483],\n",
            "         [ 0.1520,  0.1228,  0.0519,  ..., -1.0483, -1.0483, -1.0483],\n",
            "         [-0.5085,  0.0039,  0.0875,  ..., -1.0483, -1.0483, -1.0483],\n",
            "         ...,\n",
            "         [-1.0483, -1.0483, -1.0483,  ..., -1.0483, -1.0483, -1.0483],\n",
            "         [-1.0483, -1.0483, -1.0483,  ..., -1.0483, -1.0483, -1.0483],\n",
            "         [-1.0483, -1.0483, -1.0483,  ..., -1.0483, -1.0483, -1.0483]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n",
            "inputs {'input_features': tensor([[[ 0.3450,  0.2564,  0.2892,  ..., -1.1919, -1.1919, -1.1919],\n",
            "         [ 0.4425,  0.3539,  0.3868,  ..., -1.1919, -1.1919, -1.1919],\n",
            "         [ 0.3867,  0.1935,  0.3433,  ..., -1.1919, -1.1919, -1.1919],\n",
            "         ...,\n",
            "         [-1.1919, -1.1919, -1.1919,  ..., -1.1919, -1.1919, -1.1919],\n",
            "         [-1.1919, -1.1919, -1.1919,  ..., -1.1919, -1.1919, -1.1919],\n",
            "         [-1.1919, -1.1919, -1.1919,  ..., -1.1919, -1.1919, -1.1919]]])}\n",
            "Processed audio shape1: torch.Size([1, 128, 3000])\n",
            "Processed audio shape2: torch.Size([128, 3000])\n"
          ]
        }
      ],
      "source": [
        "alm_cfg = ALMConfig()\n",
        "train_cfg = TrainConfig()\n",
        "trained_model = train(train_cfg, alm_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tn3F8DfCkuk8",
      "metadata": {
        "id": "tn3F8DfCkuk8"
      },
      "outputs": [],
      "source": [
        "trained_model.save_pretrained(\"/content/\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a ALM in Google Colab!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d34d70c6",
      "metadata": {
        "id": "d34d70c6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp -r /content/drive/MyDrive/nanoALM/5/stage1 /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "33818ea3-4d2b-48c0-ed7e-81e5dcf37b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nanoVLM_From_Huggingface' already exists and is not an empty directory.\n",
            "/content/nanoVLM_From_Huggingface\n",
            "assets\t\t\tdebug_func.py\t\t\t\t  nanoALM.ipynb\n",
            "benchmark-inference.py\tdebug_tokenizer_dataset_compatibility.py  old\n",
            "benchmark_suite.py\tgenerate.py\t\t\t\t  __pycache__\n",
            "checkpoints\t\tmeasure_vram.py\t\t\t\t  README.md\n",
            "data\t\t\tmodels\t\t\t\t\t  test\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoALM'):\n",
        "    !git clone https://github.com/LWL220184016/nanoVLM_From_Huggingface.git\n",
        "%cd nanoVLM_From_Huggingface/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "54bc8463",
      "metadata": {
        "id": "54bc8463"
      },
      "outputs": [],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "# !huggingface-cli login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "collapsed": true,
        "id": "bcw8qQqoOSR7"
      },
      "outputs": [],
      "source": [
        "# # If you get an \"Error\" from pip's dependency resolver but the cell complets fine, this is not an issue, you can continue :)\n",
        "# !pip -q install torch\n",
        "# !pip -q install gcsfs\n",
        "# !pip -q install tqdm\n",
        "# !pip -q install huggingface_hub\n",
        "# !pip -q install librosa\n",
        "# !pip install soundfile librosa -q\n",
        "# # !pip install --upgrade transformers\n",
        "# !pip install datasets==3.6.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoALM\"\n",
        "# hf_model_name = \"YOUR_HF_USER_NAME/nanoALM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "eda2297d-0922-4acc-9533-83925b2e6ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# nanoALM Imports (please check out the implementations in detail, that's where all the interessting stuff is!)\n",
        "from data.collators import AlignmentCollator, AudioQACollator\n",
        "from data.datasets import SAVEEDataset, AudioQADataset\n",
        "from data.processors import get_audio_processor\n",
        "from data.processors import get_tokenizer\n",
        "from models.audio_language_model import AudioLanguageModel\n",
        "import models.utils as utils\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "#Otherwise, the tokenizer will through a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    try:\n",
        "        import torch_xla\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        device = xm.xla_device()\n",
        "    except ImportError:\n",
        "        device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "trained_model = None\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, alm_cfg, tokenizer):\n",
        "    # Create datasets\n",
        "    audio_processor = get_audio_processor(alm_cfg)\n",
        "\n",
        "    # text = \"splitting datasets, disable in get_dataloaders function\"\n",
        "    # print(f\"\\n\\033[38;5;05m{text}05m\\033[0m\")\n",
        "    # Load and combine all training datasets\n",
        "    combined_train_data = []\n",
        "    for dataset_name in train_cfg.train_dataset_name:\n",
        "        train_ds = load_dataset(\n",
        "        path = train_cfg.train_dataset_path,\n",
        "        name = dataset_name,\n",
        "    )\n",
        "        combined_train_data.append(train_ds['train'])\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    test_ds = load_dataset(train_cfg.test_dataset_path)\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    train_dataset = AudioQADataset(train_ds.select(range(train_size)), tokenizer, audio_processor)\n",
        "    val_dataset = AudioQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, audio_processor)\n",
        "    test_dataset = SAVEEDataset(test_ds, tokenizer, audio_processor)\n",
        "\n",
        "    # Create collators\n",
        "    alignment_collator = AlignmentCollator(tokenizer, alm_cfg.lm_max_length, audio_processor)\n",
        "    aqa_collator = AudioQACollator(tokenizer, alm_cfg.lm_max_length)\n",
        "    savee_collator = AudioQACollator(tokenizer, alm_cfg.lm_max_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "    alignment_train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=alignment_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=train_cfg.savee_batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=savee_collator,\n",
        "        pin_memory=True,\n",
        "        )\n",
        "\n",
        "    return alignment_train_loader, train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D7NIuEDuOuuJ",
      "metadata": {
        "id": "D7NIuEDuOuuJ"
      },
      "source": [
        "### Prepare the testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9fnh6wOlOzat",
      "metadata": {
        "id": "9fnh6wOlOzat"
      },
      "outputs": [],
      "source": [
        "def test_savee(model, tokenizer, test_loader, device):\n",
        "    total_examples = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            audio = batch['audios'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            gen = model.generate(input_ids, audio, attention_mask)\n",
        "            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "            is_correct = utils.check_multiple_choice_with_regex(model_output, correct_answer)\n",
        "\n",
        "            total_examples += len(is_correct)\n",
        "            if is_correct:\n",
        "                correct_predictions += sum(is_correct)\n",
        "    accuracy = correct_predictions / total_examples if total_examples > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def get_avg_alignment(model, val_loader, device, epoch):\n",
        "    \"\"\"\n",
        "    Validate the model's audio-text alignment on the validation set.\n",
        "    This function computes the average alignment score over the validation set.\n",
        "    It runs for a maximum of 20 batches to save time during training.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_alignment_score = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            if i >= 20:  # 只驗證前20個batch以節省時間\n",
        "                break\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            alignment_score = model.validate_audio_text_alignment(input_ids, audios)\n",
        "            total_alignment_score += alignment_score\n",
        "\n",
        "    avg_alignment = total_alignment_score / min(20, len(val_loader))\n",
        "    print(f\"Epoch {epoch+1}: Average alignment score: {avg_alignment:.4f}\")\n",
        "\n",
        "    print(\" \")\n",
        "    model.train()\n",
        "    return avg_alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe359194",
      "metadata": {
        "id": "fe359194"
      },
      "source": [
        "### Add debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5d7b7e46",
      "metadata": {
        "id": "5d7b7e46"
      },
      "outputs": [],
      "source": [
        "# 在训练开始前添加这个检查函数\n",
        "def debug_model_dimensions(model, input_ids, audio):\n",
        "    \"\"\"调试模型各层的维度\"\"\"\n",
        "    print(\"=== Model Dimension Debug ===\")\n",
        "\n",
        "    # 检查音频编码器\n",
        "    audio_features = model.audio_encoder.forward(audio, output_hidden_states=True)\n",
        "    print(f\"Audio features shape: {audio_features.shape}\")\n",
        "\n",
        "    # 检查模态投影器\n",
        "    audio_embeds = model.MP(audio_features)\n",
        "    print(f\"Audio embeds shape: {audio_embeds.shape}\")\n",
        "\n",
        "    # 检查文本嵌入\n",
        "    text_embeds = model.decoder.token_embedding(input_ids)\n",
        "    print(f\"Text embeds shape: {text_embeds.shape}\")\n",
        "\n",
        "    # 检查拼接后的嵌入\n",
        "    inputs_embeds = torch.cat([audio_embeds, text_embeds], dim=1)\n",
        "    print(f\"Combined embeds shape: {inputs_embeds.shape}\")\n",
        "\n",
        "    # 检查语言模型输出\n",
        "    logits = model.decoder(inputs_embeds)\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "    print(f\"Vocab size (last dim): {logits.shape[-1]}\")\n",
        "\n",
        "    # 检查语言模型配置\n",
        "    print(f\"LM vocab size config: {model.cfg.lm_vocab_size}\")\n",
        "    print(f\"Decoder vocab size: {getattr(model.decoder, 'vocab_size', 'Not found')}\")\n",
        "\n",
        "    return logits.shape[-1]\n",
        "\n",
        "# 在训练循环开始前调用\n",
        "# vocab_size = debug_model_dimensions(model, input_ids, audios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2d48d0df",
      "metadata": {
        "id": "2d48d0df"
      },
      "outputs": [],
      "source": [
        "def debug_training_step(model, input_ids, audios, attention_mask, labels):\n",
        "    \"\"\"调试训练步骤\"\"\"\n",
        "    # 添加这些调试行：\n",
        "    print(f\"Batch debug - input_ids shape: {input_ids.shape}, max: {input_ids.max().item()}\")\n",
        "    print(f\"Batch debug - labels shape: {labels.shape}, max: {labels.max().item()}\")\n",
        "    print(f\"Batch debug - Model vocab config: {model.cfg.lm_vocab_size}\")\n",
        "\n",
        "    # 检查decoder的实际vocab_size\n",
        "    if hasattr(model.decoder, 'head') and hasattr(model.decoder.head, 'out_features'):\n",
        "        print(f\"Decoder head in_features: {model.decoder.head.in_features}\")\n",
        "        print(f\"Decoder head out_features: {model.decoder.head.out_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04da5b01",
      "metadata": {
        "id": "04da5b01"
      },
      "source": [
        "#### Three-stage training (contrast training, generative training, instruction fine-tuning) 三段式訓練(對比訓練, 生成式訓練, 指令微調)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fbb323c8",
      "metadata": {
        "id": "fbb323c8"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.amp as GradScaler\n",
        "\n",
        "from debug_func import debug_contrastive_learning\n",
        "\n",
        "# 改進對比學習訓練\n",
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def get_contrastive_loss(audio_embeds, text_embeds, temperature=0.07):\n",
        "    \"\"\"\n",
        "    標準、高效的對比學習損失 (CLIP Loss)。\n",
        "    注意：輸入的 embeds 應該是池化後的 [B, D] 維度向量。\n",
        "    \"\"\"\n",
        "    # --- 開始計算尺度對齊損失 ---\n",
        "\n",
        "    # 1. 計算每個向量的 L2 範數 (沿著最後一個維度)\n",
        "    #    detach() 是為了確保這個輔助損失的梯度只流向投影器，而不影響上游的編碼器或文字嵌入層的穩定性（可選但推薦）\n",
        "    audio_norms = torch.norm(audio_embeds, p=2, dim=-1)\n",
        "    text_norms = torch.norm(text_embeds, p=2, dim=-1)\n",
        "\n",
        "    # 2. 計算批次內的平均範數\n",
        "    mean_audio_norm = torch.mean(audio_norms)\n",
        "    mean_text_norm = torch.mean(text_norms)\n",
        "\n",
        "    # 3. 計算尺度對齊損失 (使用 MSE)\n",
        "    #    目標是讓 mean_audio_norm 和 mean_text_norm 盡可能接近\n",
        "    scale_loss = F.mse_loss(mean_audio_norm, mean_text_norm)\n",
        "\n",
        "    # --- 結束計算尺度對齊損失 ---\n",
        "\n",
        "    # --- 開始計算交叉熵損失 ---\n",
        "    # 歸一化\n",
        "    audio_embeds = F.normalize(audio_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "\n",
        "    # 計算相似度矩陣\n",
        "    # temperature 是一個重要的超參數，CLIP 論文中是可學習的，但固定值也可以\n",
        "    logits_per_audio = torch.matmul(audio_embeds, text_embeds.T) / temperature\n",
        "    logits_per_text = logits_per_audio.T\n",
        "\n",
        "    # 創建標籤 (0, 1, 2, ..., B-1)\n",
        "    labels = torch.arange(audio_embeds.shape[0]).to(logits_per_audio.device)\n",
        "\n",
        "    # 對稱的交叉熵損失\n",
        "    loss_a = F.cross_entropy(logits_per_audio, labels)\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "\n",
        "    contrastive_loss = (loss_a + loss_t) / 2\n",
        "\n",
        "    # --- 結束計算交叉熵損失 ---\n",
        "\n",
        "\n",
        "    # 4. 組合損失\n",
        "    #    lambda_scale 是一個需要調整的超參數，用來平衡兩個損失的權重\n",
        "    lambda_scale = 0.001  # 範例值，可以從 0.01, 0.1, 1.0 等開始嘗試\n",
        "    total_loss = contrastive_loss + lambda_scale * scale_loss\n",
        "\n",
        "    # 監控指標 (可選但推薦)\n",
        "    with torch.no_grad():\n",
        "        pos_sim = torch.diagonal(logits_per_audio * temperature).mean()\n",
        "        mask = ~torch.eye(labels.shape[0], dtype=torch.bool, device=labels.device)\n",
        "        neg_sim = (logits_per_audio * temperature)[mask].mean()\n",
        "\n",
        "    return total_loss, contrastive_loss, scale_loss, {\n",
        "        \"loss\": total_loss.item(),\n",
        "        \"pos_sim\": pos_sim.item(), # 正樣本對的餘弦相似度\n",
        "        \"neg_sim\": neg_sim.item()  # 負樣本對的餘弦相似度\n",
        "    }\n",
        "\n",
        "def train_step1_alignment(train_cfg, alm_cfg, model=None, tokenizer = None, device = None):\n",
        "    # 凍結音頻編碼器和語言模型\n",
        "    model.audio_encoder.audio_encoder.requires_grad_(False)\n",
        "    model.decoder.requires_grad_(False)\n",
        "    model.MP.requires_grad_(True)\n",
        "\n",
        "    alignment_train_loader, _, val_loader, _ = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "\n",
        "    optimizer = optim.AdamW(model.MP.parameters(), lr=train_cfg.lr_mp, weight_decay=0.01)\n",
        "\n",
        "    best_alignment = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage1_epochs):\n",
        "        model.train()\n",
        "        total_contrastive_loss = 0  # 添加這個變數初始化\n",
        "        total_scale_loss = 0  # 添加這個變數初始化\n",
        "\n",
        "        for batch in tqdm(alignment_train_loader, desc=f\"Stage1 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. 音頻編碼 -> 投影\n",
        "            with torch.no_grad():\n",
        "                audio_features = model.audio_encoder.audio_encoder(audios, output_hidden_states=True)\n",
        "            projected_audio_features = model.MP(audio_features.last_hidden_state)\n",
        "\n",
        "            # 2. 文本編碼 - 修復這裡的問題\n",
        "            with torch.no_grad():\n",
        "                # 檢查 decoder 的 forward 方法簽名\n",
        "                # 根據 language_model.py，應該傳入 x 而不是分別的參數\n",
        "                text_embeds = model.decoder.token_embedding(input_ids)  # 直接獲取文本嵌入\n",
        "\n",
        "                # 如果需要通過完整的 decoder，使用以下方式：\n",
        "                # text_outputs, _ = model.decoder(text_embeds, attention_mask=attention_mask)\n",
        "                # text_embeds = text_outputs  # 使用輸出的嵌入\n",
        "\n",
        "            # 3. 池化操作 (Pooling)\n",
        "            # 音頻池化\n",
        "            audio_pooled = projected_audio_features.mean(dim=1)  # [B, D]\n",
        "\n",
        "            # 文本池化 - 修復維度問題\n",
        "            # text_embeds 現在是 [B, seq_len, hidden_dim]\n",
        "            if attention_mask is not None:\n",
        "                # 根據 attention_mask 來安全地做平均池化\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(text_embeds.size()).float()\n",
        "                sum_embeddings = torch.sum(text_embeds * input_mask_expanded, 1)\n",
        "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                text_pooled = sum_embeddings / sum_mask  # [B, D]\n",
        "            else:\n",
        "                text_pooled = text_embeds.mean(dim=1)  # [B, D]\n",
        "\n",
        "            # 如果維度仍然不匹配，添加投影層\n",
        "            if audio_pooled.shape[-1] != text_pooled.shape[-1]:\n",
        "                # 創建一個投影層來匹配維度\n",
        "                if not hasattr(model, 'text_projection'):\n",
        "                    model.text_projection = nn.Linear(text_pooled.shape[-1], audio_pooled.shape[-1]).to(device)\n",
        "                text_pooled = model.text_projection(text_pooled)\n",
        "\n",
        "            # 4. 計算對比損失\n",
        "            loss, contrastive_loss, scale_loss, metrics = get_contrastive_loss(audio_pooled, text_pooled)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.MP.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_contrastive_loss += contrastive_loss.item()\n",
        "            total_scale_loss += scale_loss.item()\n",
        "\n",
        "        avg_contrastive_loss = total_contrastive_loss / len(alignment_train_loader)\n",
        "        avg_scale_loss = total_scale_loss / len(alignment_train_loader)\n",
        "        print(f\"Stage1 Epoch {epoch+1}: Total Loss {loss:.4f}, Contrastive Loss {avg_contrastive_loss:.4f}, Scale Loss {avg_scale_loss:.4f}\")\n",
        "\n",
        "        avg_alignment = get_avg_alignment(model, val_loader, device, epoch)\n",
        "\n",
        "        if avg_alignment > best_alignment:\n",
        "            best_alignment = avg_alignment\n",
        "            model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage1_best\")\n",
        "            print(f\"  New best alignment: {best_alignment:.4f}\")\n",
        "\n",
        "    print(f\"Stage 1 completed! Best alignment: {best_alignment:.4f}\")\n",
        "    return model\n",
        "\n",
        "def train_step2_pretraining(train_cfg, alm_cfg, stage1_model=None, tokenizer=None, device = None):\n",
        "    \"\"\"第二步：语言模型预训练\"\"\"\n",
        "    print(\"=== Stage 2: Language Model Pretraining ===\")\n",
        "\n",
        "    _, train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # 加载第一阶段模型或从头开始\n",
        "    if stage1_model is not None:\n",
        "        model = stage1_model\n",
        "    else:\n",
        "        try:\n",
        "            model = AudioLanguageModel.from_pretrained(f\"{alm_cfg.alm_checkpoint_path}/stage1_final\")\n",
        "            print(\"Loaded Stage 1 model\")\n",
        "        except:\n",
        "            model = AudioLanguageModel(alm_cfg, load_backbone=True, tokenizer=tokenizer, device=device)\n",
        "            print(\"Starting Stage 2 from scratch\")\n",
        "\n",
        "    # 冻结音频编码器，解冻语言模型和模态投影器\n",
        "    for param in model.audio_encoder.audio_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.decoder.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.MP.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Stage 2: Training {trainable_params:,} parameters\")\n",
        "\n",
        "    # 不同学习率\n",
        "    param_groups = [\n",
        "        {'params': model.MP.parameters(), 'lr': train_cfg.lr_mp * 0.1},\n",
        "        {'params': model.decoder.parameters(), 'lr': train_cfg.lr_backbones}\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "    scaler = torch.amp.GradScaler(device=device)\n",
        "\n",
        "\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    best_loss = float('inf')\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage2_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage2 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                # 使用因果语言建模损失\n",
        "                logits, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            # print(\"loss: \", loss)\n",
        "            scaler.scale(loss).backward()\n",
        "            # loss.backward()\n",
        "\n",
        "            # 动态学习率调整\n",
        "            adj_lr_mp = get_lr(global_step, train_cfg.lr_mp * 0.1, len(train_loader) * train_cfg.stage2_epochs)\n",
        "            adj_lr_backbones = get_lr(global_step, train_cfg.lr_backbones, len(train_loader) * train_cfg.stage2_epochs)\n",
        "            optimizer.param_groups[0]['lr'] = adj_lr_mp\n",
        "            optimizer.param_groups[1]['lr'] = adj_lr_backbones\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            # 2. 對 unscale 後的梯度進行裁剪 (max_norm=1.0 是一個常用的值)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            # --- 修改結束 ---\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "                print(\"!!! Loss is NaN before backward pass. Problem is in the forward pass.\")\n",
        "                print(loss)\n",
        "                # 檢查 logits 是否包含 NaN 或 inf\n",
        "                if torch.isnan(logits).any():\n",
        "                    print(\"!!! Logits contain NaN.\")\n",
        "                if torch.isinf(logits).any():\n",
        "                    print(\"!!! Logits contain Inf.\")\n",
        "                # 在此處中斷，以便檢查\n",
        "                import sys; sys.exit()\n",
        "            # --- 調試碼結束 ---\n",
        "\n",
        "            # # 3. Scaler 執行優化器步驟 (如果梯度沒有 inf/nan)\n",
        "            scaler.step(optimizer)\n",
        "            # # 4. 更新 scaler 的縮放因子\n",
        "            scaler.update()\n",
        "            # optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        if avg_train_loss < best_loss:\n",
        "            best_loss = avg_train_loss\n",
        "            model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_best\")\n",
        "\n",
        "        avg_alignment = get_avg_alignment(model, val_loader, device, epoch)\n",
        "        print(f\"Stage2 Epoch {epoch+1}/{train_cfg.stage2_epochs} | Loss: {avg_train_loss:.4f} | Alignment: {avg_alignment:.4f}\")\n",
        "\n",
        "    # 保存第二阶段模型\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_final\")\n",
        "    print(\"Stage 2 completed!\")\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model=None, tokenizer=None, device = None):\n",
        "    \"\"\"第三步：指令微调\"\"\"\n",
        "    print(\"=== Stage 3: Instruction Tuning ===\")\n",
        "\n",
        "    _, train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "    scaler = torch.amp.GradScaler(device=device)\n",
        "\n",
        "    # 加载第二阶段模型\n",
        "    if stage2_model is not None:\n",
        "        model = stage2_model\n",
        "    else:\n",
        "        try:\n",
        "            model = AudioLanguageModel.from_pretrained(f\"{alm_cfg.alm_checkpoint_path}/stage2_final\")\n",
        "            print(\"Loaded Stage 2 model\")\n",
        "        except:\n",
        "            print(\"No Stage 2 model found, using current model\")\n",
        "            model = AudioLanguageModel(alm_cfg, load_backbone=True, tokenizer=tokenizer, device=device)\n",
        "\n",
        "    # 全部解冻，使用较小学习率\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    print(f\"Stage 3: Training all {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    # 更小的学习率\n",
        "    param_groups = [\n",
        "        {'params': model.MP.parameters(), 'lr': train_cfg.lr_mp * 0.01},\n",
        "        {'params': model.decoder.parameters(), 'lr': train_cfg.lr_backbones * 0.1},\n",
        "        {'params': model.audio_encoder.parameters(), 'lr': train_cfg.lr_backbones * 0.01}\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # 这里可以使用原来的训练循环，但数据应该是指令格式\n",
        "    # 暂时使用相同的数据格式\n",
        "    best_accuracy = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage3_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage3 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            # 2. 對 unscale 後的梯度進行裁剪 (max_norm=1.0 是一個常用的值)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            # --- 修改結束 ---\n",
        "\n",
        "            # 3. Scaler 執行優化器步驟 (如果梯度沒有 inf/nan)\n",
        "            scaler.step(optimizer)\n",
        "            # 4. 更新 scaler 的縮放因子\n",
        "            scaler.update()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                print(f\"Stage3 Step: {global_step}, Instruction Loss: {batch_loss:.4f}\")\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # 评估性能\n",
        "        if train_cfg.eval_in_epochs:\n",
        "            accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage3_best\")\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Loss: {avg_train_loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
        "        else:\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Instruction Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 保存最终模型\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/final_model\")\n",
        "    print(\"Stage 3 completed!\")\n",
        "    return model\n",
        "\n",
        "def train_three_stages(train_cfg, alm_cfg, device = None):\n",
        "    \"\"\"完整的三阶段训练\"\"\"\n",
        "    print(\"Starting Three-Stage Training Pipeline\")\n",
        "\n",
        "    # 第一阶段：模态投影器对齐\n",
        "    stage1_model = train_step1_alignment(train_cfg, alm_cfg, device=device)\n",
        "\n",
        "    # 第二阶段：语言模型预训练\n",
        "    stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model, device=device)\n",
        "\n",
        "    # 第三阶段：指令微调\n",
        "    final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model, device=device)\n",
        "\n",
        "    print(\"=== Training Pipeline Completed! ===\")\n",
        "    return stage1_model, stage2_model, final_model\n",
        "\n",
        "\n",
        "# # 替换原来的训练调用\n",
        "# alm_cfg = ALMConfig()\n",
        "# train_cfg = TrainConfig()\n",
        "\n",
        "# # 运行三阶段训练\n",
        "# final_model = train_three_stages(train_cfg, alm_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "### Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "BXUaUEUcJCp2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXUaUEUcJCp2",
        "outputId": "4951e693-874a-4221-ec77-da24c712bd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'checkpoints' already exists.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from backbone weights\n",
            "Successfully loaded HuggingFaceTB/SmolLM2-1.7B weights from safetensors. Model has 1,711,376,384 parameters.\n",
            "Loading weights from ../stage1/model.safetensors with non-strict loading.\n",
            "Weights loaded.\n",
            "Mismatched keys (should be decoder embedding/head): ['decoder.token_embedding.weight', 'decoder.head.weight']\n",
            "Unexpected keys: ['audio_encoder.decoder.embed_positions.weight', 'audio_encoder.decoder.embed_tokens.weight', 'audio_encoder.decoder.layer_norm.bias', 'audio_encoder.decoder.layer_norm.weight', 'audio_encoder.decoder.layers.0.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.0.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.0.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.0.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.0.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.0.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.0.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.0.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.0.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.0.fc1.bias', 'audio_encoder.decoder.layers.0.fc1.weight', 'audio_encoder.decoder.layers.0.fc2.bias', 'audio_encoder.decoder.layers.0.fc2.weight', 'audio_encoder.decoder.layers.0.final_layer_norm.bias', 'audio_encoder.decoder.layers.0.final_layer_norm.weight', 'audio_encoder.decoder.layers.0.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.0.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.0.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.0.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.0.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.0.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.0.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.0.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.0.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.1.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.1.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.1.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.1.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.1.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.1.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.1.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.1.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.1.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.1.fc1.bias', 'audio_encoder.decoder.layers.1.fc1.weight', 'audio_encoder.decoder.layers.1.fc2.bias', 'audio_encoder.decoder.layers.1.fc2.weight', 'audio_encoder.decoder.layers.1.final_layer_norm.bias', 'audio_encoder.decoder.layers.1.final_layer_norm.weight', 'audio_encoder.decoder.layers.1.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.1.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.1.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.1.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.1.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.1.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.1.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.1.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.1.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.10.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.10.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.10.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.10.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.10.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.10.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.10.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.10.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.10.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.10.fc1.bias', 'audio_encoder.decoder.layers.10.fc1.weight', 'audio_encoder.decoder.layers.10.fc2.bias', 'audio_encoder.decoder.layers.10.fc2.weight', 'audio_encoder.decoder.layers.10.final_layer_norm.bias', 'audio_encoder.decoder.layers.10.final_layer_norm.weight', 'audio_encoder.decoder.layers.10.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.10.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.10.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.10.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.10.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.10.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.10.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.10.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.10.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.11.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.11.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.11.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.11.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.11.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.11.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.11.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.11.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.11.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.11.fc1.bias', 'audio_encoder.decoder.layers.11.fc1.weight', 'audio_encoder.decoder.layers.11.fc2.bias', 'audio_encoder.decoder.layers.11.fc2.weight', 'audio_encoder.decoder.layers.11.final_layer_norm.bias', 'audio_encoder.decoder.layers.11.final_layer_norm.weight', 'audio_encoder.decoder.layers.11.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.11.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.11.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.11.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.11.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.11.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.11.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.11.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.11.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.2.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.2.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.2.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.2.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.2.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.2.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.2.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.2.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.2.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.2.fc1.bias', 'audio_encoder.decoder.layers.2.fc1.weight', 'audio_encoder.decoder.layers.2.fc2.bias', 'audio_encoder.decoder.layers.2.fc2.weight', 'audio_encoder.decoder.layers.2.final_layer_norm.bias', 'audio_encoder.decoder.layers.2.final_layer_norm.weight', 'audio_encoder.decoder.layers.2.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.2.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.2.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.2.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.2.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.2.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.2.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.2.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.2.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.3.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.3.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.3.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.3.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.3.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.3.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.3.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.3.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.3.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.3.fc1.bias', 'audio_encoder.decoder.layers.3.fc1.weight', 'audio_encoder.decoder.layers.3.fc2.bias', 'audio_encoder.decoder.layers.3.fc2.weight', 'audio_encoder.decoder.layers.3.final_layer_norm.bias', 'audio_encoder.decoder.layers.3.final_layer_norm.weight', 'audio_encoder.decoder.layers.3.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.3.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.3.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.3.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.3.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.3.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.3.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.3.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.3.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.4.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.4.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.4.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.4.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.4.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.4.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.4.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.4.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.4.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.4.fc1.bias', 'audio_encoder.decoder.layers.4.fc1.weight', 'audio_encoder.decoder.layers.4.fc2.bias', 'audio_encoder.decoder.layers.4.fc2.weight', 'audio_encoder.decoder.layers.4.final_layer_norm.bias', 'audio_encoder.decoder.layers.4.final_layer_norm.weight', 'audio_encoder.decoder.layers.4.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.4.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.4.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.4.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.4.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.4.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.4.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.4.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.4.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.5.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.5.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.5.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.5.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.5.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.5.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.5.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.5.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.5.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.5.fc1.bias', 'audio_encoder.decoder.layers.5.fc1.weight', 'audio_encoder.decoder.layers.5.fc2.bias', 'audio_encoder.decoder.layers.5.fc2.weight', 'audio_encoder.decoder.layers.5.final_layer_norm.bias', 'audio_encoder.decoder.layers.5.final_layer_norm.weight', 'audio_encoder.decoder.layers.5.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.5.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.5.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.5.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.5.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.5.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.5.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.5.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.5.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.6.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.6.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.6.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.6.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.6.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.6.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.6.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.6.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.6.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.6.fc1.bias', 'audio_encoder.decoder.layers.6.fc1.weight', 'audio_encoder.decoder.layers.6.fc2.bias', 'audio_encoder.decoder.layers.6.fc2.weight', 'audio_encoder.decoder.layers.6.final_layer_norm.bias', 'audio_encoder.decoder.layers.6.final_layer_norm.weight', 'audio_encoder.decoder.layers.6.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.6.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.6.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.6.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.6.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.6.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.6.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.6.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.6.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.7.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.7.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.7.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.7.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.7.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.7.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.7.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.7.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.7.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.7.fc1.bias', 'audio_encoder.decoder.layers.7.fc1.weight', 'audio_encoder.decoder.layers.7.fc2.bias', 'audio_encoder.decoder.layers.7.fc2.weight', 'audio_encoder.decoder.layers.7.final_layer_norm.bias', 'audio_encoder.decoder.layers.7.final_layer_norm.weight', 'audio_encoder.decoder.layers.7.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.7.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.7.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.7.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.7.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.7.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.7.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.7.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.7.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.8.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.8.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.8.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.8.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.8.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.8.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.8.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.8.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.8.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.8.fc1.bias', 'audio_encoder.decoder.layers.8.fc1.weight', 'audio_encoder.decoder.layers.8.fc2.bias', 'audio_encoder.decoder.layers.8.fc2.weight', 'audio_encoder.decoder.layers.8.final_layer_norm.bias', 'audio_encoder.decoder.layers.8.final_layer_norm.weight', 'audio_encoder.decoder.layers.8.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.8.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.8.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.8.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.8.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.8.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.8.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.8.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.8.self_attn_layer_norm.weight', 'audio_encoder.decoder.layers.9.encoder_attn.k_proj.weight', 'audio_encoder.decoder.layers.9.encoder_attn.out_proj.bias', 'audio_encoder.decoder.layers.9.encoder_attn.out_proj.weight', 'audio_encoder.decoder.layers.9.encoder_attn.q_proj.bias', 'audio_encoder.decoder.layers.9.encoder_attn.q_proj.weight', 'audio_encoder.decoder.layers.9.encoder_attn.v_proj.bias', 'audio_encoder.decoder.layers.9.encoder_attn.v_proj.weight', 'audio_encoder.decoder.layers.9.encoder_attn_layer_norm.bias', 'audio_encoder.decoder.layers.9.encoder_attn_layer_norm.weight', 'audio_encoder.decoder.layers.9.fc1.bias', 'audio_encoder.decoder.layers.9.fc1.weight', 'audio_encoder.decoder.layers.9.fc2.bias', 'audio_encoder.decoder.layers.9.fc2.weight', 'audio_encoder.decoder.layers.9.final_layer_norm.bias', 'audio_encoder.decoder.layers.9.final_layer_norm.weight', 'audio_encoder.decoder.layers.9.self_attn.k_proj.weight', 'audio_encoder.decoder.layers.9.self_attn.out_proj.bias', 'audio_encoder.decoder.layers.9.self_attn.out_proj.weight', 'audio_encoder.decoder.layers.9.self_attn.q_proj.bias', 'audio_encoder.decoder.layers.9.self_attn.q_proj.weight', 'audio_encoder.decoder.layers.9.self_attn.v_proj.bias', 'audio_encoder.decoder.layers.9.self_attn.v_proj.weight', 'audio_encoder.decoder.layers.9.self_attn_layer_norm.bias', 'audio_encoder.decoder.layers.9.self_attn_layer_norm.weight', 'audio_encoder.encoder.conv1.bias', 'audio_encoder.encoder.conv1.weight', 'audio_encoder.encoder.conv2.bias', 'audio_encoder.encoder.conv2.weight', 'audio_encoder.encoder.embed_positions.weight', 'audio_encoder.encoder.layer_norm.bias', 'audio_encoder.encoder.layer_norm.weight', 'audio_encoder.encoder.layers.0.fc1.bias', 'audio_encoder.encoder.layers.0.fc1.weight', 'audio_encoder.encoder.layers.0.fc2.bias', 'audio_encoder.encoder.layers.0.fc2.weight', 'audio_encoder.encoder.layers.0.final_layer_norm.bias', 'audio_encoder.encoder.layers.0.final_layer_norm.weight', 'audio_encoder.encoder.layers.0.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.0.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.0.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.0.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.0.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.0.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.0.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.0.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.0.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.1.fc1.bias', 'audio_encoder.encoder.layers.1.fc1.weight', 'audio_encoder.encoder.layers.1.fc2.bias', 'audio_encoder.encoder.layers.1.fc2.weight', 'audio_encoder.encoder.layers.1.final_layer_norm.bias', 'audio_encoder.encoder.layers.1.final_layer_norm.weight', 'audio_encoder.encoder.layers.1.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.1.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.1.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.1.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.1.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.1.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.1.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.1.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.1.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.10.fc1.bias', 'audio_encoder.encoder.layers.10.fc1.weight', 'audio_encoder.encoder.layers.10.fc2.bias', 'audio_encoder.encoder.layers.10.fc2.weight', 'audio_encoder.encoder.layers.10.final_layer_norm.bias', 'audio_encoder.encoder.layers.10.final_layer_norm.weight', 'audio_encoder.encoder.layers.10.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.10.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.10.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.10.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.10.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.10.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.10.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.10.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.10.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.11.fc1.bias', 'audio_encoder.encoder.layers.11.fc1.weight', 'audio_encoder.encoder.layers.11.fc2.bias', 'audio_encoder.encoder.layers.11.fc2.weight', 'audio_encoder.encoder.layers.11.final_layer_norm.bias', 'audio_encoder.encoder.layers.11.final_layer_norm.weight', 'audio_encoder.encoder.layers.11.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.11.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.11.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.11.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.11.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.11.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.11.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.11.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.11.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.2.fc1.bias', 'audio_encoder.encoder.layers.2.fc1.weight', 'audio_encoder.encoder.layers.2.fc2.bias', 'audio_encoder.encoder.layers.2.fc2.weight', 'audio_encoder.encoder.layers.2.final_layer_norm.bias', 'audio_encoder.encoder.layers.2.final_layer_norm.weight', 'audio_encoder.encoder.layers.2.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.2.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.2.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.2.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.2.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.2.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.2.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.2.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.2.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.3.fc1.bias', 'audio_encoder.encoder.layers.3.fc1.weight', 'audio_encoder.encoder.layers.3.fc2.bias', 'audio_encoder.encoder.layers.3.fc2.weight', 'audio_encoder.encoder.layers.3.final_layer_norm.bias', 'audio_encoder.encoder.layers.3.final_layer_norm.weight', 'audio_encoder.encoder.layers.3.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.3.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.3.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.3.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.3.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.3.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.3.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.3.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.3.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.4.fc1.bias', 'audio_encoder.encoder.layers.4.fc1.weight', 'audio_encoder.encoder.layers.4.fc2.bias', 'audio_encoder.encoder.layers.4.fc2.weight', 'audio_encoder.encoder.layers.4.final_layer_norm.bias', 'audio_encoder.encoder.layers.4.final_layer_norm.weight', 'audio_encoder.encoder.layers.4.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.4.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.4.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.4.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.4.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.4.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.4.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.4.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.4.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.5.fc1.bias', 'audio_encoder.encoder.layers.5.fc1.weight', 'audio_encoder.encoder.layers.5.fc2.bias', 'audio_encoder.encoder.layers.5.fc2.weight', 'audio_encoder.encoder.layers.5.final_layer_norm.bias', 'audio_encoder.encoder.layers.5.final_layer_norm.weight', 'audio_encoder.encoder.layers.5.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.5.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.5.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.5.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.5.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.5.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.5.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.5.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.5.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.6.fc1.bias', 'audio_encoder.encoder.layers.6.fc1.weight', 'audio_encoder.encoder.layers.6.fc2.bias', 'audio_encoder.encoder.layers.6.fc2.weight', 'audio_encoder.encoder.layers.6.final_layer_norm.bias', 'audio_encoder.encoder.layers.6.final_layer_norm.weight', 'audio_encoder.encoder.layers.6.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.6.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.6.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.6.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.6.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.6.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.6.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.6.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.6.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.7.fc1.bias', 'audio_encoder.encoder.layers.7.fc1.weight', 'audio_encoder.encoder.layers.7.fc2.bias', 'audio_encoder.encoder.layers.7.fc2.weight', 'audio_encoder.encoder.layers.7.final_layer_norm.bias', 'audio_encoder.encoder.layers.7.final_layer_norm.weight', 'audio_encoder.encoder.layers.7.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.7.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.7.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.7.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.7.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.7.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.7.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.7.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.7.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.8.fc1.bias', 'audio_encoder.encoder.layers.8.fc1.weight', 'audio_encoder.encoder.layers.8.fc2.bias', 'audio_encoder.encoder.layers.8.fc2.weight', 'audio_encoder.encoder.layers.8.final_layer_norm.bias', 'audio_encoder.encoder.layers.8.final_layer_norm.weight', 'audio_encoder.encoder.layers.8.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.8.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.8.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.8.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.8.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.8.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.8.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.8.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.8.self_attn_layer_norm.weight', 'audio_encoder.encoder.layers.9.fc1.bias', 'audio_encoder.encoder.layers.9.fc1.weight', 'audio_encoder.encoder.layers.9.fc2.bias', 'audio_encoder.encoder.layers.9.fc2.weight', 'audio_encoder.encoder.layers.9.final_layer_norm.bias', 'audio_encoder.encoder.layers.9.final_layer_norm.weight', 'audio_encoder.encoder.layers.9.self_attn.k_proj.weight', 'audio_encoder.encoder.layers.9.self_attn.out_proj.bias', 'audio_encoder.encoder.layers.9.self_attn.out_proj.weight', 'audio_encoder.encoder.layers.9.self_attn.q_proj.bias', 'audio_encoder.encoder.layers.9.self_attn.q_proj.weight', 'audio_encoder.encoder.layers.9.self_attn.v_proj.bias', 'audio_encoder.encoder.layers.9.self_attn.v_proj.weight', 'audio_encoder.encoder.layers.9.self_attn_layer_norm.bias', 'audio_encoder.encoder.layers.9.self_attn_layer_norm.weight']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from models.config import ALMConfig, TrainConfig\n",
        "\n",
        "# 要創建的目錄路徑\n",
        "dir_name = ALMConfig.alm_checkpoint_path\n",
        "\n",
        "try:\n",
        "    os.mkdir(dir_name)\n",
        "    print(f\"Directory '{dir_name}' created successfully.\")\n",
        "except FileExistsError:\n",
        "    print(f\"Directory '{dir_name}' already exists.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Parent directory does not exist for '{dir_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "alm_cfg = ALMConfig()\n",
        "train_cfg = TrainConfig()\n",
        "\n",
        "tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "# 創建一個帶有新 token 的模型實例\n",
        "model = AudioLanguageModel(alm_cfg, load_backbone=True, tokenizer=tokenizer, device=device)\n",
        "stage1_model = None\n",
        "\n",
        "if train_cfg.resume_from_alm_checkpoint:\n",
        "    # checkpoint_path = \"../stage1/model.safetensors\"\n",
        "    checkpoint_path = \"./checkpoints/stage2_final/model.safetensors\"\n",
        "\n",
        "    print(f\"Loading weights from {checkpoint_path} with non-strict loading.\")\n",
        "\n",
        "    # 載入舊的 state_dict\n",
        "    # 注意：這裡假設您的權重是以 safetensors 格式保存的\n",
        "    from safetensors.torch import load_file\n",
        "    state_dict = load_file(checkpoint_path, device=\"cpu\") # move to GPU later\n",
        "    state_dict.pop('decoder.token_embedding.weight', None)\n",
        "    state_dict.pop('decoder.head.weight', None)\n",
        "    # *** 修改結束 ***\n",
        "\n",
        "    # 使用 strict=False 來載入權重\n",
        "    # 這會載入所有維度匹配的權重（例如 MP），並忽略不匹配的（例如 decoder.head）\n",
        "    incompatible_keys = model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    print(\"Weights loaded.\")\n",
        "    print(\"Mismatched keys (should be decoder embedding/head):\", incompatible_keys.missing_keys)\n",
        "    print(\"Unexpected keys:\", incompatible_keys.unexpected_keys)\n",
        "\n",
        "# 將模型移動到正確的設備\n",
        "model.to(device)\n",
        "stage1_model = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "id": "9MlFpXQFSNdx"
      },
      "outputs": [],
      "source": [
        "# stage1_model = train_step1_alignment(train_cfg, alm_cfg, model, tokenizer, device)\n",
        "# stage1_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8ebd83a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ebd83a0",
        "outputId": "42513862-02a8-4ae7-bd7c-453e69aa7245"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Stage 2: Language Model Pretraining ===\n",
            "AudioProcessor_from_HF initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
            "  Target feature frames from cfg: 1500\n",
            "  Using model sampling rate: 16000, hop_length: 160, n_fft: 400\n",
            "  Calculated max raw audio samples for processor: 240240\n",
            "Stage 2: Training 1,981,650,944 parameters\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage2 Epoch 1: 100%|██████████| 205/205 [05:48<00:00,  1.70s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Average alignment score: 0.7063\n",
            " \n",
            "Stage2 Epoch 1/12 | Loss: 7.6514 | Alignment: 0.7063\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage2 Epoch 2: 100%|██████████| 205/205 [05:49<00:00,  1.71s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Average alignment score: 0.7188\n",
            " \n",
            "Stage2 Epoch 2/12 | Loss: 4.2688 | Alignment: 0.7188\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage2 Epoch 3: 100%|██████████| 205/205 [05:49<00:00,  1.71s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Average alignment score: 0.7188\n",
            " \n",
            "Stage2 Epoch 3/12 | Loss: 4.0599 | Alignment: 0.7188\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage2 Epoch 4: 100%|██████████| 205/205 [05:49<00:00,  1.71s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Average alignment score: 0.7250\n",
            " \n",
            "Stage2 Epoch 4/12 | Loss: 3.9534 | Alignment: 0.7250\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage2 Epoch 5: 100%|██████████| 205/205 [05:49<00:00,  1.70s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Average alignment score: 0.6937\n",
            " \n",
            "Stage2 Epoch 5/12 | Loss: 3.8855 | Alignment: 0.6937\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage2 Epoch 6: 100%|██████████| 205/205 [05:49<00:00,  1.71s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Average alignment score: 0.7063\n",
            " \n",
            "Stage2 Epoch 6/12 | Loss: 3.8272 | Alignment: 0.7063\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stage2 Epoch 7: 100%|██████████| 205/205 [05:50<00:00,  1.71s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Average alignment score: 0.7000\n",
            " \n",
            "Stage2 Epoch 7/12 | Loss: 3.7754 | Alignment: 0.7000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 8: 100%|██████████| 205/205 [05:49<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Average alignment score: 0.7125\n",
            " \n",
            "Stage2 Epoch 8/12 | Loss: 3.7260 | Alignment: 0.7125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 9: 100%|██████████| 205/205 [05:49<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Average alignment score: 0.7188\n",
            " \n",
            "Stage2 Epoch 9/12 | Loss: 3.6995 | Alignment: 0.7188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 10: 100%|██████████| 205/205 [05:48<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Average alignment score: 0.7125\n",
            " \n",
            "Stage2 Epoch 10/12 | Loss: 3.6679 | Alignment: 0.7125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 11: 100%|██████████| 205/205 [05:47<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Average alignment score: 0.7125\n",
            " \n",
            "Stage2 Epoch 11/12 | Loss: 3.6635 | Alignment: 0.7125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 12: 100%|██████████| 205/205 [05:48<00:00,  1.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Average alignment score: 0.7125\n",
            " \n",
            "Stage2 Epoch 12/12 | Loss: 3.6554 | Alignment: 0.7125\n",
            "Stage 2 completed!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model, tokenizer, device)\n",
        "stage2_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3c8aecfe",
      "metadata": {
        "id": "3c8aecfe",
        "outputId": "c15a2c1b-7ae1-4aa1-b3a5-b528518e6cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Stage 3: Instruction Tuning ===\n",
            "AudioProcessor_from_HF initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
            "  Target feature frames from cfg: 1500\n",
            "  Using model sampling rate: 16000, hop_length: 160, n_fft: 400\n",
            "  Calculated max raw audio samples for processor: 240240\n",
            "Stage 3: Training all 1,981,650,944 parameters\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'AudioTransformer_from_HF' object has no attribute 'parameters'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-3013684695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step3_instruction_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malm_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-11-3616923166.py\u001b[0m in \u001b[0;36mtrain_step3_instruction_tuning\u001b[0;34m(train_cfg, alm_cfg, stage2_model, tokenizer, device)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_backbones\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_backbones\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     ]\n\u001b[1;32m    324\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'AudioTransformer_from_HF' object has no attribute 'parameters'"
          ]
        }
      ],
      "source": [
        "final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model, tokenizer, device)\n",
        "final_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mZWoCjkGGfMQ",
      "metadata": {
        "id": "mZWoCjkGGfMQ"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "kjOsHkjpCoYp",
      "metadata": {
        "id": "kjOsHkjpCoYp"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/nanoALM/output_txt1.wav /content/output_txt1.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt2.wav /content/output_txt2.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt3.wav /content/output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "07dM8znZPTSj",
      "metadata": {
        "id": "07dM8znZPTSj",
        "outputId": "8b2f7879-77e1-4b4e-a8a4-d6ba6021495a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat './model.safetensors': No such file or directory\n",
            "cp: cannot stat './config.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp ../model.safetensors /content/drive/MyDrive/nanoALM/model.safetensors\n",
        "!cp ../config.json /content/drive/MyDrive/nanoALM/config.json\n",
        "!cp ./model.safetensors /content/drive/MyDrive/nanoALM\n",
        "!cp ./config.json /content/drive/MyDrive/nanoALM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "RSbBheB3HGns",
      "metadata": {
        "collapsed": true,
        "id": "RSbBheB3HGns",
        "outputId": "c33e515d-316b-473c-89e3-5e93915acac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-30 03:06:34.340178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753844794.361295   31312 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753844794.367746   31312 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Using device: cuda\n",
            "Loading weights from: ../\n",
            "=================================================\n",
            "load model from local\n",
            "=================================================\n",
            "AudioProcessor_from_HF initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
            "  Target feature frames from cfg: 1500\n",
            "  Using model sampling rate: 16000, hop_length: 160, n_fft: 400\n",
            "  Calculated max raw audio samples for processor: 240240\n",
            "\n",
            "Input:\n",
            "  What is said in this audio? \n",
            "\n",
            "Outputs:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoVLM_From_Huggingface/generate.py\", line 96, in <module>\n",
            "    main()\n",
            "  File \"/content/nanoVLM_From_Huggingface/generate.py\", line 83, in main\n",
            "    gen = model.generate(\n",
            "          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoVLM_From_Huggingface/models/audio_language_model.py\", line 217, in generate\n",
            "    current_sequence_embeds, current_attention_mask = self._prepare_decoder_inputs(\n",
            "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoVLM_From_Huggingface/models/audio_language_model.py\", line 57, in _prepare_decoder_inputs\n",
            "    audio_features = self.audio_encoder.forward(input_features, output_hidden_states=True)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/nanoVLM_From_Huggingface/models/audio_transformer.py\", line 313, in forward\n",
            "    encoder_outputs = self.audio_encoder(audio, output_hidden_states=output_hidden_states)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 677, in forward\n",
            "    inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 375, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 370, in _conv_forward\n",
            "    return F.conv1d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n"
          ]
        }
      ],
      "source": [
        "# final_model.save_pretrained(\"/content/\")\n",
        "!python generate.py --checkpoint ../ --audio ../output_txt1.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "FN6ra31lP8QM",
      "metadata": {
        "collapsed": true,
        "id": "FN6ra31lP8QM",
        "outputId": "1562328b-6da7-447a-a95d-210fa21bb5a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/generate.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt2.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eo_p1E3xP8gd",
      "metadata": {
        "collapsed": true,
        "id": "eo_p1E3xP8gd",
        "outputId": "74e86570-fb6c-465e-99be-5da9a0377948",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-30 03:17:31.903458: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-07-30 03:17:31.921732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753845451.943344   34381 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753845451.949987   34381 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-30 03:17:31.972157: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Using device: cuda\n",
            "Loading weights from: ../\n",
            "=================================================\n",
            "load model from local\n",
            "=================================================\n",
            "AudioProcessor_from_HF initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
            "  Target feature frames from cfg: 1500\n",
            "  Using model sampling rate: 16000, hop_length: 160, n_fft: 400\n",
            "  Calculated max raw audio samples for processor: 240240\n",
            "Error loading audio file: operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (3,2)\n",
            "Please check if the audio file exists and is in a supported format.\n"
          ]
        }
      ],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0cMUB4lb4zR",
      "metadata": {
        "id": "T0cMUB4lb4zR"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a ALM in Google Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "ab77f5b6-5e9a-44a1-db14-ac2e825108af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoVLM_From_Huggingface'...\n",
            "remote: Enumerating objects: 1124, done.\u001b[K\n",
            "remote: Counting objects: 100% (499/499), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 1124 (delta 385), reused 340 (delta 340), pack-reused 625 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1124/1124), 15.66 MiB | 17.45 MiB/s, done.\n",
            "Resolving deltas: 100% (766/766), done.\n",
            "/content/nanoVLM_From_Huggingface\n",
            "assets\t\t\tdebug_tokenizer_dataset_compatibility.py  old\n",
            "benchmark-inference.py\tgenerate.py\t\t\t\t  README.md\n",
            "benchmark_suite.py\tmeasure_vram.py\t\t\t\t  test\n",
            "data\t\t\tmodels\n",
            "debug_func.py\t\tnanoALM.ipynb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoALM'):\n",
        "    !git clone https://github.com/LWL220184016/nanoVLM_From_Huggingface.git\n",
        "%cd nanoVLM_From_Huggingface/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d34d70c6",
      "metadata": {
        "id": "d34d70c6",
        "outputId": "9dc73468-5478-4c22-a1f3-de8a5da29ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp -r /content/drive/MyDrive/nanoALM/6/stage2 /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54bc8463",
      "metadata": {
        "id": "54bc8463"
      },
      "outputs": [],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "# !huggingface-cli login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "collapsed": true,
        "id": "bcw8qQqoOSR7",
        "outputId": "08d4a471-4ba7-4b0d-a7dd-071fb8fddefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==3.6.0\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed datasets-3.6.0\n"
          ]
        }
      ],
      "source": [
        "# # If you get an \"Error\" from pip's dependency resolver but the cell complets fine, this is not an issue, you can continue :)\n",
        "# !pip -q install torch\n",
        "# !pip -q install gcsfs\n",
        "# !pip -q install tqdm\n",
        "# !pip -q install huggingface_hub\n",
        "# !pip -q install librosa\n",
        "# !pip install soundfile librosa -q\n",
        "# # !pip install --upgrade transformers\n",
        "# !pip install datasets==3.6.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoALM\"\n",
        "# hf_model_name = \"YOUR_HF_USER_NAME/nanoALM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "0a90d73c-59b6-478b-f15a-7272906e47d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# nanoALM Imports (please check out the implementations in detail, that's where all the interessting stuff is!)\n",
        "from data.collators import AlignmentCollator, AudioQACollator\n",
        "from data.datasets import SAVEEDataset, AudioQADataset\n",
        "from data.processors import get_audio_processor\n",
        "from data.processors import get_tokenizer\n",
        "from models.audio_language_model import AudioLanguageModel\n",
        "import models.utils as utils\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "#Otherwise, the tokenizer will through a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    try:\n",
        "        import torch_xla\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        device = xm.xla_device()\n",
        "    except ImportError:\n",
        "        device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "trained_model = None\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "n4oXPgTxQShw"
      },
      "id": "n4oXPgTxQShw"
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, alm_cfg, tokenizer):\n",
        "    # Create datasets\n",
        "    audio_processor = get_audio_processor(alm_cfg)\n",
        "\n",
        "    # text = \"splitting datasets, disable in get_dataloaders function\"\n",
        "    # print(f\"\\n\\033[38;5;05m{text}05m\\033[0m\")\n",
        "    # Load and combine all training datasets\n",
        "    combined_train_data = []\n",
        "    for dataset_name in train_cfg.train_dataset_name:\n",
        "        train_ds = load_dataset(\n",
        "        path = train_cfg.train_dataset_path,\n",
        "        name = dataset_name,\n",
        "    )\n",
        "        combined_train_data.append(train_ds['train'])\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    test_ds = load_dataset(train_cfg.test_dataset_path)\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    train_dataset = AudioQADataset(train_ds.select(range(train_size)), tokenizer, audio_processor)\n",
        "    val_dataset = AudioQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, audio_processor)\n",
        "    test_dataset = SAVEEDataset(test_ds, tokenizer, audio_processor)\n",
        "\n",
        "    # Create collators\n",
        "    alignment_collator = AlignmentCollator(tokenizer, alm_cfg.lm_max_length, audio_processor)\n",
        "    aqa_collator = AudioQACollator(tokenizer, alm_cfg.lm_max_length)\n",
        "    savee_collator = AudioQACollator(tokenizer, alm_cfg.lm_max_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "    alignment_train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=alignment_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=train_cfg.savee_batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=savee_collator,\n",
        "        pin_memory=True,\n",
        "        )\n",
        "\n",
        "    return alignment_train_loader, train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D7NIuEDuOuuJ",
      "metadata": {
        "id": "D7NIuEDuOuuJ"
      },
      "source": [
        "### Prepare the testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9fnh6wOlOzat",
      "metadata": {
        "id": "9fnh6wOlOzat"
      },
      "outputs": [],
      "source": [
        "def test_savee(model, tokenizer, test_loader, device):\n",
        "    total_examples = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            audio = batch['audios'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            gen = model.generate(input_ids, audio, attention_mask)\n",
        "            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "            is_correct = utils.check_multiple_choice_with_regex(model_output, correct_answer)\n",
        "\n",
        "            total_examples += len(is_correct)\n",
        "            if is_correct:\n",
        "                correct_predictions += sum(is_correct)\n",
        "    accuracy = correct_predictions / total_examples if total_examples > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def get_avg_alignment(model, val_loader, device, epoch):\n",
        "    \"\"\"\n",
        "    Validate the model's audio-text alignment on the validation set.\n",
        "    This function computes the average alignment score over the validation set.\n",
        "    It runs for a maximum of 20 batches to save time during training.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_alignment_score = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            if i >= 20:  # 只驗證前20個batch以節省時間\n",
        "                break\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            alignment_score = model.validate_audio_text_alignment(input_ids, audios)\n",
        "            total_alignment_score += alignment_score\n",
        "\n",
        "    avg_alignment = total_alignment_score / min(20, len(val_loader))\n",
        "    print(f\"Epoch {epoch+1}: Average alignment score: {avg_alignment:.4f}\")\n",
        "\n",
        "    print(\" \")\n",
        "    model.train()\n",
        "    return avg_alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe359194",
      "metadata": {
        "id": "fe359194"
      },
      "source": [
        "### Add debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5d7b7e46",
      "metadata": {
        "id": "5d7b7e46"
      },
      "outputs": [],
      "source": [
        "# 在训练开始前添加这个检查函数\n",
        "def debug_model_dimensions(model, input_ids, audio):\n",
        "    \"\"\"调试模型各层的维度\"\"\"\n",
        "    print(\"=== Model Dimension Debug ===\")\n",
        "\n",
        "    # 检查音频编码器\n",
        "    audio_features = model.audio_encoder.forward(audio, output_hidden_states=True)\n",
        "    print(f\"Audio features shape: {audio_features.shape}\")\n",
        "\n",
        "    # 检查模态投影器\n",
        "    audio_embeds = model.MP(audio_features)\n",
        "    print(f\"Audio embeds shape: {audio_embeds.shape}\")\n",
        "\n",
        "    # 检查文本嵌入\n",
        "    text_embeds = model.decoder.token_embedding(input_ids)\n",
        "    print(f\"Text embeds shape: {text_embeds.shape}\")\n",
        "\n",
        "    # 检查拼接后的嵌入\n",
        "    inputs_embeds = torch.cat([audio_embeds, text_embeds], dim=1)\n",
        "    print(f\"Combined embeds shape: {inputs_embeds.shape}\")\n",
        "\n",
        "    # 检查语言模型输出\n",
        "    logits = model.decoder(inputs_embeds)\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "    print(f\"Vocab size (last dim): {logits.shape[-1]}\")\n",
        "\n",
        "    # 检查语言模型配置\n",
        "    print(f\"LM vocab size config: {model.cfg.lm_vocab_size}\")\n",
        "    print(f\"Decoder vocab size: {getattr(model.decoder, 'vocab_size', 'Not found')}\")\n",
        "\n",
        "    return logits.shape[-1]\n",
        "\n",
        "# 在训练循环开始前调用\n",
        "# vocab_size = debug_model_dimensions(model, input_ids, audios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2d48d0df",
      "metadata": {
        "id": "2d48d0df"
      },
      "outputs": [],
      "source": [
        "def debug_training_step(model, input_ids, audios, attention_mask, labels):\n",
        "    \"\"\"调试训练步骤\"\"\"\n",
        "    # 添加这些调试行：\n",
        "    print(f\"Batch debug - input_ids shape: {input_ids.shape}, max: {input_ids.max().item()}\")\n",
        "    print(f\"Batch debug - labels shape: {labels.shape}, max: {labels.max().item()}\")\n",
        "    print(f\"Batch debug - Model vocab config: {model.cfg.lm_vocab_size}\")\n",
        "\n",
        "    # 检查decoder的实际vocab_size\n",
        "    if hasattr(model.decoder, 'head') and hasattr(model.decoder.head, 'out_features'):\n",
        "        print(f\"Decoder head in_features: {model.decoder.head.in_features}\")\n",
        "        print(f\"Decoder head out_features: {model.decoder.head.out_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04da5b01",
      "metadata": {
        "id": "04da5b01"
      },
      "source": [
        "#### Three-stage training (contrast training, generative training, instruction fine-tuning) 三段式訓練(對比訓練, 生成式訓練, 指令微調)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fbb323c8",
      "metadata": {
        "id": "fbb323c8"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.amp as GradScaler\n",
        "\n",
        "from debug_func import debug_contrastive_learning\n",
        "\n",
        "# 改進對比學習訓練\n",
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def get_contrastive_loss(audio_embeds, text_embeds, temperature=0.07):\n",
        "    \"\"\"\n",
        "    標準、高效的對比學習損失 (CLIP Loss)。\n",
        "    注意：輸入的 embeds 應該是池化後的 [B, D] 維度向量。\n",
        "    \"\"\"\n",
        "    # --- 開始計算尺度對齊損失 ---\n",
        "\n",
        "    # 1. 計算每個向量的 L2 範數 (沿著最後一個維度)\n",
        "    #    detach() 是為了確保這個輔助損失的梯度只流向投影器，而不影響上游的編碼器或文字嵌入層的穩定性（可選但推薦）\n",
        "    audio_norms = torch.norm(audio_embeds, p=2, dim=-1)\n",
        "    text_norms = torch.norm(text_embeds, p=2, dim=-1)\n",
        "\n",
        "    # 2. 計算批次內的平均範數\n",
        "    mean_audio_norm = torch.mean(audio_norms)\n",
        "    mean_text_norm = torch.mean(text_norms)\n",
        "\n",
        "    # 3. 計算尺度對齊損失 (使用 MSE)\n",
        "    #    目標是讓 mean_audio_norm 和 mean_text_norm 盡可能接近\n",
        "    scale_loss = F.mse_loss(mean_audio_norm, mean_text_norm)\n",
        "\n",
        "    # --- 結束計算尺度對齊損失 ---\n",
        "\n",
        "    # --- 開始計算交叉熵損失 ---\n",
        "    # 歸一化\n",
        "    audio_embeds = F.normalize(audio_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "\n",
        "    # 計算相似度矩陣\n",
        "    # temperature 是一個重要的超參數，CLIP 論文中是可學習的，但固定值也可以\n",
        "    logits_per_audio = torch.matmul(audio_embeds, text_embeds.T) / temperature\n",
        "    logits_per_text = logits_per_audio.T\n",
        "\n",
        "    # 創建標籤 (0, 1, 2, ..., B-1)\n",
        "    labels = torch.arange(audio_embeds.shape[0]).to(logits_per_audio.device)\n",
        "\n",
        "    # 對稱的交叉熵損失\n",
        "    loss_a = F.cross_entropy(logits_per_audio, labels)\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "\n",
        "    contrastive_loss = (loss_a + loss_t) / 2\n",
        "\n",
        "    # --- 結束計算交叉熵損失 ---\n",
        "\n",
        "\n",
        "    # 4. 組合損失\n",
        "    #    lambda_scale 是一個需要調整的超參數，用來平衡兩個損失的權重\n",
        "    lambda_scale = 0.001  # 範例值，可以從 0.01, 0.1, 1.0 等開始嘗試\n",
        "    total_loss = contrastive_loss + lambda_scale * scale_loss\n",
        "\n",
        "    # 監控指標 (可選但推薦)\n",
        "    with torch.no_grad():\n",
        "        pos_sim = torch.diagonal(logits_per_audio * temperature).mean()\n",
        "        mask = ~torch.eye(labels.shape[0], dtype=torch.bool, device=labels.device)\n",
        "        neg_sim = (logits_per_audio * temperature)[mask].mean()\n",
        "\n",
        "    return total_loss, contrastive_loss, scale_loss, {\n",
        "        \"loss\": total_loss.item(),\n",
        "        \"pos_sim\": pos_sim.item(), # 正樣本對的餘弦相似度\n",
        "        \"neg_sim\": neg_sim.item()  # 負樣本對的餘弦相似度\n",
        "    }\n",
        "\n",
        "def train_step1_alignment(train_cfg, alm_cfg, model=None, tokenizer = None, device = None):\n",
        "    # 凍結音頻編碼器和語言模型\n",
        "    model.audio_encoder.audio_encoder.requires_grad_(False)\n",
        "    model.decoder.requires_grad_(False)\n",
        "    model.MP.requires_grad_(True)\n",
        "\n",
        "    alignment_train_loader, _, val_loader, _ = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "\n",
        "    optimizer = optim.AdamW(model.MP.parameters(), lr=train_cfg.lr_mp, weight_decay=0.01)\n",
        "\n",
        "    best_alignment = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage1_epochs):\n",
        "        model.train()\n",
        "        total_contrastive_loss = 0  # 添加這個變數初始化\n",
        "        total_scale_loss = 0  # 添加這個變數初始化\n",
        "\n",
        "        for batch in tqdm(alignment_train_loader, desc=f\"Stage1 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. 音頻編碼 -> 投影\n",
        "            with torch.no_grad():\n",
        "                audio_features = model.audio_encoder.audio_encoder(audios, output_hidden_states=True)\n",
        "            projected_audio_features = model.MP(audio_features.last_hidden_state)\n",
        "\n",
        "            # 2. 文本編碼 - 修復這裡的問題\n",
        "            with torch.no_grad():\n",
        "                # 檢查 decoder 的 forward 方法簽名\n",
        "                # 根據 language_model.py，應該傳入 x 而不是分別的參數\n",
        "                text_embeds = model.decoder.token_embedding(input_ids)  # 直接獲取文本嵌入\n",
        "\n",
        "                # 如果需要通過完整的 decoder，使用以下方式：\n",
        "                # text_outputs, _ = model.decoder(text_embeds, attention_mask=attention_mask)\n",
        "                # text_embeds = text_outputs  # 使用輸出的嵌入\n",
        "\n",
        "            # 3. 池化操作 (Pooling)\n",
        "            # 音頻池化\n",
        "            audio_pooled = projected_audio_features.mean(dim=1)  # [B, D]\n",
        "\n",
        "            # 文本池化 - 修復維度問題\n",
        "            # text_embeds 現在是 [B, seq_len, hidden_dim]\n",
        "            if attention_mask is not None:\n",
        "                # 根據 attention_mask 來安全地做平均池化\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(text_embeds.size()).float()\n",
        "                sum_embeddings = torch.sum(text_embeds * input_mask_expanded, 1)\n",
        "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                text_pooled = sum_embeddings / sum_mask  # [B, D]\n",
        "            else:\n",
        "                text_pooled = text_embeds.mean(dim=1)  # [B, D]\n",
        "\n",
        "            # 如果維度仍然不匹配，添加投影層\n",
        "            if audio_pooled.shape[-1] != text_pooled.shape[-1]:\n",
        "                # 創建一個投影層來匹配維度\n",
        "                if not hasattr(model, 'text_projection'):\n",
        "                    model.text_projection = nn.Linear(text_pooled.shape[-1], audio_pooled.shape[-1]).to(device)\n",
        "                text_pooled = model.text_projection(text_pooled)\n",
        "\n",
        "            # 4. 計算對比損失\n",
        "            loss, contrastive_loss, scale_loss, metrics = get_contrastive_loss(audio_pooled, text_pooled)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.MP.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_contrastive_loss += contrastive_loss.item()\n",
        "            total_scale_loss += scale_loss.item()\n",
        "\n",
        "        avg_contrastive_loss = total_contrastive_loss / len(alignment_train_loader)\n",
        "        avg_scale_loss = total_scale_loss / len(alignment_train_loader)\n",
        "        print(f\"Stage1 Epoch {epoch+1}: Total Loss {loss:.4f}, Contrastive Loss {avg_contrastive_loss:.4f}, Scale Loss {avg_scale_loss:.4f}\")\n",
        "\n",
        "        avg_alignment = get_avg_alignment(model, val_loader, device, epoch)\n",
        "\n",
        "        if avg_alignment > best_alignment:\n",
        "            best_alignment = avg_alignment\n",
        "            model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage1_best\")\n",
        "            print(f\"  New best alignment: {best_alignment:.4f}\")\n",
        "\n",
        "    print(f\"Stage 1 completed! Best alignment: {best_alignment:.4f}\")\n",
        "    return model\n",
        "\n",
        "def train_step2_pretraining(train_cfg, alm_cfg, stage1_model=None, tokenizer=None, device = None):\n",
        "    \"\"\"第二步：语言模型预训练\"\"\"\n",
        "    print(\"=== Stage 2: Language Model Pretraining ===\")\n",
        "\n",
        "    _, train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "    tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "    # 加载第一阶段模型或从头开始\n",
        "    if stage1_model is not None:\n",
        "        model = stage1_model\n",
        "    else:\n",
        "        try:\n",
        "            model = AudioLanguageModel.from_pretrained(f\"{alm_cfg.alm_checkpoint_path}/stage1_final\")\n",
        "            print(\"Loaded Stage 1 model\")\n",
        "        except:\n",
        "            model = AudioLanguageModel(alm_cfg, load_backbone=True, tokenizer=tokenizer, device=device)\n",
        "            print(\"Starting Stage 2 from scratch\")\n",
        "\n",
        "    # 冻结音频编码器，解冻语言模型和模态投影器\n",
        "    for param in model.audio_encoder.audio_encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.decoder.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.MP.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Stage 2: Training {trainable_params:,} parameters\")\n",
        "\n",
        "    # 不同学习率\n",
        "    param_groups = [\n",
        "        {'params': model.MP.parameters(), 'lr': train_cfg.lr_mp * 0.1},\n",
        "        {'params': model.decoder.parameters(), 'lr': train_cfg.lr_backbones}\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "    scaler = torch.amp.GradScaler(device=device)\n",
        "\n",
        "\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    best_loss = float('inf')\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage2_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage2 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                # 使用因果语言建模损失\n",
        "                logits, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            # print(\"loss: \", loss)\n",
        "            scaler.scale(loss).backward()\n",
        "            # loss.backward()\n",
        "\n",
        "            # 动态学习率调整\n",
        "            adj_lr_mp = get_lr(global_step, train_cfg.lr_mp * 0.1, len(train_loader) * train_cfg.stage2_epochs)\n",
        "            adj_lr_backbones = get_lr(global_step, train_cfg.lr_backbones, len(train_loader) * train_cfg.stage2_epochs)\n",
        "            optimizer.param_groups[0]['lr'] = adj_lr_mp\n",
        "            optimizer.param_groups[1]['lr'] = adj_lr_backbones\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            # 2. 對 unscale 後的梯度進行裁剪 (max_norm=1.0 是一個常用的值)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            # --- 修改結束 ---\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "                print(\"!!! Loss is NaN before backward pass. Problem is in the forward pass.\")\n",
        "                print(loss)\n",
        "                # 檢查 logits 是否包含 NaN 或 inf\n",
        "                if torch.isnan(logits).any():\n",
        "                    print(\"!!! Logits contain NaN.\")\n",
        "                if torch.isinf(logits).any():\n",
        "                    print(\"!!! Logits contain Inf.\")\n",
        "                # 在此處中斷，以便檢查\n",
        "                import sys; sys.exit()\n",
        "            # --- 調試碼結束 ---\n",
        "\n",
        "            # # 3. Scaler 執行優化器步驟 (如果梯度沒有 inf/nan)\n",
        "            scaler.step(optimizer)\n",
        "            # # 4. 更新 scaler 的縮放因子\n",
        "            scaler.update()\n",
        "            # optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        if avg_train_loss < best_loss:\n",
        "            best_loss = avg_train_loss\n",
        "            model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_best\")\n",
        "\n",
        "        avg_alignment = get_avg_alignment(model, val_loader, device, epoch)\n",
        "        print(f\"Stage2 Epoch {epoch+1}/{train_cfg.stage2_epochs} | Loss: {avg_train_loss:.4f} | Alignment: {avg_alignment:.4f}\")\n",
        "\n",
        "    # 保存第二阶段模型\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_final\")\n",
        "    print(\"Stage 2 completed!\")\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model=None, tokenizer=None, device = None):\n",
        "    \"\"\"第三步：指令微调\"\"\"\n",
        "    print(\"=== Stage 3: Instruction Tuning ===\")\n",
        "\n",
        "    _, train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "    scaler = torch.amp.GradScaler(device=device)\n",
        "\n",
        "    # 加载第二阶段模型\n",
        "    if stage2_model is not None:\n",
        "        model = stage2_model\n",
        "    else:\n",
        "        try:\n",
        "            model = AudioLanguageModel.from_pretrained(f\"{alm_cfg.alm_checkpoint_path}/stage2_final\")\n",
        "            print(\"Loaded Stage 2 model\")\n",
        "        except:\n",
        "            print(\"No Stage 2 model found, using current model\")\n",
        "            model = AudioLanguageModel(alm_cfg, load_backbone=True, tokenizer=tokenizer, device=device)\n",
        "\n",
        "    # 全部解冻，使用较小学习率\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    print(f\"Stage 3: Training all {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    # 更小的学习率\n",
        "    param_groups = [\n",
        "        {'params': model.MP.parameters(), 'lr': train_cfg.lr_mp * 0.01},\n",
        "        {'params': model.decoder.parameters(), 'lr': train_cfg.lr_backbones * 0.1},\n",
        "        {'params': model.audio_encoder.parameters(), 'lr': train_cfg.lr_backbones * 0.01}\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # 这里可以使用原来的训练循环，但数据应该是指令格式\n",
        "    # 暂时使用相同的数据格式\n",
        "    best_accuracy = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage3_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage3 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            # 2. 對 unscale 後的梯度進行裁剪 (max_norm=1.0 是一個常用的值)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            # --- 修改結束 ---\n",
        "\n",
        "            # 3. Scaler 執行優化器步驟 (如果梯度沒有 inf/nan)\n",
        "            scaler.step(optimizer)\n",
        "            # 4. 更新 scaler 的縮放因子\n",
        "            scaler.update()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                print(f\"Stage3 Step: {global_step}, Instruction Loss: {batch_loss:.4f}\")\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # 评估性能\n",
        "        if train_cfg.eval_in_epochs:\n",
        "            accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage3_best\")\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Loss: {avg_train_loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
        "        else:\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Instruction Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 保存最终模型\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/final_model\")\n",
        "    print(\"Stage 3 completed!\")\n",
        "    return model\n",
        "\n",
        "def train_three_stages(train_cfg, alm_cfg, device = None):\n",
        "    \"\"\"完整的三阶段训练\"\"\"\n",
        "    print(\"Starting Three-Stage Training Pipeline\")\n",
        "\n",
        "    # 第一阶段：模态投影器对齐\n",
        "    stage1_model = train_step1_alignment(train_cfg, alm_cfg, device=device)\n",
        "\n",
        "    # 第二阶段：语言模型预训练\n",
        "    stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model, device=device)\n",
        "\n",
        "    # 第三阶段：指令微调\n",
        "    final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model, device=device)\n",
        "\n",
        "    print(\"=== Training Pipeline Completed! ===\")\n",
        "    return stage1_model, stage2_model, final_model\n",
        "\n",
        "\n",
        "# # 替换原来的训练调用\n",
        "# alm_cfg = ALMConfig()\n",
        "# train_cfg = TrainConfig()\n",
        "\n",
        "# # 运行三阶段训练\n",
        "# final_model = train_three_stages(train_cfg, alm_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "## Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "BXUaUEUcJCp2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXUaUEUcJCp2",
        "outputId": "6be4a12a-8678-483c-82ad-8e743eb9fe56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'checkpoints' already exists.\n",
            "Loading from backbone weights\n",
            "Successfully loaded HuggingFaceTB/SmolLM2-1.7B weights from safetensors. Model has 1,711,376,384 parameters.\n",
            "Loading weights from ../stage2/model.safetensors with non-strict loading.\n",
            "Weights loaded.\n",
            "Mismatched keys (should be decoder embedding/head): ['decoder.token_embedding.weight', 'decoder.head.weight']\n",
            "Unexpected keys: []\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from models.config import ALMConfig, TrainConfig\n",
        "\n",
        "# 要創建的目錄路徑\n",
        "dir_name = ALMConfig.alm_checkpoint_path\n",
        "\n",
        "try:\n",
        "    os.mkdir(dir_name)\n",
        "    print(f\"Directory '{dir_name}' created successfully.\")\n",
        "except FileExistsError:\n",
        "    print(f\"Directory '{dir_name}' already exists.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Parent directory does not exist for '{dir_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "alm_cfg = ALMConfig()\n",
        "train_cfg = TrainConfig()\n",
        "\n",
        "tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "# 創建一個帶有新 token 的模型實例\n",
        "model = AudioLanguageModel(alm_cfg, load_backbone=True, tokenizer=tokenizer, device=device)\n",
        "stage1_model = None\n",
        "\n",
        "if train_cfg.resume_from_alm_checkpoint:\n",
        "    # checkpoint_path = \"../stage1/model.safetensors\"\n",
        "    checkpoint_path = \"../stage2/model.safetensors\"\n",
        "\n",
        "    print(f\"Loading weights from {checkpoint_path} with non-strict loading.\")\n",
        "\n",
        "    # 載入舊的 state_dict\n",
        "    # 注意：這裡假設您的權重是以 safetensors 格式保存的\n",
        "    from safetensors.torch import load_file\n",
        "    state_dict = load_file(checkpoint_path, device=\"cpu\") # move to GPU later\n",
        "    state_dict.pop('decoder.token_embedding.weight', None)\n",
        "    state_dict.pop('decoder.head.weight', None)\n",
        "    # *** 修改結束 ***\n",
        "\n",
        "    # 使用 strict=False 來載入權重\n",
        "    # 這會載入所有維度匹配的權重（例如 MP），並忽略不匹配的（例如 decoder.head）\n",
        "    incompatible_keys = model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    print(\"Weights loaded.\")\n",
        "    print(\"Mismatched keys (should be decoder embedding/head):\", incompatible_keys.missing_keys)\n",
        "    print(\"Unexpected keys:\", incompatible_keys.unexpected_keys)\n",
        "\n",
        "# 將模型移動到正確的設備\n",
        "model.to(device)\n",
        "stage1_model = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "id": "9MlFpXQFSNdx"
      },
      "outputs": [],
      "source": [
        "# stage1_model = train_step1_alignment(train_cfg, alm_cfg, model, tokenizer, device)\n",
        "# stage1_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ebd83a0",
      "metadata": {
        "id": "8ebd83a0"
      },
      "outputs": [],
      "source": [
        "stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model, tokenizer, device)\n",
        "stage2_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8aecfe",
      "metadata": {
        "id": "3c8aecfe"
      },
      "outputs": [],
      "source": [
        "final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model, tokenizer, device)\n",
        "final_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mZWoCjkGGfMQ",
      "metadata": {
        "id": "mZWoCjkGGfMQ"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "kjOsHkjpCoYp",
      "metadata": {
        "id": "kjOsHkjpCoYp"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/nanoALM/output_txt1.wav /content/output_txt1.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt2.wav /content/output_txt2.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt3.wav /content/output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dM8znZPTSj",
      "metadata": {
        "id": "07dM8znZPTSj"
      },
      "outputs": [],
      "source": [
        "!cp ../model.safetensors /content/drive/MyDrive/nanoALM/model.safetensors\n",
        "!cp ../config.json /content/drive/MyDrive/nanoALM/config.json\n",
        "!cp ./model.safetensors /content/drive/MyDrive/nanoALM\n",
        "!cp ./config.json /content/drive/MyDrive/nanoALM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSbBheB3HGns",
      "metadata": {
        "collapsed": true,
        "id": "RSbBheB3HGns"
      },
      "outputs": [],
      "source": [
        "# final_model.save_pretrained(\"/content/\")\n",
        "!python generate.py --checkpoint ../ --audio ../output_txt1.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FN6ra31lP8QM",
      "metadata": {
        "collapsed": true,
        "id": "FN6ra31lP8QM"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt2.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo_p1E3xP8gd",
      "metadata": {
        "collapsed": true,
        "id": "eo_p1E3xP8gd"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0cMUB4lb4zR",
      "metadata": {
        "id": "T0cMUB4lb4zR"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is said in this audio? <AUDIO>\"},\n",
        "]\n",
        "full_input_ids = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=True, add_generation_prompt=False\n",
        ")\n",
        "full_input_ids = torch.tensor(full_input_ids).to(device)\n",
        "if full_input_ids.dim() == 1:\n",
        "    full_input_ids = full_input_ids.unsqueeze(0)  # 添加 batch 維度\n",
        "print(\"full_input_ids.shape = \", full_input_ids.shape)\n",
        "\n",
        "generations = 5\n",
        "max_new_tokens = 100\n",
        "\n",
        "try:\n",
        "    audio_array, sr = librosa.load(\"../output_txt3.wav\", sr=16000)\n",
        "    # 轉換為 torch tensor 並添加 batch 維度\n",
        "    audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
        "    if audio_tensor.dim() == 1:\n",
        "        audio_tensor = audio_tensor.unsqueeze(0)  # 添加 channel 維度\n",
        "\n",
        "    # 使用 audio_processor 處理音頻\n",
        "    ap = get_audio_processor(alm_cfg)\n",
        "    audio_t = ap(audio_array, sr).unsqueeze(0).to(device)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading audio file: {e}\")\n",
        "    print(\"Please check if the audio file exists and is in a supported format.\")\n",
        "\n",
        "print(\"\\nInput:\\n \", messages[0][\"content\"], \"\\n\\nOutputs:\")\n",
        "for i in range(generations):\n",
        "    # 可以調整生成參數，例如 temperature, top_k, top_p\n",
        "    gen = model.generate(\n",
        "        full_input_ids,\n",
        "        audio_t,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        greedy=True,  # 先使用greedy解碼測試\n",
        "        top_k=10,     # 減小top_k\n",
        "        top_p=0.8,    # 減小top_p\n",
        "        temperature=0.3  # 降低temperature\n",
        "    )\n",
        "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "    print(f\"  >> Generation {i+1}: {out}\")"
      ],
      "metadata": {
        "id": "9pLrp4r8Sh5n",
        "outputId": "72c85cab-fc41-4bd5-c7ca-25588302a44f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9pLrp4r8Sh5n",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full_input_ids.shape =  torch.Size([1, 14])\n",
            "AudioProcessor_from_HF initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
            "  Target feature frames from cfg: 1500\n",
            "  Using model sampling rate: 16000, hop_length: 160, n_fft: 400\n",
            "  Calculated max raw audio samples for processor: 240240\n",
            "\n",
            "Input:\n",
            "  What is said in this audio? <AUDIO> \n",
            "\n",
            "Outputs:\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Audio Embeds = \n",
            ": shape=torch.Size([1, 25, 2048]), dtype=torch.float32, min=-4.0526, max=6.1099, mean=-0.0001\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Text Embeds = \n",
            ": shape=torch.Size([1, 14, 2048]), dtype=torch.float32, min=-1.1797, max=0.6328, mean=-0.0004\n",
            "  >> Generation 1: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Audio Embeds = \n",
            ": shape=torch.Size([1, 25, 2048]), dtype=torch.float32, min=-4.0526, max=6.1099, mean=-0.0001\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Text Embeds = \n",
            ": shape=torch.Size([1, 14, 2048]), dtype=torch.float32, min=-1.1797, max=0.6328, mean=-0.0004\n",
            "  >> Generation 2: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Audio Embeds = \n",
            ": shape=torch.Size([1, 25, 2048]), dtype=torch.float32, min=-4.0526, max=6.1099, mean=-0.0001\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Text Embeds = \n",
            ": shape=torch.Size([1, 14, 2048]), dtype=torch.float32, min=-1.1797, max=0.6328, mean=-0.0004\n",
            "  >> Generation 3: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Audio Embeds = \n",
            ": shape=torch.Size([1, 25, 2048]), dtype=torch.float32, min=-4.0526, max=6.1099, mean=-0.0001\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Text Embeds = \n",
            ": shape=torch.Size([1, 14, 2048]), dtype=torch.float32, min=-1.1797, max=0.6328, mean=-0.0004\n",
            "  >> Generation 4: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Audio Embeds = \n",
            ": shape=torch.Size([1, 25, 2048]), dtype=torch.float32, min=-4.0526, max=6.1099, mean=-0.0001\n",
            "\n",
            "Stats for Debug(AudioLanguageModel): Text Embeds = \n",
            ": shape=torch.Size([1, 14, 2048]), dtype=torch.float32, min=-1.1797, max=0.6328, mean=-0.0004\n",
            "  >> Generation 5: \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
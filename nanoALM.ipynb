{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a ALM in Google Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "3f74f04e-2fb8-4100-b24c-ed773e2c6f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nanoVLM_From_Huggingface' already exists and is not an empty directory.\n",
            "/content/nanoVLM_From_Huggingface\n",
            "assets\t\t\tcheckpoints  generate.py      nanoALM.ipynb  test\n",
            "benchmark-inference.py\tdata\t     measure_vram.py  old\n",
            "benchmark_suite.py\tdebug\t     models\t      README.md\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoALM'):\n",
        "    !git clone https://github.com/LWL220184016/nanoVLM_From_Huggingface.git\n",
        "%cd nanoVLM_From_Huggingface/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "54bc8463",
      "metadata": {
        "id": "54bc8463"
      },
      "outputs": [],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "# !huggingface-cli login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "collapsed": true,
        "id": "bcw8qQqoOSR7",
        "outputId": "55a0fbbb-ce29-496c-d63b-67674737af22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "偵測到檔案存在於 '/content/stage1'。將停止複製檔案和後續 pip 安裝。\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "check_dir = '/content/stage1'\n",
        "source_dir = '/content/drive/MyDrive/nanoALM/7/stage1'\n",
        "\n",
        "# 檢查來源資料夾是否存在且非空\n",
        "if os.path.isdir(check_dir) and os.listdir(check_dir):\n",
        "    print(f\"偵測到檔案存在於 '{check_dir}'。將停止複製檔案和後續 pip 安裝。\")\n",
        "else:\n",
        "    print(f\"'{check_dir}' 是空的或不存在。將複製檔案並繼續執行 pip 安裝。\")\n",
        "    !cp -r {source_dir} /content\n",
        "\n",
        "    # If you get an \"Error\" from pip's dependency resolver but the cell complets fine, this is not an issue, you can continue :)\n",
        "    !pip -q install torch\n",
        "    !pip -q install gcsfs\n",
        "    !pip -q install tqdm\n",
        "    !pip -q install huggingface_hub\n",
        "    !pip -q install librosa\n",
        "    !pip install soundfile librosa -q\n",
        "    # !pip install --upgrade transformers\n",
        "    !pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoALM\"\n",
        "# hf_model_name = \"YOUR_HF_USER_NAME/nanoALM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "c4678117-e002-4cd6-a73c-7a985cb42c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# nanoALM Imports (please check out the implementations in detail, that's where all the interessting stuff is!)\n",
        "from data.collators import AlignmentCollator, AudioQACollator\n",
        "from data.datasets import SAVEEDataset, AudioQADataset\n",
        "from data.processors import get_audio_processor\n",
        "from data.processors import get_tokenizer\n",
        "from models.audio_language_model import AudioLanguageModel\n",
        "import models.utils as utils\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "#Otherwise, the tokenizer will through a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n4oXPgTxQShw",
      "metadata": {
        "id": "n4oXPgTxQShw"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, alm_cfg, tokenizer):\n",
        "    # Create datasets\n",
        "    audio_processor = get_audio_processor(alm_cfg)\n",
        "\n",
        "    # text = \"splitting datasets, disable in get_dataloaders function\"\n",
        "    # print(f\"\\n\\033[38;5;05m{text}05m\\033[0m\")\n",
        "    # Load and combine all training datasets\n",
        "    combined_train_data = []\n",
        "    for dataset_name in train_cfg.train_dataset_name:\n",
        "        train_ds = load_dataset(\n",
        "        path = train_cfg.train_dataset_path,\n",
        "        name = dataset_name,\n",
        "    )\n",
        "        combined_train_data.append(train_ds['train'])\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    test_ds = load_dataset(train_cfg.test_dataset_path)\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    train_dataset = AudioQADataset(train_ds.select(range(train_size)), tokenizer, audio_processor)\n",
        "    val_dataset = AudioQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, audio_processor)\n",
        "    test_dataset = SAVEEDataset(test_ds, tokenizer, audio_processor)\n",
        "\n",
        "    # Create collators\n",
        "    alignment_collator = AlignmentCollator(tokenizer, alm_cfg.lm_max_length, audio_processor)\n",
        "    aqa_collator = AudioQACollator(tokenizer)\n",
        "    savee_collator = AudioQACollator(tokenizer)\n",
        "\n",
        "    # Create dataloaders\n",
        "    alignment_train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=alignment_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=aqa_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=train_cfg.savee_batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=savee_collator,\n",
        "        pin_memory=True,\n",
        "        )\n",
        "\n",
        "    return alignment_train_loader, train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D7NIuEDuOuuJ",
      "metadata": {
        "id": "D7NIuEDuOuuJ"
      },
      "source": [
        "### Prepare the testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9fnh6wOlOzat",
      "metadata": {
        "id": "9fnh6wOlOzat"
      },
      "outputs": [],
      "source": [
        "def test_savee(model, tokenizer, test_loader, device):\n",
        "    total_examples = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            audio = batch['audio'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            gen = model.generate(input_ids, audio, attention_mask)\n",
        "            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "            is_correct = utils.check_multiple_choice_with_regex(model_output, correct_answer)\n",
        "\n",
        "            total_examples += len(is_correct)\n",
        "            if is_correct:\n",
        "                correct_predictions += sum(is_correct)\n",
        "    accuracy = correct_predictions / total_examples if total_examples > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def get_avg_alignment(model, val_loader, device, epoch):\n",
        "    \"\"\"\n",
        "    Validate the model's audio-text alignment on the validation set.\n",
        "    This function computes the average alignment score over the validation set.\n",
        "    It runs for a maximum of 20 batches to save time during training.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_alignment_score = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            if i >= 20:  # 只驗證前20個batch以節省時間\n",
        "                break\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            alignment_score = model.validate_audio_text_alignment(input_ids, audios)\n",
        "            total_alignment_score += alignment_score\n",
        "\n",
        "    avg_alignment = total_alignment_score / min(20, len(val_loader))\n",
        "    print(f\"Epoch {epoch+1}: Average alignment score: {avg_alignment:.4f}\")\n",
        "\n",
        "    print(\" \")\n",
        "    model.train()\n",
        "    return avg_alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04da5b01",
      "metadata": {
        "id": "04da5b01"
      },
      "source": [
        "#### Three-stage training (contrast training, generative training, instruction fine-tuning) 三段式訓練(對比訓練, 生成式訓練, 指令微調)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fbb323c8",
      "metadata": {
        "id": "fbb323c8"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.amp as GradScaler\n",
        "\n",
        "from debug.debug_func import debug_contrastive_learning\n",
        "\n",
        "# 改進對比學習訓練\n",
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def get_contrastive_loss(audio_embeds, text_embeds, temperature=0.07):\n",
        "    \"\"\"\n",
        "    標準、高效的對比學習損失 (CLIP Loss)。\n",
        "    注意：輸入的 embeds 應該是池化後的 [B, D] 維度向量。\n",
        "    \"\"\"\n",
        "    # --- 開始計算尺度對齊損失 ---\n",
        "\n",
        "    # 1. 計算每個向量的 L2 範數 (沿著最後一個維度)\n",
        "    #    detach() 是為了確保這個輔助損失的梯度只流向投影器，而不影響上游的編碼器或文字嵌入層的穩定性（可選但推薦）\n",
        "    audio_norms = torch.norm(audio_embeds, p=2, dim=-1)\n",
        "    text_norms = torch.norm(text_embeds, p=2, dim=-1)\n",
        "\n",
        "    # 2. 計算批次內的平均範數\n",
        "    mean_audio_norm = torch.mean(audio_norms)\n",
        "    mean_text_norm = torch.mean(text_norms)\n",
        "\n",
        "    # 3. 計算尺度對齊損失 (使用 MSE)\n",
        "    #    目標是讓 mean_audio_norm 和 mean_text_norm 盡可能接近\n",
        "    scale_loss = F.mse_loss(mean_audio_norm, mean_text_norm)\n",
        "\n",
        "    # --- 結束計算尺度對齊損失 ---\n",
        "\n",
        "    # --- 開始計算交叉熵損失 ---\n",
        "    # 歸一化\n",
        "    audio_embeds = F.normalize(audio_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "\n",
        "    # 計算相似度矩陣\n",
        "    # temperature 是一個重要的超參數，CLIP 論文中是可學習的，但固定值也可以\n",
        "    logits_per_audio = torch.matmul(audio_embeds, text_embeds.T) / temperature\n",
        "    logits_per_text = logits_per_audio.T\n",
        "\n",
        "    # 創建標籤 (0, 1, 2, ..., B-1)\n",
        "    labels = torch.arange(audio_embeds.shape[0]).to(logits_per_audio.device)\n",
        "\n",
        "    # 對稱的交叉熵損失\n",
        "    loss_a = F.cross_entropy(logits_per_audio, labels)\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "\n",
        "    contrastive_loss = (loss_a + loss_t) / 2\n",
        "\n",
        "    # --- 結束計算交叉熵損失 ---\n",
        "\n",
        "\n",
        "    # 4. 組合損失\n",
        "    #    lambda_scale 是一個需要調整的超參數，用來平衡兩個損失的權重\n",
        "    lambda_scale = 0.001  # 範例值，可以從 0.01, 0.1, 1.0 等開始嘗試\n",
        "    total_loss = contrastive_loss + lambda_scale * scale_loss\n",
        "\n",
        "    # 監控指標 (可選但推薦)\n",
        "    with torch.no_grad():\n",
        "        pos_sim = torch.diagonal(logits_per_audio * temperature).mean()\n",
        "        mask = ~torch.eye(labels.shape[0], dtype=torch.bool, device=labels.device)\n",
        "        neg_sim = (logits_per_audio * temperature)[mask].mean()\n",
        "\n",
        "    return total_loss, contrastive_loss, scale_loss, {\n",
        "        \"loss\": total_loss.item(),\n",
        "        \"pos_sim\": pos_sim.item(), # 正樣本對的餘弦相似度\n",
        "        \"neg_sim\": neg_sim.item()  # 負樣本對的餘弦相似度\n",
        "    }\n",
        "\n",
        "def train_step1_alignment(train_cfg, alm_cfg, model=None, tokenizer=None, device=None):\n",
        "    # 凍結音頻編碼器和語言模型\n",
        "    model.audio_encoder.audio_encoder.requires_grad_(False)\n",
        "    model.decoder.requires_grad_(False)\n",
        "    model.MP.requires_grad_(True)\n",
        "\n",
        "    alignment_train_loader, _, val_loader, _ = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "\n",
        "    optimizer = optim.AdamW(model.MP.parameters(), lr=train_cfg.lr_mp, weight_decay=0.01)\n",
        "\n",
        "    best_alignment = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage1_epochs):\n",
        "        model.train()\n",
        "        total_contrastive_loss = 0  # 添加這個變數初始化\n",
        "        total_scale_loss = 0  # 添加這個變數初始化\n",
        "\n",
        "        for batch in tqdm(alignment_train_loader, desc=f\"Stage1 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. 音頻編碼 -> 投影\n",
        "            with torch.no_grad():\n",
        "                audio_features = model.audio_encoder.audio_encoder(audios, output_hidden_states=True)\n",
        "            projected_audio_features = model.MP(audio_features.last_hidden_state)\n",
        "\n",
        "            # 2. 文本編碼 - 修復這裡的問題\n",
        "            with torch.no_grad():\n",
        "                # 檢查 decoder 的 forward 方法簽名\n",
        "                # 根據 language_model.py，應該傳入 x 而不是分別的參數\n",
        "                text_embeds = model.decoder.token_embedding(input_ids)  # 直接獲取文本嵌入\n",
        "\n",
        "                # 如果需要通過完整的 decoder，使用以下方式：\n",
        "                # text_outputs, _ = model.decoder(text_embeds, attention_mask=attention_mask)\n",
        "                # text_embeds = text_outputs  # 使用輸出的嵌入\n",
        "\n",
        "            # 3. 池化操作 (Pooling)\n",
        "            # 音頻池化\n",
        "            audio_pooled = projected_audio_features.mean(dim=1)  # [B, D]\n",
        "\n",
        "            # 文本池化 - 修復維度問題\n",
        "            # text_embeds 現在是 [B, seq_len, hidden_dim]\n",
        "            if attention_mask is not None:\n",
        "                # 根據 attention_mask 來安全地做平均池化\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(text_embeds.size()).float()\n",
        "                sum_embeddings = torch.sum(text_embeds * input_mask_expanded, 1)\n",
        "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                text_pooled = sum_embeddings / sum_mask  # [B, D]\n",
        "            else:\n",
        "                text_pooled = text_embeds.mean(dim=1)  # [B, D]\n",
        "\n",
        "            # 如果維度仍然不匹配，添加投影層\n",
        "            if audio_pooled.shape[-1] != text_pooled.shape[-1]:\n",
        "                # 創建一個投影層來匹配維度\n",
        "                if not hasattr(model, 'text_projection'):\n",
        "                    model.text_projection = nn.Linear(text_pooled.shape[-1], audio_pooled.shape[-1]).to(device)\n",
        "                text_pooled = model.text_projection(text_pooled)\n",
        "\n",
        "            # 4. 計算對比損失\n",
        "            loss, contrastive_loss, scale_loss, metrics = get_contrastive_loss(audio_pooled, text_pooled)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.MP.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_contrastive_loss += contrastive_loss.item()\n",
        "            total_scale_loss += scale_loss.item()\n",
        "\n",
        "        avg_contrastive_loss = total_contrastive_loss / len(alignment_train_loader)\n",
        "        avg_scale_loss = total_scale_loss / len(alignment_train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1}: Total Loss {loss:.4f}, Contrastive Loss {avg_contrastive_loss:.4f}, Scale Loss {avg_scale_loss:.4f}\")\n",
        "\n",
        "        avg_alignment = get_avg_alignment(model, val_loader, device, epoch)\n",
        "\n",
        "        if avg_alignment > best_alignment:\n",
        "            best_alignment = avg_alignment\n",
        "            model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage1_best\")\n",
        "            print(f\"  New best alignment: {best_alignment:.4f}\")\n",
        "\n",
        "    print(f\"Stage 1 completed! Best alignment: {best_alignment:.4f}\")\n",
        "    return model\n",
        "\n",
        "def train_step2_pretraining(train_cfg, alm_cfg, stage1_model=None, tokenizer=None, device=None):\n",
        "    print(\"=== Stage 2: Language Model Pretraining ===\")\n",
        "\n",
        "    # 使用傳入的 tokenizer，不要重建\n",
        "    _, train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "\n",
        "    model = stage1_model\n",
        "    # 冻结/解冻\n",
        "    model.audio_encoder.audio_encoder.requires_grad_(False)\n",
        "    model.MP.requires_grad_(True)\n",
        "    model.decoder.requires_grad_(True)  # 或僅解凍頂層幾層\n",
        "\n",
        "    # 調小 decoder LR\n",
        "    param_groups = [\n",
        "        {'params': [p for p in model.MP.parameters() if p.requires_grad], 'lr': train_cfg.lr_mp * 0.05},\n",
        "        {'params': [p for p in model.decoder.parameters() if p.requires_grad], 'lr': train_cfg.lr_backbones},\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups, weight_decay=0.01)\n",
        "    scaler = torch.amp.GradScaler(device=device)\n",
        "\n",
        "    batch_losses = []\n",
        "    best_loss = float('inf')\n",
        "    global_step = 0\n",
        "    total_steps = len(train_loader) * train_cfg.stage2_epochs\n",
        "\n",
        "    for epoch in range(train_cfg.stage2_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage2 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            if alm_cfg.dtype == torch.float16:\n",
        "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    _, loss = model(input_ids, audios, attention_mask=attention_mask, labels=labels)\n",
        "            else:\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # 調整 LR\n",
        "            optimizer.param_groups[0]['lr'] = get_lr(global_step, param_groups[0]['lr'], total_steps)\n",
        "            optimizer.param_groups[1]['lr'] = get_lr(global_step, param_groups[1]['lr'], total_steps)\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad and p.grad is not None],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        if avg_train_loss < best_loss:\n",
        "            best_loss = avg_train_loss\n",
        "            model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_best\")\n",
        "\n",
        "        avg_alignment = get_avg_alignment(model, val_loader, device, epoch)\n",
        "        print(f\"\\nEpoch {epoch+1}/{train_cfg.stage2_epochs} | Loss: {avg_train_loss:.4f} | Alignment: {avg_alignment:.4f}\")\n",
        "\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage2_final\")\n",
        "    print(\"Stage 2 completed!\")\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model=None, tokenizer=None, device=None):\n",
        "    \"\"\"第三步：指令微调\"\"\"\n",
        "    print(\"=== Stage 3: Instruction Tuning ===\")\n",
        "\n",
        "    _, train_loader, val_loader, test_loader = get_dataloaders(train_cfg, alm_cfg, tokenizer)\n",
        "    scaler = torch.amp.GradScaler(device=device)\n",
        "\n",
        "    model = stage2_model\n",
        "    # 全部解冻，使用较小学习率\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    print(f\"Stage 3: Training all {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    # 更小的学习率\n",
        "    param_groups = [\n",
        "        {'params': model.MP.parameters(), 'lr': train_cfg.lr_mp * 0.01},\n",
        "        {'params': model.decoder.parameters(), 'lr': train_cfg.lr_backbones * 0.1},\n",
        "        {'params': model.audio_encoder.audio_encoder.parameters(), 'lr': train_cfg.lr_backbones * 0.01}\n",
        "    ]\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # 这里可以使用原来的训练循环，但数据应该是指令格式\n",
        "    # 暂时使用相同的数据格式\n",
        "    best_accuracy = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(train_cfg.stage3_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Stage3 Epoch {epoch+1}\"):\n",
        "            audios = batch[\"audio\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if alm_cfg.dtype == torch.float16:\n",
        "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    _, loss = model(input_ids, audios, attention_mask=attention_mask, labels=labels)\n",
        "            else:\n",
        "                _, loss = model(input_ids, audios, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "\n",
        "            scaler.unscale_(optimizer)\n",
        "            # 2. 對 unscale 後的梯度進行裁剪 (max_norm=1.0 是一個常用的值)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            # --- 修改結束 ---\n",
        "\n",
        "            # 3. Scaler 執行優化器步驟 (如果梯度沒有 inf/nan)\n",
        "            scaler.step(optimizer)\n",
        "            # 4. 更新 scaler 的縮放因子\n",
        "            scaler.update()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            if global_step % 50 == 0:\n",
        "                print(f\"Stage3 Step: {global_step}, Instruction Loss: {batch_loss:.4f}\")\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # 评估性能\n",
        "        if train_cfg.eval_in_epochs:\n",
        "            accuracy = test_savee(model, tokenizer, test_loader, device)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/stage3_best\")\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Loss: {avg_train_loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
        "        else:\n",
        "            print(f\"Stage3 Epoch {epoch+1}/{train_cfg.stage3_epochs} | Instruction Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # 保存最终模型\n",
        "    model.save_pretrained(save_directory=f\"{alm_cfg.alm_checkpoint_path}/final_model\")\n",
        "    print(\"Stage 3 completed!\")\n",
        "    return model\n",
        "\n",
        "def train_three_stages(train_cfg, alm_cfg, device=None):\n",
        "    \"\"\"完整的三阶段训练\"\"\"\n",
        "    print(\"Starting Three-Stage Training Pipeline\")\n",
        "\n",
        "    # 第一阶段：模态投影器对齐\n",
        "    stage1_model = train_step1_alignment(train_cfg, alm_cfg, device=device)\n",
        "\n",
        "    # 第二阶段：语言模型预训练\n",
        "    stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model, device=device)\n",
        "\n",
        "    # 第三阶段：指令微调\n",
        "    final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model, device=device)\n",
        "\n",
        "    print(\"=== Training Pipeline Completed! ===\")\n",
        "    return stage1_model, stage2_model, final_model\n",
        "\n",
        "\n",
        "# # 替换原来的训练调用\n",
        "# alm_cfg = ALMConfig()\n",
        "# train_cfg = TrainConfig()\n",
        "\n",
        "# # 运行三阶段训练\n",
        "# final_model = train_three_stages(train_cfg, alm_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a588871",
      "metadata": {
        "id": "2a588871"
      },
      "source": [
        "## Debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "42a8cecc",
      "metadata": {
        "id": "42a8cecc"
      },
      "outputs": [],
      "source": [
        "# !python ./debug/debug_forward.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "## Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "BXUaUEUcJCp2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXUaUEUcJCp2",
        "outputId": "63eb749e-9782-4c01-9833-74de22aabedb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'checkpoints' already exists.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================\n",
            "load model from local\n",
            "=================================================\n",
            "Initializing empty audio encoder: openai/whisper-small.en\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from models.config import ALMConfig, TrainConfig\n",
        "\n",
        "# 要創建的目錄路徑\n",
        "dir_name = ALMConfig.alm_checkpoint_path\n",
        "\n",
        "try:\n",
        "    os.mkdir(dir_name)\n",
        "    print(f\"Directory '{dir_name}' created successfully.\")\n",
        "except FileExistsError:\n",
        "    print(f\"Directory '{dir_name}' already exists.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Parent directory does not exist for '{dir_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "alm_cfg = ALMConfig()\n",
        "train_cfg = TrainConfig()\n",
        "dtype = alm_cfg.dtype\n",
        "device = alm_cfg.device\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "tokenizer = get_tokenizer(alm_cfg.lm_tokenizer)\n",
        "\n",
        "# 創建一個帶有新 token 的模型實例\n",
        "model = None\n",
        "stage1_model = None\n",
        "\n",
        "if train_cfg.resume_from_alm_checkpoint:\n",
        "    checkpoint_path = \"../stage1\"\n",
        "    # checkpoint_path = \"./checkpoints/stage1_best\"\n",
        "    stage1_model = AudioLanguageModel.from_pretrained(checkpoint_path, tokenizer=tokenizer)\n",
        "\n",
        "else:\n",
        "    model = AudioLanguageModel(alm_cfg, load_from_HF=True, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "id": "9MlFpXQFSNdx"
      },
      "outputs": [],
      "source": [
        "if model != None:\n",
        "    stage1_model = train_step1_alignment(train_cfg, alm_cfg, model, tokenizer, device)\n",
        "    stage1_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8ebd83a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8ebd83a0",
        "outputId": "7893db46-1540-44e9-c8cb-bdc6fc895f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Stage 2: Language Model Pretraining ===\n",
            "AudioProcessor_from_HF initialized with model: <class 'transformers.models.whisper.processing_whisper.WhisperProcessor'>\n",
            "  Target feature frames from cfg: 1500\n",
            "  Using model sampling rate: 16000, hop_length: 160, n_fft: 400\n",
            "  Calculated max raw audio samples for processor: 240240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rStage2 Epoch 1:   0%|          | 0/68 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198, 34763,    28,  1303,   549,   820,  1272,\n",
            "           30, 16912,    28,  6820,   617,   260, 12541,   314,    47,   657,\n",
            "          506,   549,    30,     0,     2,   198,     1,   520,  9531,   198,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 34763,    28,  1303,   549,   820,  1272,\n",
            "           30], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([   28,  6820,   617,   260, 12541,   314,    47,   657,   506,   549,\n",
            "           30,     0,     2,   198,     1,   520,  9531,   198,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198, 34763,    28,  1303,   549,   820,  1272,\n",
            "           30,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,    28,  6820,   617,   260,\n",
            "        12541,   314,    47,   657,   506,   549,    30,     0,     2,   198,\n",
            "            1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198, 34763,    28,   702,   339,  4415,  1092,\n",
            "          339,   761,  1296,  1035,  1165,    28,  8427, 30969, 13645,   282,\n",
            "         6137,   105,    30,   216,  1848,   506,   441,   908,  2001,   288,\n",
            "          820,   253,   811,   282,   827, 25386,    30,     0,     2,   198,\n",
            "            1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 34763,    28,   702,   339,  4415,  1092,\n",
            "          339], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([ 1296,  1035,  1165,    28,  8427, 30969, 13645,   282,  6137,   105,\n",
            "           30,   216,  1848,   506,   441,   908,  2001,   288,   820,   253,\n",
            "          811,   282,   827, 25386,    30,     0,     2,   198,     1,   520,\n",
            "         9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198, 34763,    28,   702,   339,  4415,  1092,\n",
            "          339,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  1296,  1035,  1165,    28,\n",
            "         8427, 30969, 13645,   282,  6137,   105,    30,   216,  1848,   506,\n",
            "          441,   908,  2001,   288,   820,   253,   811,   282,   827, 25386,\n",
            "           30,     0,     2,   198,     1,   520,  9531,   198,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,  3929,   451,   314,  2159,  6412,    30,\n",
            "          216,   339,  1441,    29,   339,   915,    29,   339,  6789,   982,\n",
            "          325, 17929,  1048,  1209,    30,     0,     2,   198,     1,   520,\n",
            "         9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 3929,  451,  314, 2159, 6412,   30,  216],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([ 1441,    29,   339,   915,    29,   339,  6789,   982,   325, 17929,\n",
            "         1048,  1209,    30,     0,     2,   198,     1,   520,  9531,   198,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,  3929,   451,   314,  2159,  6412,    30,\n",
            "          216,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  1441,    29,   339,   915,\n",
            "           29,   339,  6789,   982,   325, 17929,  1048,  1209,    30,     0,\n",
            "            2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,   933,    50,  3256, 12455,  4728,    77,\n",
            "          338,   506,  1123,    30,     0,     2,   198,     1,   520,  9531,\n",
            "          198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,   933,    50,  3256, 12455,  4728,    77,\n",
            "          338], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([1123,   30,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,   933,    50,  3256, 12455,  4728,    77,\n",
            "          338,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  1123,    30,     0,     2,\n",
            "          198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,  6910,    28,   346,   699,   787,  1542,\n",
            "         2166, 15411,   502,   359,  1035,  1165, 13645,    30,     0,     2,\n",
            "          198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 6910,   28,  346,  699,  787, 1542, 2166],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([  502,   359,  1035,  1165, 13645,    30,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,  6910,    28,   346,   699,   787,  1542,\n",
            "         2166,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   502,   359,  1035,  1165,\n",
            "        13645,    30,     0,     2,   198,     1,   520,  9531,   198,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([   1,  520, 9531,  198, 8432,  418,  260, 3163,  392, 3363,   30,  216,\n",
            "         339, 7336,  982, 3735,  451,  327, 3534,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 8432,  418,  260, 3163,  392, 3363,   30],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([ 339, 7336,  982, 3735,  451,  327, 3534,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([   1,  520, 9531,  198, 8432,  418,  260, 3163,  392, 3363,   30, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "         339, 7336,  982, 3735,  451,  327, 3534,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,  4184,   346,   501, 24471,    47,   216,\n",
            "         1046,  6737,  3956,   357,    30,  1046,   915,  3363,  1535,    28,\n",
            "          732,   506,   260,  1225,   282,  4167,   578,  1535,   585,   392,\n",
            "          359,   915,  2045,   288,  3934,    30,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,  4184,   346,   501, 24471,    47,   216,\n",
            "         1046], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([3956,  357,   30, 1046,  915, 3363, 1535,   28,  732,  506,  260, 1225,\n",
            "         282, 4167,  578, 1535,  585,  392,  359,  915, 2045,  288, 3934,   30,\n",
            "           0,    2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,  4184,   346,   501, 24471,    47,   216,\n",
            "         1046,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  3956,   357,    30,  1046,\n",
            "          915,  3363,  1535,    28,   732,   506,   260,  1225,   282,  4167,\n",
            "          578,  1535,   585,   392,   359,   915,  2045,   288,  3934,    30,\n",
            "            0,     2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,  4896,  3009,  1701, 10606,   982,   384,\n",
            "         7456,   282,  1272,    47, 13436,  1130,    28,  7003,   327,  1272,\n",
            "          418,  3163,   339,  1441,   536,   392, 13028,  1272,    28,   536,\n",
            "          392,  1643,  5173,   578,   338,   392,  1326,   982,   457,   750,\n",
            "         3826, 13532,   338,   392,  8540,   982,   761,  3826,   327,   929,\n",
            "         1209,    47,     0,     2,   198,     1,   520,  9531,   198],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,  4896,  3009,  1701, 10606,   982,   384,\n",
            "         7456], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([ 1272,    47, 13436,  1130,    28,  7003,   327,  1272,   418,  3163,\n",
            "          339,  1441,   536,   392, 13028,  1272,    28,   536,   392,  1643,\n",
            "         5173,   578,   338,   392,  1326,   982,   457,   750,  3826, 13532,\n",
            "          338,   392,  8540,   982,   761,  3826,   327,   929,  1209,    47,\n",
            "            0,     2,   198,     1,   520,  9531,   198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,  4896,  3009,  1701, 10606,   982,   384,\n",
            "         7456,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  1272,    47, 13436,  1130,\n",
            "           28,  7003,   327,  1272,   418,  3163,   339,  1441,   536,   392,\n",
            "        13028,  1272,    28,   536,   392,  1643,  5173,   578,   338,   392,\n",
            "         1326,   982,   457,   750,  3826, 13532,   338,   392,  8540,   982,\n",
            "          761,  3826,   327,   929,  1209,    47,     0,     2,   198,     1,\n",
            "          520,  9531,   198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,   669,   314,   915,   451,    30,   216,\n",
            "         1206,   699,    47,   216,   339,  1441,   357,   506,   253,  2341,\n",
            "          284,  3117,   564,   357,  3247,   982, 10105,  1745,    30,   216,\n",
            "         3315,   346,   699,   732,   339,  5248,  3396,   288,  1643,    47,\n",
            "            0,     2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198,  669,  314,  915,  451,   30,  216, 1206],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([   47,   216,   339,  1441,   357,   506,   253,  2341,   284,  3117,\n",
            "          564,   357,  3247,   982, 10105,  1745,    30,   216,  3315,   346,\n",
            "          699,   732,   339,  5248,  3396,   288,  1643,    47,     0,     2,\n",
            "          198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,   669,   314,   915,   451,    30,   216,\n",
            "         1206,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,    47,   216,   339,  1441,\n",
            "          357,   506,   253,  2341,   284,  3117,   564,   357,  3247,   982,\n",
            "        10105,  1745,    30,   216,  3315,   346,   699,   732,   339,  5248,\n",
            "         3396,   288,  1643,    47,     0,     2,   198,     1,   520,  9531,\n",
            "          198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,   339,   744,   260, 12541,    28, 12102,\n",
            "           23,   332,    30,     0,     2,   198,     1,   520,  9531,   198,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,   339,   744,   260, 12541,    28, 12102,\n",
            "           23], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([  30,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,   339,   744,   260, 12541,    28, 12102,\n",
            "           23,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,    30,     0,     2,   198,\n",
            "            1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([    1,   520,  9531,   198,  1626, 20031,   351,   338,    17,     0,\n",
            "            2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,  1626, 20031,   351,   338,    17,     0,\n",
            "            2], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([   1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([    1,   520,  9531,   198,  1626, 20031,   351,   338,    17,     0,\n",
            "            2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,     1,   520,  9531,   198,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-4.5904, max=4.5588, mean=-0.0003\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 93, 2048]), dtype=torch.float32, min=-57.6961, max=52.4467, mean=0.0054\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([59]) li:  tensor([   1,  520, 9531,  198,  339, 3060,  820,  578,   30,  216,  339, 3060,\n",
            "         685,  820, 7553,  634, 2361, 1745,   28,  339,   29,  339, 3060, 2330,\n",
            "         634, 2361, 1745,   28, 5196, 1315, 2863,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198,  339, 3060,  820,  578,   30,  216,  339],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([47]) right:  tensor([ 685,  820, 7553,  634, 2361, 1745,   28,  339,   29,  339, 3060, 2330,\n",
            "         634, 2361, 1745,   28, 5196, 1315, 2863,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([83]) li_expanded:  tensor([   1,  520, 9531,  198,  339, 3060,  820,  578,   30,  216,  339, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "         685,  820, 7553,  634, 2361, 1745,   28,  339,   29,  339, 3060, 2330,\n",
            "         634, 2361, 1745,   28, 5196, 1315, 2863,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "labels:  tensor([[ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rStage2 Epoch 1:   1%|▏         | 1/68 [00:04<04:52,  4.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([   1,  520, 9531,  198, 1206, 2316,  441, 2045,  702,  451,   30,    0,\n",
            "           2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 1206, 2316,  441, 2045,  702,  451,   30],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([   2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([   1,  520, 9531,  198, 1206, 2316,  441, 2045,  702,  451,   30, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "           2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([   1,  520, 9531,  198, 1073,  800,   47,    0,    2,  198,    1,  520,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 1073,  800,   47,    0,    2,  198,    1],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([   1,  520, 9531,  198, 1073,  800,   47,    0,    2,  198,    1, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([   1,  520, 9531,  198, 2838, 6954,   30,    0,    2,  198,    1,  520,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 2838, 6954,   30,    0,    2,  198,    1],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([   1,  520, 9531,  198, 2838, 6954,   30,    0,    2,  198,    1, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([    1,   520,  9531,   198,   339,  5248,  2969,  3100, 12364,   284,\n",
            "        20063,   282,   451,  6634,    30,     0,     2,   198,     1,   520,\n",
            "         9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,   339,  5248,  2969,  3100, 12364,   284,\n",
            "        20063], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([ 451, 6634,   30,    0,    2,  198,    1,  520, 9531,  198, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([    1,   520,  9531,   198,   339,  5248,  2969,  3100, 12364,   284,\n",
            "        20063,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   451,  6634,    30,     0,\n",
            "            2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([    1,   520,  9531,   198,  6910,    28,  1206,   359,   588,   973,\n",
            "        13442,   515,    30,   216,  3315,   346,  2853,    30,     0,     2,\n",
            "          198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,  6910,    28,  1206,   359,   588,   973,\n",
            "        13442], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([  30,  216, 3315,  346, 2853,   30,    0,    2,  198,    1,  520, 9531,\n",
            "         198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([    1,   520,  9531,   198,  6910,    28,  1206,   359,   588,   973,\n",
            "        13442,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,    30,   216,  3315,   346,\n",
            "         2853,    30,     0,     2,   198,     1,   520,  9531,   198,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([    1,   520,  9531,   198, 41827,    30,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 41827,    30,     0,     2,   198,     1,\n",
            "          520], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([ 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([    1,   520,  9531,   198, 41827,    30,     0,     2,   198,     1,\n",
            "          520,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([    1,   520,  9531,   198,   284,   392,  3060,  2217,   284,   346,\n",
            "         2045,   288,   457,   715,   253,  1123,   655,    28,   346,  2316,\n",
            "         2045,   288,  3230,   281,   588,   876,    28,   357,   506,  2045,\n",
            "          288,   325, 16626,     0,     2,   198,     1,   520,  9531,   198],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198,  284,  392, 3060, 2217,  284,  346, 2045],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([  457,   715,   253,  1123,   655,    28,   346,  2316,  2045,   288,\n",
            "         3230,   281,   588,   876,    28,   357,   506,  2045,   288,   325,\n",
            "        16626,     0,     2,   198,     1,   520,  9531,   198],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([    1,   520,  9531,   198,   284,   392,  3060,  2217,   284,   346,\n",
            "         2045,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   457,   715,   253,  1123,\n",
            "          655,    28,   346,  2316,  2045,   288,  3230,   281,   588,   876,\n",
            "           28,   357,   506,  2045,   288,   325, 16626,     0,     2,   198,\n",
            "            1,   520,  9531,   198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([    1,   520,  9531,   198, 16626,    30,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 16626,    30,     0,     2,   198,     1,\n",
            "          520], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([ 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([    1,   520,  9531,   198, 16626,    30,     0,     2,   198,     1,\n",
            "          520,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([   1,  520, 9531,  198, 1812,   29, 1812, 6688, 1250,  346, 1690,  281,\n",
            "         335,   47,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 1812,   29, 1812, 6688, 1250,  346, 1690],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([ 335,   47,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([   1,  520, 9531,  198, 1812,   29, 1812, 6688, 1250,  346, 1690, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "         335,   47,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([   1,  520, 9531,  198,  732,   47,    0,    2,  198,    1,  520, 9531,\n",
            "         198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198,  732,   47,    0,    2,  198,    1,  520],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([ 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([   1,  520, 9531,  198,  732,   47,    0,    2,  198,    1,  520, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "         198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([   1,  520, 9531,  198, 4184,  346, 3199,   47,  216, 1206, 2316, 2967,\n",
            "        7553,   30,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 4184,  346, 3199,   47,  216, 1206, 2316],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([7553,   30,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([   1,  520, 9531,  198, 4184,  346, 3199,   47,  216, 1206, 2316, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        7553,   30,    0,    2,  198,    1,  520, 9531,  198, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-4.5366, max=4.7515, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 74, 2048]), dtype=torch.float32, min=-54.5700, max=49.4284, mean=0.0065\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([40]) li:  tensor([    1,   520,  9531,   198,   339,   699,    30,  6910,   555,    30,\n",
            "        16222,    94,   982,  2875,   357,    30,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,   339,   699,    30,  6910,   555,    30,\n",
            "        16222], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([28]) right:  tensor([ 982, 2875,  357,   30,    0,    2,  198,    1,  520, 9531,  198, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([64]) li_expanded:  tensor([    1,   520,  9531,   198,   339,   699,    30,  6910,   555,    30,\n",
            "        16222,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   982,  2875,   357,    30,\n",
            "            0,     2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100], device='cuda:0')\n",
            "labels:  tensor([[  520,  9531,   198,  1206,  2316,   441,  2045,   702,   451,    30,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,     2,   198,     1,   520,  9531,\n",
            "           198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,  1073,   800,    47,     0,     2,   198,     1,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  9531,   198,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,  2838,  6954,    30,     0,     2,   198,     1,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  9531,   198,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,   339,  5248,  2969,  3100, 12364,   284, 20063,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   451,  6634,    30,     0,     2,\n",
            "           198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,  6910,    28,  1206,   359,   588,   973, 13442,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,    30,   216,  3315,   346,  2853,\n",
            "            30,     0,     2,   198,     1,   520,  9531,   198,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198, 41827,    30,     0,     2,   198,     1,   520,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   198,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,   284,   392,  3060,  2217,   284,   346,  2045,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   457,   715,   253,  1123,   655,\n",
            "            28,   346,  2316,  2045,   288,  3230,   281,   588,   876,    28,\n",
            "           357,   506,  2045,   288,   325, 16626,     0,     2,   198,     1,\n",
            "           520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198, 16626,    30,     0,     2,   198,     1,   520,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   198,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,  1812,    29,  1812,  6688,  1250,   346,  1690,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   335,    47,     0,     2,   198,\n",
            "             1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,   732,    47,     0,     2,   198,     1,   520,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   198,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,  4184,   346,  3199,    47,   216,  1206,  2316,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  7553,    30,     0,     2,   198,\n",
            "             1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100],\n",
            "        [  520,  9531,   198,   339,   699,    30,  6910,   555,    30, 16222,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,   982,  2875,   357,    30,     0,\n",
            "             2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "          -100,  -100,  -100]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rStage2 Epoch 1:   3%|▎         | 2/68 [00:06<03:13,  2.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([    1,   520,  9531,   198,   657,  3514,   335, 22865,  1434,   327,\n",
            "         2009,    30,     0,     2,   198,     1,   520,  9531,   198,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,   657,  3514,   335, 22865,  1434,   327,\n",
            "         2009], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([   0,    2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([    1,   520,  9531,   198,   657,  3514,   335, 22865,  1434,   327,\n",
            "         2009,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([    1,   520,  9531,   198, 43376,    28,   588,    30,     0,     2,\n",
            "          198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 43376,    28,   588,    30,     0,     2,\n",
            "          198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([ 520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([    1,   520,  9531,   198, 43376,    28,   588,    30,     0,     2,\n",
            "          198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   520,  9531,   198,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([   1,  520, 9531,  198, 1495,  915, 4775,  982,  732,  502, 2395,   30,\n",
            "           0,    2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 1495,  915, 4775,  982,  732,  502, 2395],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([   0,    2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([   1,  520, 9531,  198, 1495,  915, 4775,  982,  732,  502, 2395, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "           0,    2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([    1,   520,  9531,   198,  2838,   339,  5248,   915,   702,   346,\n",
            "           30,   216,   339,  2275,   357,   436,  2045,   288,   325,   511,\n",
            "        26140,   284, 43568,   284,   260, 11916,   736,   915,  2290,  2253,\n",
            "           30,   216,  1350,   392,  3060,   457,  1296,    28,  2760,   651,\n",
            "         1560, 31254,   277,  1122,    30,   216,   407,  6640,    28,  4516,\n",
            "           28,   260, 24021,   506,   335,   468,    30,   216,  4651,   346,\n",
            "          699,   732,   346,   820,    47,     0,     2,   198,     1,   520,\n",
            "         9531,   198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 2838,  339, 5248,  915,  702,  346,   30],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([  339,  2275,   357,   436,  2045,   288,   325,   511, 26140,   284,\n",
            "        43568,   284,   260, 11916,   736,   915,  2290,  2253,    30,   216,\n",
            "         1350,   392,  3060,   457,  1296,    28,  2760,   651,  1560, 31254,\n",
            "          277,  1122,    30,   216,   407,  6640,    28,  4516,    28,   260,\n",
            "        24021,   506,   335,   468,    30,   216,  4651,   346,   699,   732,\n",
            "          346,   820,    47,     0,     2,   198,     1,   520,  9531,   198],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([    1,   520,  9531,   198,  2838,   339,  5248,   915,   702,   346,\n",
            "           30,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   339,  2275,   357,   436,\n",
            "         2045,   288,   325,   511, 26140,   284, 43568,   284,   260, 11916,\n",
            "          736,   915,  2290,  2253,    30,   216,  1350,   392,  3060,   457,\n",
            "         1296,    28,  2760,   651,  1560, 31254,   277,  1122,    30,   216,\n",
            "          407,  6640,    28,  4516,    28,   260, 24021,   506,   335,   468,\n",
            "           30,   216,  4651,   346,   699,   732,   346,   820,    47,     0,\n",
            "            2,   198,     1,   520,  9531,   198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([    1,   520,  9531,   198,  1812, 39527,    47,     0,     2,   198,\n",
            "            1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,  1812, 39527,    47,     0,     2,   198,\n",
            "            1], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([    1,   520,  9531,   198,  1812, 39527,    47,     0,     2,   198,\n",
            "            1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  9531,   198,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([    1,   520,  9531,   198, 28844,   614,    30,   216, 28844,   614,\n",
            "           30,   339,  7336,   982, 17690,   346,  1163,   585,   346,  2422,\n",
            "        35354,   288,   549,   335,   469,   278,  1753, 17571,    30,     0,\n",
            "            2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 28844,   614,    30,   216, 28844,   614,\n",
            "           30], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([ 7336,   982, 17690,   346,  1163,   585,   346,  2422, 35354,   288,\n",
            "          549,   335,   469,   278,  1753, 17571,    30,     0,     2,   198,\n",
            "            1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([    1,   520,  9531,   198, 28844,   614,    30,   216, 28844,   614,\n",
            "           30,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  7336,   982, 17690,   346,\n",
            "         1163,   585,   346,  2422, 35354,   288,   549,   335,   469,   278,\n",
            "         1753, 17571,    30,     0,     2,   198,     1,   520,  9531,   198,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([   1,  520, 9531,  198, 2838,   30,    0,    2,  198,    1,  520, 9531,\n",
            "         198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 2838,   30,    0,    2,  198,    1,  520],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([ 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([   1,  520, 9531,  198, 2838,   30,    0,    2,  198,    1,  520, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "         198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([   1,  520, 9531,  198, 1812,  536,  392,  536,   47,    0,    2,  198,\n",
            "           1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 1812,  536,  392,  536,   47,    0,    2],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([   1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([   1,  520, 9531,  198, 1812,  536,  392,  536,   47,    0,    2, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "           1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([   1,  520, 9531,  198, 6910,   28,  957, 1839,   30,    0,    2,  198,\n",
            "           1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 6910,   28,  957, 1839,   30,    0,    2],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([   1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([   1,  520, 9531,  198, 6910,   28,  957, 1839,   30,    0,    2, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "           1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([   1,  520, 9531,  198,  339,  457,  288, 5847,  253, 1443, 3737,  327,\n",
            "         701, 1690,  685,  738,  957, 1495,  288,  820,  260, 2478, 1056,  338,\n",
            "         339, 3683,  982, 5344,   47,    0,    2,  198,    1,  520, 9531,  198,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198,  339,  457,  288, 5847,  253, 1443, 3737],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([ 701, 1690,  685,  738,  957, 1495,  288,  820,  260, 2478, 1056,  338,\n",
            "         339, 3683,  982, 5344,   47,    0,    2,  198,    1,  520, 9531,  198,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([   1,  520, 9531,  198,  339,  457,  288, 5847,  253, 1443, 3737, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "         701, 1690,  685,  738,  957, 1495,  288,  820,  260, 2478, 1056,  338,\n",
            "         339, 3683,  982, 5344,   47,    0,    2,  198,    1,  520, 9531,  198,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([    1,   520,  9531,   198,  4250,   982, 13731,   418,   549,    30,\n",
            "          216,   339,  5248,  3199,    30,     0,     2,   198,     1,   520,\n",
            "         9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,  4250,   982, 13731,   418,   549,    30,\n",
            "          216], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([5248, 3199,   30,    0,    2,  198,    1,  520, 9531,  198, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([    1,   520,  9531,   198,  4250,   982, 13731,   418,   549,    30,\n",
            "          216,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  5248,  3199,    30,     0,\n",
            "            2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-4.7913, max=4.4587, mean=-0.0005\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 106, 2048]), dtype=torch.float32, min=-54.5699, max=49.6458, mean=0.0082\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([72]) li:  tensor([    1,   520,  9531,   198, 10007,    30,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 10007,    30,     0,     2,   198,     1,\n",
            "          520], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([60]) right:  tensor([ 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([96]) li_expanded:  tensor([    1,   520,  9531,   198, 10007,    30,     0,     2,   198,     1,\n",
            "          520,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "labels:  tensor([[ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rStage2 Epoch 1:   4%|▍         | 3/68 [00:08<02:44,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([    1,   520,  9531,   198,  3009,  8320,   699,    47,  9413,   282,\n",
            "          260,  3191, 12053,    30,     0,     2,   198,     1,   520,  9531,\n",
            "          198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 3009, 8320,  699,   47, 9413,  282,  260],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([12053,    30,     0,     2,   198,     1,   520,  9531,   198,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([    1,   520,  9531,   198,  3009,  8320,   699,    47,  9413,   282,\n",
            "          260,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100, 12053,    30,     0,     2,\n",
            "          198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([    1,   520,  9531,   198, 22096,    28,   564,   787,    30,  3929,\n",
            "           28,   502,   457,   253,  3163,  4252,   564,   346,   699,   665,\n",
            "          506,   540,   702,  3163,  4252,   502,  2316,   441,   702,  5738,\n",
            "         4252,    28,   346,   699,    30,   339,  6820,   357,   506,  1942,\n",
            "          282,  1142,  2775,    30,     0,     2,   198,     1,   520,  9531,\n",
            "          198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198, 22096,    28,   564,   787,    30,  3929,\n",
            "           28], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([ 457,  253, 3163, 4252,  564,  346,  699,  665,  506,  540,  702, 3163,\n",
            "        4252,  502, 2316,  441,  702, 5738, 4252,   28,  346,  699,   30,  339,\n",
            "        6820,  357,  506, 1942,  282, 1142, 2775,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([    1,   520,  9531,   198, 22096,    28,   564,   787,    30,  3929,\n",
            "           28,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   457,   253,  3163,  4252,\n",
            "          564,   346,   699,   665,   506,   540,   702,  3163,  4252,   502,\n",
            "         2316,   441,   702,  5738,  4252,    28,   346,   699,    30,   339,\n",
            "         6820,   357,   506,  1942,   282,  1142,  2775,    30,     0,     2,\n",
            "          198,     1,   520,  9531,   198], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([   1,  520, 9531,  198, 6910,   28,  957,  310, 8619,   28, 5847,  588,\n",
            "         645, 1250,  346, 5754,  288,  536,  451,   28,  339, 1441,   28,    0,\n",
            "           2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 6910,   28,  957,  310, 8619,   28, 5847],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([ 645, 1250,  346, 5754,  288,  536,  451,   28,  339, 1441,   28,    0,\n",
            "           2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([   1,  520, 9531,  198, 6910,   28,  957,  310, 8619,   28, 5847, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "         645, 1250,  346, 5754,  288,  536,  451,   28,  339, 1441,   28,    0,\n",
            "           2,  198,    1,  520, 9531,  198, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([   1,  520, 9531,  198, 3569, 5872,   30,    0,    2,  198,    1,  520,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 3569, 5872,   30,    0,    2,  198,    1],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([   1,  520, 9531,  198, 3569, 5872,   30,    0,    2,  198,    1, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([   1,  520, 9531,  198, 6810,  732,   47,    0,    2,  198,    1,  520,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 6810,  732,   47,    0,    2,  198,    1],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([   1,  520, 9531,  198, 6810,  732,   47,    0,    2,  198,    1, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([   1,  520, 9531,  198, 1206, 1250,   47, 6910,   28,  957,  452, 8619,\n",
            "          28,  339, 5248,  588, 9723,  282,  346,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 1206, 1250,   47, 6910,   28,  957,  452],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([  28,  339, 5248,  588, 9723,  282,  346,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([   1,  520, 9531,  198, 1206, 1250,   47, 6910,   28,  957,  452, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "          28,  339, 5248,  588, 9723,  282,  346,   30,    0,    2,  198,    1,\n",
            "         520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([    1,   520,  9531,   198,  1268,  1687,   702,   260,   805,  4622,\n",
            "          418,   451,  1225,   975, 12032,   260,  8379,   905,   553,   915,\n",
            "        11238,   549,   282,   511,  1636,  6985,    30,     0,     2,   198,\n",
            "            1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 1268, 1687,  702,  260,  805, 4622,  418],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([ 1225,   975, 12032,   260,  8379,   905,   553,   915, 11238,   549,\n",
            "          282,   511,  1636,  6985,    30,     0,     2,   198,     1,   520,\n",
            "         9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([    1,   520,  9531,   198,  1268,  1687,   702,   260,   805,  4622,\n",
            "          418,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  1225,   975, 12032,   260,\n",
            "         8379,   905,   553,   915, 11238,   549,   282,   511,  1636,  6985,\n",
            "           30,     0,     2,   198,     1,   520,  9531,   198,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([    1,   520,  9531,   198,   657,   506,   441, 12416, 25185,    30,\n",
            "            0,     2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,   657,   506,   441, 12416, 25185,    30,\n",
            "            0], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([ 198,    1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([    1,   520,  9531,   198,   657,   506,   441, 12416, 25185,    30,\n",
            "            0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,   198,     1,   520,  9531,\n",
            "          198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([    1,   520,  9531,   198,  3315,   346,  1277,   288,   820,  7553,\n",
            "         1163,    47,    29,  1812,    47,   216,  1812,    28,   536,   346,\n",
            "         1277,   253, 16532,    47,     0,     2,   198,     1,   520,  9531,\n",
            "          198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 3315,  346, 1277,  288,  820, 7553, 1163],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([   29,  1812,    47,   216,  1812,    28,   536,   346,  1277,   253,\n",
            "        16532,    47,     0,     2,   198,     1,   520,  9531,   198,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([    1,   520,  9531,   198,  3315,   346,  1277,   288,   820,  7553,\n",
            "         1163,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,    29,  1812,    47,   216,\n",
            "         1812,    28,   536,   346,  1277,   253, 16532,    47,     0,     2,\n",
            "          198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([    1,   520,  9531,   198,  1644,   346,  8540,   982,  2269,   469,\n",
            "        10720,  1675,  3518,    30,  1206, 18927, 23712, 35479, 17506,   288,\n",
            "         1535,    30,     0,     2,   198,     1,   520,  9531,   198,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,  1644,   346,  8540,   982,  2269,   469,\n",
            "        10720], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([ 3518,    30,  1206, 18927, 23712, 35479, 17506,   288,  1535,    30,\n",
            "            0,     2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([    1,   520,  9531,   198,  1644,   346,  8540,   982,  2269,   469,\n",
            "        10720,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  3518,    30,  1206, 18927,\n",
            "        23712, 35479, 17506,   288,  1535,    30,     0,     2,   198,     1,\n",
            "          520,  9531,   198,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([   1,  520, 9531,  198, 8095, 1573, 2537, 6783,   30,    0,    2,  198,\n",
            "           1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([   1,  520, 9531,  198, 8095, 1573, 2537, 6783,   30,    0,    2],\n",
            "       device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([   1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([   1,  520, 9531,  198, 8095, 1573, 2537, 6783,   30,    0,    2, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "           1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): Input Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-4.8557, max=4.7542, mean=-0.0004\n",
            "Debug(AudioLanguageModel): Decoder Output Embeds = \n",
            ": shape=torch.Size([12, 85, 2048]), dtype=torch.float32, min=-54.5699, max=49.3462, mean=0.0079\n",
            "Debug(AudioLanguageModel): labels:  torch.Size([51]) li:  tensor([    1,   520,  9531,   198,   669,   314, 34319,   674,    30,     0,\n",
            "            2,   198,     1,   520,  9531,   198,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): <AUDIO> pos: 11, A (audio patches): 25\n",
            "Debug(AudioLanguageModel): A:  25\n",
            "Debug(AudioLanguageModel): left:  torch.Size([11]) left:  tensor([    1,   520,  9531,   198,   669,   314, 34319,   674,    30,     0,\n",
            "            2], device='cuda:0')\n",
            "Debug(AudioLanguageModel): right:  torch.Size([39]) right:  tensor([   1,  520, 9531,  198, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): audio_ign:  torch.Size([25]) audio_ign:  tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100], device='cuda:0')\n",
            "Debug(AudioLanguageModel): li_expanded:  torch.Size([75]) li_expanded:  tensor([    1,   520,  9531,   198,   669,   314, 34319,   674,    30,     0,\n",
            "            2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,     1,   520,  9531,   198,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
            "labels:  tensor([[ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100],\n",
            "        [ 520, 9531,  198,  ..., -100, -100, -100]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rStage2 Epoch 1:   4%|▍         | 3/68 [00:10<03:38,  3.37s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2545122792.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstage2_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step2_pretraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malm_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage1_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstage2_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3378914174.py\u001b[0m in \u001b[0;36mtrain_step2_pretraining\u001b[0;34m(train_cfg, alm_cfg, stage1_model, tokenizer, device)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# 調整 LR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "stage2_model = train_step2_pretraining(train_cfg, alm_cfg, stage1_model, tokenizer, device)\n",
        "stage2_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8aecfe",
      "metadata": {
        "id": "3c8aecfe"
      },
      "outputs": [],
      "source": [
        "final_model = train_step3_instruction_tuning(train_cfg, alm_cfg, stage2_model, tokenizer, device)\n",
        "final_model.save_pretrained(\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mZWoCjkGGfMQ",
      "metadata": {
        "id": "mZWoCjkGGfMQ"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kjOsHkjpCoYp",
      "metadata": {
        "id": "kjOsHkjpCoYp"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/nanoALM/output_txt1.wav /content/output_txt1.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt2.wav /content/output_txt2.wav\n",
        "!cp /content/drive/MyDrive/nanoALM/output_txt3.wav /content/output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dM8znZPTSj",
      "metadata": {
        "id": "07dM8znZPTSj"
      },
      "outputs": [],
      "source": [
        "!cp ../model.safetensors /content/drive/MyDrive/nanoALM/model.safetensors\n",
        "!cp ../config.json /content/drive/MyDrive/nanoALM/config.json\n",
        "!cp ./model.safetensors /content/drive/MyDrive/nanoALM\n",
        "!cp ./config.json /content/drive/MyDrive/nanoALM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RSbBheB3HGns",
      "metadata": {
        "collapsed": true,
        "id": "RSbBheB3HGns"
      },
      "outputs": [],
      "source": [
        "# final_model.save_pretrained(\"/content/\")\n",
        "!python generate.py --checkpoint ../ --audio ../output_txt1.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FN6ra31lP8QM",
      "metadata": {
        "collapsed": true,
        "id": "FN6ra31lP8QM"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt2.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo_p1E3xP8gd",
      "metadata": {
        "collapsed": true,
        "id": "eo_p1E3xP8gd"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T0cMUB4lb4zR",
      "metadata": {
        "id": "T0cMUB4lb4zR"
      },
      "outputs": [],
      "source": [
        "!python generate.py --checkpoint ../ --audio ../output_txt3.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9pLrp4r8Sh5n",
      "metadata": {
        "collapsed": true,
        "id": "9pLrp4r8Sh5n"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is said in this audio? <AUDIO>\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"},  # 讓模板能加上生成提示\n",
        "]\n",
        "full_input_ids = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=True, add_generation_prompt=True\n",
        ")\n",
        "full_input_ids = torch.tensor(full_input_ids, dtype=torch.long).to(device)\n",
        "if full_input_ids.dim() == 1:\n",
        "    full_input_ids = full_input_ids.unsqueeze(0)\n",
        "\n",
        "generations = 5\n",
        "max_new_tokens = 100\n",
        "\n",
        "try:\n",
        "    audio_array, sr = librosa.load(\"../output_txt1.wav\", sr=16000)\n",
        "    # 轉換為 torch tensor 並添加 batch 維度\n",
        "    audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
        "    if audio_tensor.dim() == 1:\n",
        "        audio_tensor = audio_tensor.unsqueeze(0)  # 添加 channel 維度\n",
        "\n",
        "    # 使用 audio_processor 處理音頻\n",
        "    ap = get_audio_processor(alm_cfg)\n",
        "    audio_t = ap(audio_array, sr).unsqueeze(0).to(device)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading audio file: {e}\")\n",
        "    print(\"Please check if the audio file exists and is in a supported format.\")\n",
        "\n",
        "tested_model = final_model if 'final_model' in globals() and final_model is not None else stage2_model\n",
        "tested_model.to(device).eval()\n",
        "\n",
        "print(\"\\nInput:\\n \", messages[0][\"content\"], \"\\n\\nOutputs:\")\n",
        "for i in range(generations):\n",
        "    gen = tested_model.generate(\n",
        "        full_input_ids,\n",
        "        audio_t,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        greedy=True,\n",
        "        top_k=10,\n",
        "        top_p=0.8,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "    print(f\"  >> Generation {i+1}: {out}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}